{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced (cooler, groovier) features\n",
    "\n",
    "In this section we will take a look at some features used in the NLP research. Some of them are pretty standard (__n-grams__), others are quickly becoming the new standard (__word embeddings__ learned in neural network-like manner), and some are used only in really desperate situations :D\n",
    "\n",
    "\n",
    "## Sequential models\n",
    "\n",
    "HMM\n",
    "Link. Link\n",
    "http://cs229.stanford.edu/section/cs229-hmm.pdf\n",
    "https://en.wikipedia.org/wiki/Hidden_Markov_model\n",
    "\n",
    "\n",
    "Hidden Markov Models\n",
    "Markov chains\n",
    "http://setosa.io/ev/markov-chains/\n",
    "\n",
    "Is deep learning a Markov chain in disguise?\n",
    "http://www.r-bloggers.com/is-deep-learning-a-markov-chain-in-disguise/\n",
    "\n",
    "CRF\n",
    "Link\n",
    "https://en.wikipedia.org/wiki/Conditional_random_field\n",
    "\n",
    "\n",
    "IOB\n",
    "\n",
    "\n",
    "### PoS-tags\n",
    "\n",
    "Part-of-speech tagging\n",
    "Example\n",
    "https://spacy.io/blog/part-of-speech-POS-tagger-in-python\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### N-grams\n",
    "N-gram models\n",
    "Language detection\n",
    "http://web.stanford.edu/class/cs124/lec/languagemodeling.pdf\n",
    "N-gram\n",
    "Skip-gram\n",
    "Window / Context\n",
    "\n",
    "\n",
    "### Collocations\n",
    "\n",
    "Term extraction / Idioms vs. multiwords vs. collocations\n",
    "\n",
    "#\tcollocations\n",
    "http://www.nltk.org/howto/collocations.html\t[tutorial]\n",
    "http://www.nltk.org/_modules/nltk/collocations.html\t[code]\n",
    "http://www.nltk.org/_modules/nltk/metrics/association.html\t[metrics\n",
    "\n",
    "\n",
    "\n",
    "## Brown clusters\n",
    "Solve overfitting problem with Logistic Regression: if several words, they must share the score (non-naive assumption, but also strict assumption because of divide-and-conquer: each predictor now weights less due to the others). Given a document containing: doctor, nursel they would each share 0.2 prob, for instance, which will make them harder for them to compete with a document containing only one of them: 0.2 for either. However, if we can tell the system that doctor and nurse belong to the same domain, HOSPITAL, then we can learn a 0.4 weight for the overall topic so that, every time we find a single word from that topic, we get the full score, 0.4 instead of 0.2. We suddenly doubled the confidence of our estimates, and helped our system not to make a mistake for data like \"My sister is a student\" just because the student domain didn't happen to be split in more words sharing the confidence.\n",
    "= logic for Brown clusters and all other knowledge-intensive methods we will see now.\n",
    "\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "See embeddings\n",
    "gensim\n",
    "\n",
    "\n",
    "## Synsets (*syn*onym *sets*)\n",
    "See synsets\n",
    "\n",
    "Taxonomies\n",
    "\n",
    "Ontologies\n",
    "\n",
    "Hypernyms and hyponyms\n",
    "\n",
    "Part-of (meronyms)\n",
    "\n",
    "''Extraction of Concrete Entities and Part-Whole Relations'' proposes a method for meronymy (part-whole) extraction of concrete objects from a corpus. Precision rates were somewhat low, but the authors suggest this could be improved with better filtering of non-relevant words.\n",
    "\n",
    "Metonymy\n",
    "\n",
    "\n",
    "## Entities\n",
    "\n",
    "\n",
    "Semantics\n",
    "NLTK (letter case, joke!) https://groups.google.com/forum/#!topic/nltk-users/3o92c3ESnVI\n",
    "\n",
    "Link\n",
    "https://groups.google.com/forum/#!topic/nltk-users/3o92c3ESnVI\n",
    "Dear Group,\n",
    "\n",
    "NLTK has a good solution for Name Entity Recognition. We can also do by using PoS Tagger. \n",
    "I am trying to solve name entity resolution. May I use NLTK for the same?\n",
    "\n",
    "Please suggest. \n",
    "\n",
    "Regards,\n",
    "Richard Parker \n",
    "\n",
    "\n",
    "#here is an example:\n",
    "sent=\"Deletion of the ADH6 gene has no significant effect on the expression of the ADH7 gene.\"\n",
    "words=nltk.word_tokenize(sent)\n",
    "tags=nltk.pos_tag(words)\n",
    "chunks=nltk.ne_chunk(tags,binary=True)\n",
    "NE=[]\n",
    "for a in chunks.subtrees():\n",
    "\tif a.label()==\"NE\":\n",
    "    \tNE.append(a[0][0])\n",
    "print(NE)\n",
    "#we can get three named entity:['Deletion', 'ADH6', 'ADH7']\n",
    "\n",
    "I hope this can help you.\n",
    "\n",
    "\n",
    "\n",
    "=== NER basics ===\n",
    "1) Standard NLTK test\n",
    "No custom data:\n",
    "\thttps://gist.github.com/onyxfish/322906\n",
    "\tNER test\n",
    "\t\tner_test.py\t# https://pythonprogramming.net/named-entity-recognition-nltk-python/\n",
    "\tWould like custom data but no:\n",
    "http://stackoverflow.com/questions/11333903/nltk-named-entity-recognition-with-custom-data\n",
    "\n",
    "\n",
    "Standard NER\n",
    "https://en.wikipedia.org/wiki/Named-entity_recognition\n",
    "Semantic tags\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Sequential models (CRF)\n",
    "\n",
    "Commonsense knowledge\n",
    "Folksonomies\n",
    "Factoids\n",
    "Factual knowledge\n",
    "Freebase\n",
    "Inference\n",
    "Knowledge-based NER (BabelNet/SEW, OmegaWiki, ConceptNet, Freebase) ===> GREAT FOR BOOTSTRAPPING\n",
    "\n",
    "### Supersenses\n",
    "Supersenses\n",
    "Wikipedia categories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
