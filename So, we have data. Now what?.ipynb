{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, we have data. Now what?\n",
    "\n",
    "Now we slice it and then we cook it :D\n",
    "\n",
    "At this point, the input we are getting from all these __streamers__ or __readers__ are documents or sentences, usually as Python strings. That is a start but it still does not count as NLP :)\n",
    "\n",
    "The main problem is that string are too large to be useful: for whole documents, it is very unlikely our system will ever come across the same string. Therefore, even if we trained it to perform a perfect action for that document, it would never get a chance to perform that action. That means zero generalization and no automation. In order to really automate something, we need to ensure it is repeatable.\n",
    "\n",
    "At the core of repeatability lies resolution: the higher our system's resolution, and the more parts of a unit it is sensitive to, the greater the likelihood that it will find a pattern shared by its training data that can lead it to provide the correct answer.\n",
    "\n",
    "The process of reducing strings into smaller subunits is called __feature extraction__ and it is the first step towards turning something a person wrote into something a computer can understand.\n",
    "\n",
    "\n",
    "# Basics of __feature extraction__\n",
    "Some of the most trivial preprocessing steps we can perform are sentence splitting and word tokenization. Again, NLTK can help us with that. Let's see some examples from the Reuters corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION\n",
      "  Mines and Energy Minister Subroto\n",
      "  confirmed Indonesian support for an extension of the sixth\n",
      "  International Tin Agreement (ITA), but said a new pact was not\n",
      "  necessary.\n",
      "      Asked by Reuters to clarify his statement on Monday in\n",
      "  which he said the pact should be allowed to lapse, Subroto said\n",
      "  Indonesia was ready to back extension of the ITA.\n",
      "      \"We can support extension of the sixth agreement,\" he said.\n",
      "  \"But a seventh accord we believe to be unnecessary.\"\n",
      "      The sixth ITA will expire at the end of June unless a\n",
      "  two-thirds majority of members vote for an extension.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "i = list(reuters.fileids())[10]\n",
    "print reuters.raw(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'SUBROTO', u'SAYS', u'INDONESIA', u'SUPPORTS', u'TIN', u'PACT', u'EXTENSION', u'Mines', u'and', u'Energy', u'Minister', u'Subroto', u'confirmed', u'Indonesian', u'support', u'for', u'an', u'extension', u'of', u'the', u'sixth', u'International', u'Tin', u'Agreement', u'(', u'ITA', u'),', u'but', u'said', u'a', u'new', u'pact', u'was', u'not', u'necessary', u'.'], [u'Asked', u'by', u'Reuters', u'to', u'clarify', u'his', u'statement', u'on', u'Monday', u'in', u'which', u'he', u'said', u'the', u'pact', u'should', u'be', u'allowed', u'to', u'lapse', u',', u'Subroto', u'said', u'Indonesia', u'was', u'ready', u'to', u'back', u'extension', u'of', u'the', u'ITA', u'.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print reuters.sents(i.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'SUBROTO', u'SAYS', u'INDONESIA', u'SUPPORTS', ...]\n"
     ]
    }
   ],
   "source": [
    "print reuters.words(i.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances of the NLTK's CorpusReader class will come with built-in methods for accessing this information, which will therefore be available for the corpus supported in NLTK. What if we are given a new dataset, not in NLTK?\n",
    "\n",
    "We will still use NLTK :) (told you, it helps ;) More specifically, methods such as __wordpunct_tokenize__ and __sent_tokenize__. Let's pretend the Reuters text is our new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION\\n  Mines and Energy Minister Subroto\\n  confirmed Indonesian support for an extension of the sixth\\n  International Tin Agreement (ITA), but said a new pact was not\\n  necessary.', 'Asked by Reuters to clarify his statement on Monday in\\n  which he said the pact should be allowed to lapse, Subroto said\\n  Indonesia was ready to back extension of the ITA.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import (\n",
    "    sent_tokenize as splitter,\n",
    "    wordpunct_tokenize as tokenizer\n",
    ")\n",
    "\n",
    "text = \"\"\"SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION\n",
    "  Mines and Energy Minister Subroto\n",
    "  confirmed Indonesian support for an extension of the sixth\n",
    "  International Tin Agreement (ITA), but said a new pact was not\n",
    "  necessary.\n",
    "      Asked by Reuters to clarify his statement on Monday in\n",
    "  which he said the pact should be allowed to lapse, Subroto said\n",
    "  Indonesia was ready to back extension of the ITA.\n",
    "      \"We can support extension of the sixth agreement,\" he said.\n",
    "  \"But a seventh accord we believe to be unnecessary.\"\n",
    "      The sixth ITA will expire at the end of June unless a\n",
    "  two-thirds majority of members vote for an extension.\"\"\"\n",
    "\n",
    "print splitter(text)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SUBROTO', 'SAYS', 'INDONESIA', 'SUPPORTS', 'TIN', 'PACT', 'EXTENSION', 'Mines', 'and', 'Energy', 'Minister', 'Subroto', 'confirmed', 'Indonesian', 'support', 'for', 'an', 'extension', 'of', 'the', 'sixth', 'International', 'Tin', 'Agreement', '(', 'ITA', '),', 'but', 'said', 'a', 'new', 'pact', 'was', 'not', 'necessary', '.'], ['Asked', 'by', 'Reuters', 'to', 'clarify', 'his', 'statement', 'on', 'Monday', 'in', 'which', 'he', 'said', 'the', 'pact', 'should', 'be', 'allowed', 'to', 'lapse', ',', 'Subroto', 'said', 'Indonesia', 'was', 'ready', 'to', 'back', 'extension', 'of', 'the', 'ITA', '.']]\n"
     ]
    }
   ],
   "source": [
    "print [tokenizer(sent) for sent in splitter(text)[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can easily use NLTK's methods to get the sentences in a text, or the words in that text. Some systems do not need sentences as input and are designed to work with the individual words in each document (text classification, sentiment analysis, or term extraction can all work either at the sentence or the document level with little difference overall). However, sentences will be useful when building statistical models like n-gram models (we will see that in more detail in a bit).\n",
    "\n",
    "## Tokens\n",
    "The term _words_ we used in the previous paragraph is technically incorrect. At this level, we are still working with __tokens__, which include all words in a text plus any punctuation signs, plus any numerical expressions, plus virtually anything else. Strictly speaking, the number \"103\" is not a word but it can appear in a text. Therefore, it is a __token__. A __token__ refers to anything that appears within __word boundaries__. Word boundaries are not always well-defined and even today there are inconsistencies in the way some tokens are handled by different systems: some consider _didn't_ two separate words, and tokenize it as _did not_, whereas others take it as a single word. For our purposes, _did not_ is preferable to _didn't_ but, generally speaking, we would like our system to be robust to this type of thing. That is, we don't want to have to teach it explicitly that _did not_ and _didn't_ are two variants of the same linguistic unit. We want the system to infer that from their behavior as observed in the data. That will be when we will be able to claim we are doing something close to machine learning.\n",
    "\n",
    "## Punctuation\n",
    "Although some advanced applications can handle non-word tokens such as punctuation (for instance, to skip parenthetical elements inside a sentence, or appositions), most applications do not do much with it once the sentences have been split. Generally speaking, we can ignore punctuation in most tasks and we will still get the desired results, assuming we can train a good sequence model (we will see what this means later).\n",
    "\n",
    "## Numerical expressions\n",
    "The case of numerical expressions is more complicated because most systems tend to remove any patterns involving digits, which is fine in many cases. Sometimes, however, and depending on the task you are working on, numerical information will be crucial for the system: in many applications in which time information is important (for example, a website for booking flights needs to detect and process the dates of a trip), numerical information cannot be removed. Instead, a specific parser is usually required to handle each particular type of information: temporal expressions, clothing sizes for eBay product match, serial number recognition, etc.\n",
    "\n",
    "##### A quick note on feature engineering\n",
    "_These are all important points regarding __feature engineering__, and the main reason why we said earlier that developing a good understanding of the data is so important in NLP: unless you are certain that your data contains enough examples to account for the phenomena you want to model (so that enough features can be derived from it and the system can learn the appropriate behavior), the algorithm will not be able to make sense of the data. You cannot train a system for detecting flight information without information about flights. NLP often has the responsibility of keeping things real in machine learning and asking questions like **\"So you want your system to detect funny content... Are you detecting any jokes right now?\"**_ :)\n",
    "\n",
    "## Letter case\n",
    "The previous examples also contain a particular type of word variant, __letter case__. Case refers to whether a word is __UPPERCASE__, __lowercase__, __Titlecase__, or some __CoMbInAtIoN__ of both (the last one is often called __camel case__).\n",
    "\n",
    "A common type of preprocessing step consists in lowercasing all words, collapsing or case variants into their lowercase form. This will normally preserve most of the original information (the meaning of _combination_ does not usually change based on its case), although there are obviously exceptions (a _bond_ refers to a type of agreement or to a strong link, whereas _Bond_ is a secret agent. In most corpora there will be no confusion, but that might not be the case in a dataset about movie reviews. Again, knowing your data is important to make this kind of decisions).\n",
    "\n",
    "The goal of collapsing all these variants into a single one is to prevent the system from considering _the, The_ and _THE_ three different words, which would result in incorrectly dividing their frequency counts three-fold when calculating their probabilities, for instance. This type of normalization can easily boost the frequency of some terms several times, which results in a statistical model much closer to the actual truth of the data.\n",
    "\n",
    "## On the topic of variants\n",
    "Letter case is just a subtype of the more general fact that words have variants, and there are other types of variation that will make your NLP lives miserable. The two main types are __inflectional variation__ and __misspellings__.\n",
    "\n",
    "Inflectional variation is what the type of variation whereby we recognize the words _child_ and _children_ to be two __forms__ of the same __lemma__. The lemma is usually seen as referring to the actual concept, whereas the forms account for morphological variation, alternative shapes that the lemma takes to fit in particular linguistic contexts such as the plural number, or verb tenses. In languages like Spanish or Ukrainian, inflections are much richer than in English, and the amount of variation is also much higher, which usually requires some additional processing.\n",
    "\n",
    "In NLP, the two main ways of dealing with __forms__ is to either __stem__ them or __lemmatize__ them. Stemming is usually faster, easier, and error-prone. The main problem is that there is no guarantee that you end up with actual lemma. Lemmatization, on the other hand, tries to fulfill that requirement, but it has a number of relatively strong pressupositions: 1) part-of-speech tagging, which is not always trivial and that we will introduce later on, and 2) most often, a language-specific dictionary with mappings between forms and their lemmas. For now, let's take a look at the differences with a simple example. As always, NLTK can help us, although there are now other libraries we can use for this: __pattern__ and __spacy__, which offer better performance than NLTK for some tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_stemmer \t=\t[u'Strang', u'listen', u'women', u'lie', u'children', u'govern', u'distribut', u'basi', u'suprem', u'power', u'deriv', u'adequ', u'ceremoni'] \n",
      "\n",
      "lancaster_stemmer \t=\t['strange', 'list', 'wom', 'lying', 'childr', 'govern', 'distribut', 'bas', 'suprem', 'pow', 'der', 'adequ', u'ceremony'] \n",
      "\n",
      "snowball_stemmer \t=\t[u'strang', u'listen', u'women', u'lie', u'children', u'govern', u'distribut', u'basi', u'suprem', u'power', u'deriv', u'adequ', u'ceremoni'] \n",
      "\n",
      "wordnet_lemmatizer \t=\t['Strange', 'listening', u'woman', 'lying', u'child', 'government', 'distribution', 'basis', 'supreme', 'power', 'derives', 'adequate', u'ceremony'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#    Using NLTK\n",
    "import nltk\n",
    "\n",
    "#   stemmers\n",
    "stemmer1 = nltk.PorterStemmer()\n",
    "stemmer2 = nltk.LancasterStemmer()\n",
    "stemmer3 = nltk.SnowballStemmer(u'english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatizer.stem = lemmatizer.lemmatize\n",
    "normalizers = [\n",
    "    ('porter_stemmer', stemmer1),\n",
    "    ('lancaster_stemmer', stemmer2),\n",
    "    ('snowball_stemmer', stemmer3),\n",
    "    ('wordnet_lemmatizer', lemmatizer)\n",
    "]\n",
    "\n",
    "test = 'Strange listening women lying children government distribution basis supreme power derives adequate ceremonies'\n",
    "\n",
    "for name, normalizer in normalizers:\n",
    "    print name, '\\t=\\t', [normalizer.stem(token) for token in tokenizer(test)], '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern_lemmatizer\t=\t[u'strange', u'listen', u'woman', u'lie', u'child', u'government', u'distribution', u'basis', u'supreme', u'power', u'derive', u'adequate', u'ceremoney']\n"
     ]
    }
   ],
   "source": [
    "#    Using pattern\n",
    "import pattern\n",
    "from pattern.en import parse\n",
    "\n",
    "#    Pattern is unnecessarily powerful at this point: its main output are parses,\n",
    "#    complete syntactic analysis. We still don't need that so we will implement a\n",
    "#    wrapper that returns the lemmas only:\n",
    "\n",
    "class PatternStemmer:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        tree = parse(text, lemmata=True)\n",
    "        return [\n",
    "            part[-1] for part in tree.split()[0]\n",
    "        ]\n",
    "\n",
    "stemmer = PatternStemmer()\n",
    "print 'pattern_lemmatizer\\t=\\t', stemmer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'strange', u'listen', u'woman', u'lie', u'child', u'government', u'distribution', u'basis', u'supreme', u'power', u'derive', u'adequate', u'ceremony']\n"
     ]
    }
   ],
   "source": [
    "#    Using spacy\n",
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "en_doc = en_nlp(test.decode('utf-8'))\n",
    "\n",
    "print [en_doc.vocab[x.lemma].orth_ for x in en_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__spacy__'s lemmas look pretty great, actually, and are the best of all systems (for this particular example). Although we have skipped it here, __spacy__, like __pattern__, provides lemmas integrated into the syntactic analysis, which as we saw earlier is the only way of being able to choose the correct lemma: lemmas can differ based on a word's part-of-speech, which in turn must be determined by looking at its context in the sentence.\n",
    "\n",
    "## Parts of speech\n",
    "Now is probably a good time to introduce [parts of speech](https://en.wikipedia.org/wiki/Part_of_speech) (PoS for short). The basic idea comes from linguistics and it actually goes back many centuries. Latin grammarians (!!) already described word behavior based on parts of speech, which simply means how a word (form) behaves within a sequence of words.\n",
    "\n",
    "To fully grasp parts of speech, it is better to think in terms of concepts:\n",
    "\n",
    "> _person, height_\n",
    "\n",
    "That is just a sequence of two ideas, two __nouns__. As such, there is no relationship between the two.\n",
    "\n",
    "At this point, language is still not very informative: we are just naming things and all speakers are supposed to know the names of most things anyway. Nouns are a prerequisite for linguistic behavior, but not really its end goal. Language starts to be really useful when it __adds knowledge__, when new information is added through combinations.\n",
    "\n",
    "We started with two ideas, _person_ and _height_ -what if there is a connection between the two? What if that person is tall? _Height_ now becomes an attribute of _person_ (both ideas go together), and language will now use an __adjective__ to express that the two ideas in the sequence are not independent but are actually connected. That is, when an idea specifies another idea, it becomes an adjective (or behaves like one, such as the genitive case in Ukrainian or _de_-preposition phrases in Spanish):\n",
    "\n",
    "> _tall person_\n",
    "\n",
    "So far, we have seen words or combinations of words, but we are still not conveying thoughts. That is, we can build any sequence of nouns and adjectives, as long as we want. The question is, what for? Or rather, where do we stop? The standard way to mark that a sequence of ideas is a closed, complete unit that we can then transmit, is through a __verb__:\n",
    "\n",
    "> _People grow (become taller until they reach adulthood)_    (~_stature_ sense of _height_)\n",
    "\n",
    "> _People climb_                            (_vertical movement_ sense of _height_)\n",
    "\n",
    "> _People (are) at the top_                 (_vertical_ position sense of _height_)\n",
    "\n",
    "> Etc.\n",
    "\n",
    "The examples above still involve the same concepts as before, yet only these last examples are valid sentences that we can find in a corpus, something with a truth value that can be regarded as an assertion about the world. Verbs turn potentially endless sequences of noun combinations into a bounded thought we can express.\n",
    "\n",
    "These are the main parts of speech, although there are a few others:\n",
    "* pronouns (placeholders for nouns),\n",
    "* adverbs (adjectives for verbs or for other adjectives),\n",
    "* prepositions (they create adverbs from nouns or noun phrases, including pronouns),\n",
    "* conjunctions (they create nouns, adjectives or adverbs from verbs and their nouns),\n",
    "* determiners and quantifiers (usually adjectives in origin, but they end up as an indispensable part of nouns, to the point that nouns cannot be used without some determiner. For example, countable nouns in the singular can almost never be used without a determiner: _The dog barked_ versus _*Dog barked_).\n",
    "\n",
    "The three first categories (noun, adjectives and verbs) are open classes: new words are being added every day (especially true for nouns, just think of all new movie and song titles, people names, software version names, etc.) The last four are usually closed classes and their elements tend to remain constant over very large time spans.\n",
    "\n",
    "This is a very broad and overly simplified overview of parts of speech. Since language is messy, there are often cases of unclear boundaries across categories, as well as many cases of ambiguous words intersecting several categories. We will go back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords (or rather, _high-frequency noise_)\n",
    "\n",
    "As it turns out, words belonging to closed classes are those that are usually considered __stop words__. In the field of Linguistics, the words in those classes are often called __function words__ because, rather than conveying meaning, they often perform some more abstract, functional role inside the sentence (that is, they replace open-class words or change their part of speech).\n",
    "\n",
    "In many NLP applications, it is not necessary to go too deep into the linguistic structure of a sentence, which means that the system does not need to understand stop words. Sometimes they even hurt performance: they have extremely high probabilities that can easily mislead a naïve statistical system.\n",
    "\n",
    "For this reason, stop words are very often removed as part of the text preprocessing step.\n",
    "\n",
    "\n",
    "### Long-tail distributions\n",
    "Natural language vocabulary is well-known for exhibiting a pattern corresponding to [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law), which is a subcase of the more general [long tail](https://en.wikipedia.org/wiki/Long_tail) statistical distribution (long-tail distributions [have a well-known shape](https://www.dropbox.com/s/ev0r95z8o9iahxc/Long%20tail.png?dl=0)), whereby a small number of outcomes have high frequency and a large number of outcomes have a very low frequency. In language, however, this will not be likely to result in a standard [power law](https://en.wikipedia.org/wiki/Power_law) distribution. Zipf went probably too far when he stated that the frequency of the word at rank _i_-th is twice that of the word at rank _i + 1_-th, but his general point still applies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 475891), (u'of', 263591), (u'and', 253408), (u'to', 210363), (u'a', 176384), (u'in', 148874), (u'is', 97747), (u'that', 85177), (u'for', 72867), (u's', 63172)]\n",
      "7507642.0\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from lib.TextStreamer import TextStreamer\n",
    "\n",
    "class ZipfsLaw(Counter):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def total(self):\n",
    "        return float(sum(zip(*self.most_common())[1]))\n",
    "    \n",
    "    def half(self):\n",
    "        acc = 0\n",
    "        t = self.total()\n",
    "        for i, (w, f) in enumerate(self.most_common()):\n",
    "            acc += f\n",
    "            if acc / t >= 0.5:\n",
    "                return i + 1\n",
    "        return i\n",
    "\n",
    "                \n",
    "path = '/Users/jordi/Laboratorio/corpora/raw/umbc/webbase_all/delorme.com_shu.pages_89.txt'\n",
    "strr = TextStreamer(path)\n",
    "i = 0\n",
    "zipf = ZipfsLaw()\n",
    "for record in strr:\n",
    "    zipf.update([w.lower() for w in tokenizer(record.strip()) if w.isalpha()])\n",
    "\n",
    "print zipf.most_common(10)\n",
    "print zipf.total()\n",
    "print zipf.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words in the top-10 above are __all__ stop words (function words). In Zipf's original experiments, 135 words were enough to account for half of the probability mass of the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus). We see that this number remains largely the same even today, and that the number is still very close in our case: 144 words account for half the probability mass of our dataset.\n",
    "\n",
    "In some way, there is a constant core of what makes a language the way it is, and we can expect that to be true of any dataset. Of course, this also means that any dataset will contain at least these words and, based on these words, any dataset of a language will \"match\" any other dataset of that language to some extent, making classification tasks hard (there is always some similarity between two English texts: English).\n",
    "\n",
    "This is the central idea behind applications like [__language detection__](https://en.wikipedia.org/wiki/Language_identification), but other NLP tasks aiming at a higher resolution need a way to ignore these common linguistic core.\n",
    "\n",
    "The first way anyone ever thought of doing that, was to create a list with those ~150 words (more or less) and use it as a blacklist for the system: any word in that list would be removed from the document's tokens, otherwise they stayed. As usual, we will rely on NLTK for a well-maintained list of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'one', 21721), (u'also', 13697), (u'time', 13692), (u'new', 13110), (u'work', 11086), (u'would', 11078), (u'first', 10841), (u'story', 10437), (u'like', 10389), (u'book', 10291)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import defaultdict as deft\n",
    "\n",
    "blacklist = deft(bool)\n",
    "for w in stopwords.words(u'english'):\n",
    "    blacklist[w] = True\n",
    "\n",
    "new_top_10 = [(w, f) for w, f in zipf.most_common() if not blacklist[w]][:10]\n",
    "print new_top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is definitely better (words such as _work, story_ or _book_ now have reached the top-10), but we still have a few words that do not tells us much: _one, time, new_ or _would_. These words can be considered high-frequency noise: words that may be actual words (rather than __function words__) strictly speaking, but that still convey too little information anyway to be helpful in our applications.\n",
    "\n",
    "However, we have already used the stopword-list trick. Now what? This is where things start to get interesting. \n",
    "\n",
    "### Dealing with high-frequency noise\n",
    "\n",
    "At some point, people realized that having to tell the system which words are important and which ones are not, is not only boring for us, but also does not say much of our NLP skills: is it not possible to find a way of automating this? As it turns out, there is :)\n",
    "\n",
    "One could argue that, if our goal is to develop a really smart machine, something that gets close to artificial intelligence in any meaningful sense,  then that kind of system should also be able to figure out on its own what is important from what is not (to at least a reasonable extent). Although that claim is a bit vague, we can make it more specific by saying that we want the system to be able to learn on its own at least which words convey too little information to be meaningful within a particular dataset.\n",
    "\n",
    "There are many ways of doing that but we will take a look at a few of the most effective: __background-foreground overlap, TF-IDF__, and __conditional probability__ (important because we will be using it in the context of classifiers like Naive Bayes).\n",
    "\n",
    "#### Background-foreground overlap\n",
    "The idea here is simple, we take a frequency distribution for one dataset, another frequency distribution for another dataset (totally independent, or as unrelated as possible), look at the top _n_ words in each distribution, and remove any items in the intersection. We will be left with much more informative content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise: 193 [u'the', u'of', u'to', u'in', u'and', u'said', u'a', u's', u'for', u'it']\n",
      "content: 307 [u'mln', u'vs', u'dlrs', u'pct', u'lt', u'cts', u'net', u'billion', u'loss', u'company']\n",
      "known stopwords: 153\n",
      "re-used stopwords: 91\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from collections import Counter\n",
    "from lib.Tools import tokenizer\n",
    "\n",
    "fg_sents = [tokenizer(record) for record in strr]\n",
    "fg_dist = Counter()\n",
    "for sent in fg_sents:\n",
    "    fg_dist.update([w.lower() for w in sent if w.isalpha()])\n",
    "\n",
    "bg_dist = Counter([w.lower() for w in reuters.words() if w.isalpha()])\n",
    "\n",
    "topn = 500\n",
    "\n",
    "bg_top = [w for w, _ in fg_dist.most_common(topn)]\n",
    "fg_top = [w for w, _ in bg_dist.most_common(topn)]\n",
    "\n",
    "noise = [w for w in fg_top if w in bg_top]\n",
    "content = [w for w in fg_top if w not in bg_top]\n",
    "\n",
    "print 'noise:', len(noise), noise[:10]\n",
    "print 'content:', len(content), content[:10]\n",
    "print 'known stopwords:', len([w for w, boolean in blacklist.items() if boolean])\n",
    "print 're-used stopwords:', len([w for w in fg_top if w in bg_top and blacklist[w]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF (mutual information) and conditional probability\n",
    "TFIDF stands for __term frequency * inverse document frequency__. The intuition behind it is that the most relevant words in any document will be the most frequent words in that document (because they appear all over the place: \"work, work, work\") that are, at the same time, the least frequent on the overall dataset (in the sense that they will be unique to the current document: if no or very few other documents speak about the same topic, that topic can be assumed to be very characteristic of that particular document). So, it is a mathematical formula of what is both __important (frequent) and specific (inversely frequent over all documents)__ about a given document. This type of information is usually called the most __salient__ information.\n",
    "\n",
    "TFIDF often works at the document level within the same dataset, so here the system in fact needs the dataset to be divided into documents (that means that TFIDF cannot be directly calculated for short text inputs such as tweets, or at least that it will not yield any meaningful improvements over raw frequency counts unless some other segmentation of the data can be provided, at the topic level for instance).\n",
    "\n",
    "Fortunately, the Reuters corpus is divided into documents so we can use it for our experiments. It is no coincidence ;)\n",
    "\n",
    "The TFIDF formula is quite popular in the field of Information Retrieval and it has been successfully applied in many NLP applications. It is often a standard measure of term relevance and you can hardly go wrong if you use it. Sometimes it seems that half of all NLP happens around the TFIDF formula :) (or some variation of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788\n",
      "top-5 by frequency:\t[(u'.', 62), (u'the', 32), (u'of', 30), (u'to', 26), (u',', 20)]\n",
      "top-5 by TF-IDF:\t[(u'Japan', 12), (u'trade', 13), (u'U', 19), (u'S', 19), (u'tariffs', 5)]\n",
      "\n",
      "top-5 by frequency:\t[(u',', 8), (u'.', 7), (u'and', 6), (u'of', 5), (u'to', 3)]\n",
      "top-5 by TF-IDF:\t[(u'preservation', 2), (u'China', 3), (u'waste', 2), (u'storage', 2), (u'vermin', 1)]\n",
      "\n",
      "top-5 by frequency:\t[(u'the', 11), (u',', 8), (u'of', 8), (u'energy', 6), (u'.', 6)]\n",
      "top-5 by TF-IDF:\t[(u'energy', 6), (u'MITI', 4), (u'demand', 4), (u'electric', 2), (u'supply', 3)]\n",
      "\n",
      "top-5 by frequency:\t[(u'.', 13), (u'pct', 10), (u',', 7), (u'billion', 6), (u'to', 5)]\n",
      "top-5 by TF-IDF:\t[(u'baht', 3), (u'pct', 10), (u'billion', 6), (u'Thailand', 2), (u'Janunary', 1)]\n",
      "\n",
      "top-5 by frequency:\t[(u',', 12), (u'to', 8), (u'.', 6), (u'of', 5), (u'CPO', 4)]\n",
      "top-5 by TF-IDF:\t[(u'CPO', 4), (u'Harahap', 3), (u'palm', 4), (u'Indonesia', 3), (u'Indonesian', 2)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict as deft\n",
    "\n",
    "\n",
    "#    The number of documents in our corpus:\n",
    "N = len(reuters.fileids())\n",
    "print N\n",
    "\n",
    "\n",
    "#    The TFIDF formula:\n",
    "def tfidf(word, frequency, document_frequencies):\n",
    "    tf = frequency\n",
    "    idf = math.log(N / float(document_frequencies[word]), 10)\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "#    First pass over the data to collect the document frequency for each word:\n",
    "document_frequencies = Counter()\n",
    "for i in reuters.fileids():\n",
    "    doc = reuters.words(i)\n",
    "    for w in set(doc): \n",
    "        document_frequencies[w] += 1\n",
    "\n",
    "        \n",
    "#    Second pass over the data to calculate the TFIDF scores for the words\n",
    "#    in each document:\n",
    "for i in list(reuters.fileids())[:5]:\n",
    "    doc = reuters.words(i)\n",
    "    counts = Counter(doc)\n",
    "    print 'top-5 by frequency:\\t', counts.most_common(5)\n",
    "    tfidf_weighted = sorted(\n",
    "        counts.most_common(),\n",
    "        key=lambda x: tfidf(x[0], x[1], document_frequencies),\n",
    "        reverse=True\n",
    "    )\n",
    "    print 'top-5 by TF-IDF:\\t', tfidf_weighted[:5]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional probability\n",
    "\n",
    "__Conditional probability__, on the other hand, is based on a slightly different idea: for conditional probability, the most important words in a document are not necessarily the most frequent ones.\n",
    "\n",
    "For conditional probability, what makes a word relevant is how much its probability changes in a given subsample of the data with respect to its overall probability over the entire dataset. So, rather than simply taking the most frequent words in a document that are not high-frequency noise (as TFIDF does), conditional probability focuses on the words whose probability changes the most, regardless of how frequent they are.\n",
    "\n",
    "Sometimes, this results in more informative features for our system: whereas TFIDF is really good at modelling topical information about a document, it may also overlook relevant specificities about that document. Let's see it with an example -the class ProbDist below will calculate for us both __TFIDF__ and __conditional probability__ scores for the same documents, and display the top-5 words according to each metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'trade']\n",
      "top-5 by frequency:\t[u'.', u'the', u'of', u'to', u',']\n",
      "top-5 by COND:\t[u'Unofficial', u'RIFT', u'Sheen', u'Mounting', u'inflict']\n",
      "top-5 by TFIDF:\t[u'Japan', u'trade', u'U', u'S', u'tariffs']\n",
      "\n",
      "[u'grain']\n",
      "top-5 by frequency:\t[u',', u'.', u'and', u'of', u'to']\n",
      "top-5 by COND:\t[u'vermin', u'EAT', u'VERMIN', u'additives', u'preservation']\n",
      "top-5 by TFIDF:\t[u'preservation', u'China', u'waste', u'storage', u'vermin']\n",
      "\n",
      "[u'crude', u'nat-gas']\n",
      "top-5 by frequency:\t[u'the', u',', u'of', u'energy', u'.']\n",
      "top-5 by COND:\t[u'kilolitres', u'kl', u'DOWNWARDS', u'REVISE', u'kilowatt']\n",
      "top-5 by TFIDF:\t[u'energy', u'MITI', u'demand', u'electric', u'supply']\n",
      "\n",
      "[u'corn', u'grain', u'rice', u'rubber', u'sugar', u'tin', u'trade']\n",
      "top-5 by frequency:\t[u'.', u'pct', u',', u'billion', u'to']\n",
      "top-5 by COND:\t[u'Janunary', u'pineapples', u'jewellery', u'registering', u'baht']\n",
      "top-5 by TFIDF:\t[u'baht', u'pct', u'billion', u'Thailand', u'Janunary']\n",
      "\n",
      "[u'palm-oil', u'veg-oil']\n",
      "top-5 by frequency:\t[u',', u'to', u'.', u'of', u'CPO']\n",
      "top-5 by COND:\t[u'Harahap', u'Hasrul', u'Sumatran', u'CPO', u'Moslem']\n",
      "top-5 by TFIDF:\t[u'CPO', u'Harahap', u'palm', u'Indonesia', u'Indonesian']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import (\n",
    "    Counter,\n",
    "    defaultdict as deft\n",
    ")\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "\n",
    "import nltk\n",
    "import math\n",
    "from collections import (\n",
    "    Counter,\n",
    "    defaultdict as deft\n",
    ")\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "\n",
    "class ProbDist:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.priors = Counter()\n",
    "        self.mass = 0.0\n",
    "        self.classf = deft(float)\n",
    "        self.posteriors = deft(Counter) \n",
    "        self.masses = deft(float)\n",
    "        self.idf = deft(float)\n",
    "        self.n = 0\n",
    "\n",
    "    def update(self, i, tokens):\n",
    "        l = len(tokens)\n",
    "        self.classf[i] += 1\n",
    "        self.posteriors[i].update(tokens)\n",
    "        self.masses[i] += l\n",
    "        self.priors.update(tokens)\n",
    "        self.mass += l\n",
    "        self.__idf_update(i, tokens)\n",
    "    \n",
    "    def __idf_update(self, i, tokens):\n",
    "        for w in set(tokens):\n",
    "            self.idf[w] += 1\n",
    "        self.n += 1\n",
    "    \n",
    "    def most_common(self, i, n=None):\n",
    "        return self.posteriors[i].most_common(n)\n",
    "    \n",
    "    def most_cond(self, i, n=None):\n",
    "        pxy = [(w, self.prob(w, i)) for w, f in self.posteriors[i].items()]\n",
    "        px_y = {w: self.prob(w) for w, _ in pxy}\n",
    "        conds = sorted(\n",
    "            [(w, (math.log((p / px_y[w]), 2))) for w, p in pxy],\n",
    "#             [(w, p * (p / px_y[w])) for w, p in pxy],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        if not n:\n",
    "            n = len(conds)\n",
    "        return conds[:n]\n",
    "    \n",
    "    def tfidf(self, i, w):\n",
    "        tf = self.posteriors[i][w]\n",
    "        idf = math.log(self.n / self.idf[w], 10)\n",
    "        return tf * idf\n",
    "    \n",
    "    def most_tfidf(self, i, n=None):\n",
    "        tfidfs = [\n",
    "            (w, self.tfidf(i, w))\n",
    "            for w, _ in self.posteriors[i].items()\n",
    "        ]\n",
    "        tfidfs.sort(key=lambda x: x[1], reverse=True)\n",
    "        if not n:\n",
    "            n = len(tfidfs)\n",
    "        return tfidfs[:n]\n",
    "               \n",
    "    def prob(self, w, i=None):\n",
    "        if i:\n",
    "            return self.posteriors[i][w] / self.masses[i]\n",
    "        else:\n",
    "            return self.priors[w] / self.mass\n",
    "    \n",
    "\n",
    "#    First pass over the data to collect prior probabilities:\n",
    "prob_dist = ProbDist()\n",
    "for i in reuters.fileids():\n",
    "    doc = reuters.words(i)\n",
    "    prob_dist.update(i, doc)\n",
    "\n",
    "        \n",
    "#    Second pass over the data and calculation of posterior probabilities:\n",
    "for i in list(reuters.fileids())[:5]:\n",
    "    print reuters.categories(i)\n",
    "    print 'top-5 by frequency:\\t', [w for w, _ in prob_dist.most_common(i, 5)]\n",
    "    print 'top-5 by COND:\\t', [w for w, _ in prob_dist.most_cond(i, 5)]\n",
    "    print 'top-5 by TFIDF:\\t', [w for w, _ in prob_dist.most_tfidf(i, 5)]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, __conditional probability__ scores higher slightly more specific (less frequent) words that are still relevant to the topic, such as _Sumatran_ in the last document (Sumatra being a part of Indonesia), _additives_ in the second one (definitely relevant in the context of food _preservation_) and _kilowatt, kilolitres_ and _kl_ in the third one, which all make sense in a document about energy production.\n",
    "\n",
    "As we can see, __TFIDF__ and __conditional probability__ complement each other nicely, in the sense that the former is good at describing the general topic of a document, whereas the latter is good at spotting strongly associated terms that can be used for a more fine-grained semantic analysis of the document's information.\n",
    "\n",
    "Incidentally, this is also good to spot potentially important features in our dataset. Remember how we said earlier that, sometimes, it is up to us to decide if our system should take numerical expressions into account or not? (the flight-booking website example). In these examples we are seeing the system trying to find out on its own :)\n",
    "\n",
    "The _kilowatt_ word (and a potential numerical expression nearby associated to it) may not be the most frequent word in a document about energy (and TFIDF may therefore tend to not see), but it is still very salient when measured using conditional probability (because, even if it appears only a few times in the document, it appears even more rarely in the entire dataset). That means that it is probably a good predictor of the class (every time the system finds the word _kilowatt_, it can classify the docuemnt as being about energy) and an equally good feature for our system.\n",
    "\n",
    "__Tip:__ it follows that, if we multiply the conditional probability of a word by its posterior probability in a document, we get a value which is nearly identical to its TFIDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOWs\n",
    "\n",
    "We have now reached a crucial point: after everything we have seen, we not only have a number of ways of breaking down a document into its constituent parts beyond simple tokens, but we also have metrics to approximate an idea of content relevance that is within the range of cognitive plausibility[1], what we as human perceive as relevant ourselves.\n",
    "\n",
    "This means we already have a granular, more discrete representation of a text in terms of dimensions that we can use to measure it, as well as quantitative measurements along those dimensions using the metrics we introduced. Put in other way: until now, all we had was an area. Now we have axes along the dimensions of that area that we can use to place coordinates and locate specific points inside that area.\n",
    "\n",
    "We are finally in a position to start working with text in a scientific, formal way. And if its formal, the a computer can understand it :)\n",
    "\n",
    "The representation that we have been introduced in the previous sections is known as the __Bag-of-Words (BOW)__ model. It is based on the metaphor of our space as a bag in which words are placed, just like [marbles](https://www.dropbox.com/s/bf62o5jljma6ezd/Marbles.jpg?dl=0) :D (Yep, illustrating that was absolutely necessary ;)\n",
    "\n",
    "Each document is a bag of marbles, sorry, __words__ :) and we can assess how similar two documents are by comparing the marbles they contain, sorry, __words__. Oversimplifying __a lot__, it is possible to say that this is the main idea behind most __search engines__, in terms of how retrieve documents from an index.\n",
    "\n",
    "Actually, let's try that ourselves: let's pick a random document from the Reuters corpus and let's display the top-5 most similar documents, to see if we just hacked our own home-made Google. First, we'll do it with simple Python tools, then we will learn the proper way of doing it ;)\n",
    "\n",
    "[1] __Plausible__: something that makes sense to a human observer, or something that gets results using a method that can be expected to be close to the actual phenomenon generating those results. We can explain a result of 2 as either _1 + 1_ or _(6 + 3) - (6 + 1)_. The first option is the most plausible (most reasonable). In this sense, the notion of plausibility has strong ties with notions like [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) and [theoretical elegance](https://en.wikipedia.org/wiki/Elegance):\n",
    "> The proof of a mathematical theorem exhibits mathematical elegance if it is surprisingly simple yet effective and constructive; similarly, a computer program or algorithm is elegant if it uses a small amount of code to great effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/1684\n",
      "PARTNERSHIP CUTS STAKE IN ERC INTERNATIONAL & lt ; ERC > Parsow Partnership Ltd , a Nevada investment partnership , said it lowered its stake in ERC International Inc to 343 , 500 shares or 8 . 3 pct of the total outstanding common stock , from 386 , 300 shares , or 9 . 3 pct . In a filing with the Securities and Exchange Commission , Parsow said it sold 42 , 800 ERC common shares between Jan 9 and March 2 at prices ranging from 12 . 125 to 14 . 50 dlrs each . The partnership\n",
      "[u'acq']\n",
      "---\n",
      "   training/1291\n",
      "   [u'acq']\n",
      "   (0.4199075338796443, [(u'partnership', 4.040207426151363), (u'shares', 2.8787677061818573), (u'dealings', 2.635000929108816), (u'investment', 2.533056181336908), (u'common', 2.3881836940871963)])\n",
      "   HARRIS CUTS STAKE IN BELL INDUSTRIES & lt ; BI > Harris Associates L . P ., a Chicago investment advisory limited partnership , said it lowered its stake in Bell Industries Inc to 1 , 015 , 800 shares , or 18 . 7 pct of the total outstanding , from 1 , 083 , 800 shares , or 20 . 0 pct . In a filing with the Securities and Exchange Commission , Harris said it sold 68 , 000 Bell common shares between Dec 18 and Feb 20 at prices ranging from 20 . 25 to 25\n",
      "   ----\n",
      "   training/14394\n",
      "   [u'acq']\n",
      "   (0.4034209611151166, [(u'Parsow', 7.463821884233743), (u'partnership', 4.040207426151363), (u'PARTNERSHIP', 2.9189975854740164), (u'shares', 2.8787677061818573), (u'Partnership', 2.690518256958647)])\n",
      "   PARTNERSHIP BUYS 5 . 1 PCT ORANGE - CO & lt ; OJ > STAKE Parsow Partnership Ltd , an Elkhorn , Neb ., investment partnership , said it bought a 5 . 1 pct stake in Orange - Co Inc common stock as an investment . In a filing with the Securities and Exchange Commission , Parsow said it had bought all its 220 , 000 Orange - Co shares in the open market with funds from its working capital . The partnership , whose sole general partner is Elkhorn investor Alan Parsow , said it \" has no\n",
      "   ----\n",
      "   training/13182\n",
      "   [u'acq']\n",
      "   (0.3906129804609507, [(u'shares', 2.8787677061818573), (u'investment', 2.533056181336908), (u'common', 2.3881836940871963), (u'lowered', 2.1298509507889096), (u'ranging', 2.0786984283415286)])\n",
      "   INVESTOR GROUP CUTS ROSPATCH & lt ; RPCH > STAKE A group of New York investors said it lowered its stake in Rospatch Corp to 170 , 250 shares , or 7 . 0 pct of the total outstanding common stock , from 202 , 108 shares , or 8 . 3 pct . In a filing with the Securities and Exchange Commission , the group said it sold a net 31 , 858 shares of Rospatch common stock between February 2 and March 31 at prices ranging from 21 . 50 to 25 . 13 dlrs a share .\n",
      "   ----\n",
      "=====\n",
      "\n",
      "training/1686\n",
      "IRANIAN OIL MINISTER DUE IN ALGERIA ON FRIDAY Iranian Oil Minister Gholamreza Aqazadeh is expected here on Friday for talks with his Algerian counterpart Belkacem Nabi , the official Algerian news agency APS said today . Aqazadeh , who will be accompanied by a large delegation , will have talks on bilateral relations in the field of energy and exchange views with Algerian officials on the current world energy situation , it said .\n",
      "[u'crude']\n",
      "---\n",
      "   training/2838\n",
      "   [u'crude']\n",
      "   (0.3991739805566007, [(u'Algerian', 9.09882281334256), (u'Aqazadeh', 6.259701901577819), (u'Nabi', 3.7319109421168717), (u'Belkacem', 3.7319109421168717), (u'APS', 3.3339709334448346)])\n",
      "   IRANIAN OIL MINISTER ARRIVES IN ALGERIA Iranian Oil Minister Gholamreza Aqazadeh arrived in Algiers at the head of a large delegation for talks on stabilizing oil prices , the official news agency APS said . In a brief arrival statement , he said Iran and Algeria were engaged in \" continuous and stronger cooperation \" on the world petroleum market and had \" deployed considerable efforts to stablise petroleum prices .\" He was greeted on arrival by Belkacem Nabi , the Algerian Minister of Energy , Chemical and Petro - Chemical Industries .\n",
      "   ----\n",
      "   test/15244\n",
      "   [u'crude']\n",
      "   (0.2378274496116361, [(u'Aqazadeh', 6.259701901577819), (u'Gholamreza', 3.1298509507889096), (u'delegation', 2.514426997902966), (u'Iranian', 2.1035220120665605), (u'agency', 1.8398163394263918)])\n",
      "   GHANA TO BUY CRUDE OIL FROM IRAN Ghana will import 15 , 000 tonnes of crude oil annually from Iran under an agreement reached in Tehran today , the Iranian news agency IRNA reported . The agency , received in London , said the accord was reached between Iranian Oil Minister Gholamreza Aqazadeh and a visiting Ghanaian delegation headed by Foreign Minister Obed Asamoah . IRNA said that under the agreement , Iran will also provide technical and scientific assistance in manpower training and oil exploitation , production and refining .\n",
      "   ----\n",
      "   training/3452\n",
      "   [u'crude']\n",
      "   (0.1918745373294754, [(u'Aqazadeh', 6.259701901577819), (u'Gholamreza', 3.1298509507889096), (u'talks', 2.972796548605445), (u'IRANIAN', 2.856849678725172), (u'MINISTER', 2.3086650681800642)])\n",
      "   IRANIAN OIL MINISTER IN UAE TALKS Iranian Oil Minister Gholamreza Aqazadeh is in the United Arab Emirates ( UAE ) to discuss oil prices and the general market situation , Iranian officials accompanying him said . He will meet UAE President Sheikh Zaid bin Sultan al - Nahayan and Oil Minister Mana Said al - Oteiba . Aqazadeh arrived last night after a brief stopover in Riyadh , where he met Saudi Arabia ' s Oil Minister Hisham Nazir . The official Saudi Press Agency quoted him as saying his talks at Riyadh with Nazir had been constructive and good\n",
      "   ----\n",
      "=====\n",
      "\n",
      "training/1681\n",
      "COLOMBIAN INFLATION RISES 2 . 03 PCT IN FEBRUARY colombia ' s cost of living index rose 2 . 03 pct in february after a 3 . 27 pct increase in january and a 3 . 15 pct rise in february 1986 , the government statistics institute said . The rise brought year - on - year inflation to 19 . 77 pct compared with 23 . 72 pct at end - february 1986 and 21 . 66 pct for the year ending january 1987 . The government has predicted that inflation this year would be lower than in 1986\n",
      "[u'cpi']\n",
      "---\n",
      "   training/13007\n",
      "   [u'cpi']\n",
      "   (0.6934782133401869, [(u'03', 4.083429724176717), (u'pct', 3.79497990038999), (u'COLOMBIAN', 3.430880946452891), (u'inflation', 3.357664997266905), (u'institute', 2.476638437013566)])\n",
      "   COLOMBIAN INFLATION STABLE AT AROUND 20 PCT Colombia ' s cost of living index rose 2 . 71 pct in March , after a 2 . 03 pct increase in February and a 2 . 21 pct rise in March 1986 , the government statistics institute said . The result brought year - on - year inflation to 20 . 36 pct compared with 22 . 65 pct at end - March 1986 and 19 . 77 pct for the year ending February 1987 . The government has predicted that inflation this year would be slightly lower than in 1986\n",
      "   ----\n",
      "   training/13172\n",
      "   [u'cpi']\n",
      "   (0.3616056665600782, [(u'february', 9.764369062191628), (u'pct', 3.79497990038999), (u'inflation', 3.357664997266905), (u'institute', 2.476638437013566), (u'rise', 2.38692578081331)])\n",
      "   PERU CONSUMER PRICES RISE 5 . 3 PCT IN MARCH peru ' s consuemr price index rose 5 . 3 pct last month to 13 , 914 . 4 ( base 100 ) following a 5 . 3 pct increase in february and 5 . 3 pct rise in march 1986 , the national statistics institute said . It said that the accumulated inflation for the first three months of 1987 was 18 . 5 pct compared to 15 . 4 pct for the same period last year . Inflation for the 12 - month period ending march 1987 was\n",
      "   ----\n",
      "   test/16415\n",
      "   [u'cpi']\n",
      "   (0.33654279236924234, [(u'pct', 3.79497990038999), (u'inflation', 3.357664997266905), (u'rise', 2.38692578081331), (u'year', 2.2857696439913395), (u'government', 2.2009343462074002)])\n",
      "   PORTUGUESE CONSUMER PRICES UP 1 . 4 PCT IN MARCH Portugal ' s consumer prices rose 1 . 4 pct in March after a one pct increase in February and a 1 . 2 pct rise in March 1986 , the National Statistics Institute said . The consumer price index ( base 100 for 1976 ) rose to 772 . 0 from 761 . 3 in February and compared with 703 . 4 in March 1986 . This gave a year - on - year March inflation rate of 9 . 8 pct against 9 . 5 pct in February\n",
      "   ----\n",
      "=====\n",
      "\n",
      "training/1680\n",
      "GATT SETS UP DISPUTE PANEL ON CANADIAN HERRING The ruling GATT Council set up a formal dispute panel to examine a U . S . Complaint that a Canadian law prohibiting export of unprocessed herring and salmon was discriminatory . David Wood , official spokesman of the General Agreement on Tariffs and Trade ( GATT ), told a news briefing the decision was taken after bilateral consultations failed to resolve the row . U . S . Ambassador Michael Samuels charged during the Council debate that Canada was trying to preserve domestic jobs by insisting that herring and salmon be\n",
      "[u'trade']\n",
      "---\n",
      "   training/1675\n",
      "   [u'trade']\n",
      "   (0.3144536864814365, [(u'GATT', 8.476508341588547), (u'panel', 5.0831584878931615), (u'dispute', 4.015270145032166), (u'Council', 3.7798522750535173), (u'Samuels', 3.430880946452891)])\n",
      "   GATT COUNCIL DEFERS DECISION ON SEMICONDUCTORS The ruling GATT Council deferred a decision on whether to set up a dispute panel on the basis of a European Community complaint against the U . S .- Japanese agreement on exports of computer semiconductors . David Woods , spokesman of the General Agreement on Tariffs and Trade ( GATT ), told a news briefing that the main parties would continue bilateral talks . This was in the hope of resolving the row before the next Council meeting on April 15 . The five - year accord signed in July 1986 aims to\n",
      "   ----\n",
      "   training/5518\n",
      "   [u'corn', u'grain']\n",
      "   (0.23417772668335612, [(u'GATT', 8.476508341588547), (u'dispute', 4.015270145032166), (u'Canada', 3.0583005094473448), (u'formal', 2.2405492482825995), (u'Tariffs', 2.0835509311359406)])\n",
      "   U . S . COULD COMPLAIN TO GATT ON CANADA CORN DUTY U . S . Trade Representative Clayton Yeutter suggested the U . S . could file a formal complaint with the General Agreement on Tariffs and Trade ( GATT ) challenging Canada ' s decision to impose duties on U . S . corn imports . Asked about the Canadian government decision to apply a duty of 84 . 9 cents per bushel on U . S . corn shipments , Yeutter said the U . S . could file a formal complaint with GATT under the dispute\n",
      "   ----\n",
      "   training/7907\n",
      "   [u'trade']\n",
      "   (0.21703681700346195, [(u'GATT', 8.476508341588547), (u'herring', 7.463821884233743), (u'was', 2.862470769500665), (u'processed', 2.731910942116872), (u'bilateral', 2.2847529107746527)])\n",
      "   JAPAN OPENS HOME MARKET TO U . S . FISH Japan has agreed to drop barriers to American - caught herring and pollock , opening the way for shipments that could reach 300 mln dlrs annually , U . S . Trade Representative Clayton Yeutter announced . Yeutter said the accord was reached after extensive bilateral negotiations that ended earlier today in Tokyo . He said the Commerce Department estimated U . S . shipments of processed pollock products and herring should rise to 85 mln dlrs this year and to more than 300 mln dlrs annually in later years\n",
      "   ----\n",
      "=====\n",
      "\n",
      "training/1682\n",
      "TALKING POINT / GENERAL MOTORS & lt ; GM > General Motors Corp staged an explosive rally on Wall Street after a share buyback program announced yesterday , but analysts said GM ' s future remains clouded by stiff competition and erosion of market share . GM shares rose 3 - 1 / 2 to 79 - 1 / 8 in active trading . Analysts agreed that investors liked the repurchase program but they differed sharply over the carmaker ' s long term prospects . \" I ' m very positive on GM ,\" said Jack Kirnan of Kidder Peabody\n",
      "[u'earn']\n",
      "---\n",
      "   test/15639\n",
      "   [u'iron-steel']\n",
      "   (0.32011428106217776, [(u'positive', 8.314793713366115), (u'share', 8.25037407394802), (u'1988', 7.84523974397473), (u'earnings', 7.741294838856932), (u'he', 7.7116471085265506)])\n",
      "   TALKING POINT / STEEL COMPANIES Steel stocks , which have had a healthy runup recently , still present some short term investment opportunities , according to some steel analysts . But others say the upturn , based on strong orders and firm prices this spring , has been strictly seasonal and will end soon . They recommend taking profits . \" It ' s that time of year . This is strictly seasonal ,\" said Charles Bradford of Merrill Lynch and Co . \" Orders will be strong for about two months , and there are signs that some (\n",
      "   ----\n",
      "   training/13115\n",
      "   [u'crude']\n",
      "   (0.2990265200374463, [(u'positive', 8.314793713366115), (u'share', 8.25037407394802), (u'1988', 7.84523974397473), (u'earnings', 7.741294838856932), (u'he', 7.7116471085265506)])\n",
      "   CANADA OIL INDUSTRY SET FOR RECOVERY - ANALYSTS Firmer crude oil prices , government aid packages and corporate spending cuts will help Canada ' s oil industry recover from last year sharp downturn , industry analysts said . They said there will be significant earnings gains in 1987 compared to last year ' s dismal results when oil prices dropped about 50 pct . On Canada ' s stock exchanges , energy shares have soared to their highest levels since 1983 , with many issues trading at record highs . \" This is reflecting a tremendous amount of optimism on\n",
      "   ----\n",
      "   training/5888\n",
      "   [u'copper']\n",
      "   (0.2980582096974305, [(u'1988', 7.84523974397473), (u'earnings', 7.741294838856932), (u'he', 7.7116471085265506), (u'will', 7.609131920267377), (u'analysts', 7.407454699039891)])\n",
      "   TALKING POINT / COPPER STOCKS Copper shares , which have lagged behind the market , should pick up steam this year on stronger demand and improving prices for the metal , analysts said . \" Copper prices should move up over the next four to six quarters ,\" said Daniel Roling of Merrill Lynch and Co & lt ; MER >. \" I see average copper prices at 70 cts a pound in 1988 , up from around 63 or 64 cts , based on tight supply and continued world economic growth .\" Other analysts see metal prices ranging from\n",
      "   ----\n",
      "=====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#    Home-made search engine using BoWs -the \"hack way\"\n",
    "\n",
    "#    First, we build the index of documents by their words\n",
    "#    and compute their BoWs at the same time:\n",
    "\n",
    "documents_by_word = deft(list)    # We will use an inverted index to speed up lookup\n",
    "\n",
    "bows = dict([])\n",
    "for i in reuters.fileids():\n",
    "    bow = dict([])\n",
    "    for word, weight in prob_dist.most_tfidf(i):\n",
    "        bow[word] = weight\n",
    "        documents_by_word[word].append(i)\n",
    "    bows[i] = bow\n",
    "\n",
    "\n",
    "#    After that, we iterate over the documents again\n",
    "#    and retrieve the most similar documents:\n",
    "\n",
    "#    A function to get all BoWs sharing some word\n",
    "#    with the input BoW:\n",
    "def documents_with_some_word_in_common(bow, top=None):\n",
    "    if not top:\n",
    "        docs = set([])\n",
    "        for w in bow.keys():\n",
    "            docs.update(documents_by_word[w])\n",
    "        return docs\n",
    "    else:\n",
    "        docs = Counter()\n",
    "        for w in bow.keys():\n",
    "            docs.update(documents_by_word[w])\n",
    "        return set([doc for doc, f in docs.most_common(top)])\n",
    "\n",
    "\n",
    "#    A function to remind us of the top-5 words\n",
    "#    by TFIDF in a particular BoW:\n",
    "def top5(bow):\n",
    "    return sorted(\n",
    "        bow.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "\n",
    "\n",
    "#    A function to calculate the similarity\n",
    "#    between two BoWs, for now simply as the\n",
    "#    ratio between the TFIDF of all words\n",
    "#    shared between the two, and the total\n",
    "#    TFIDF of all their words, averaged, and\n",
    "#    minus the average of difference between\n",
    "#    the two:\n",
    "def similarity(bow, i):\n",
    "    sim = 0.0\n",
    "    _sim = 0.0\n",
    "    _bow = bows[i]\n",
    "    shared_words = set(bow.keys()).intersection(set(_bow.keys()))\n",
    "    total_tfidf = sum(bow.values())\n",
    "    _total_tfidf = sum(_bow.values())\n",
    "    matches = []\n",
    "    for w in shared_words:\n",
    "        sim += bow[w] / total_tfidf\n",
    "        _sim += _bow[w] / _total_tfidf\n",
    "        matches.append((w, bow[w]))\n",
    "    return (((sim + _sim) / 2) - (abs(sim - _sim) / 2),\n",
    "            sorted(matches, key=lambda x: x[1], reverse=True)[:5])\n",
    "\n",
    "\n",
    "#    A function to see the first 20 words of a\n",
    "#    particular Reuters docuemnt:\n",
    "def txt(i):\n",
    "    return ' '.join(list(reuters.words(i))[:100])\n",
    "\n",
    "\n",
    "\n",
    "for i, bow in bows.items()[:5]:\n",
    "    candidate_bows = documents_with_some_word_in_common(bow)\n",
    "    sim_bows = sorted(\n",
    "        [(candidate, similarity(bow, candidate), txt(candidate))\n",
    "         for candidate in candidate_bows],\n",
    "        key=lambda x: x[1][0],\n",
    "        reverse=True\n",
    "    )\n",
    "    print i\n",
    "    print txt(i)\n",
    "    print reuters.categories(i)\n",
    "    print '---'\n",
    "    for candidate, sim, text in sim_bows[1:4]:\n",
    "        print '  ', candidate\n",
    "        print '  ', reuters.categories(candidate)\n",
    "        print '  ', sim\n",
    "        print '  ', text\n",
    "        print '  ', '----'\n",
    "    print '====='\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several cool things going on here:\n",
    "\n",
    "1. Highly-relevant, topically-related matches for the first group about company acquisitions.\n",
    "2. It goes deeper than that, though -the first example is also an instance of a thoroughly correct category prediction __just by adding up TFIDF weights over an inverted index__.\n",
    "3. The same considerations apply to the second and third groups, on the crude industry and the consumer price index, respectively.\n",
    "4. Something even cooler can be observed in the fourth group: a machine prediction that __goes beyond__ the human annotation, that is, __an actual instance of machine learning__ :) The original document is about trade disputes (_trade_ category), but the system is matching to two other documents in the _trade_ category __and__ a document belonging to the _corn, grain_ categories. When we take a look at the document, however, we can see it is in fact about **trade disputes in the _corn, gran_ industry**. The system has been able to make a correct inference and assign a new, correct category to the document, even when the human annotators did not do that themselves. This is a perfect example of bootstrapped/semi-supervised learning: we can first train an algorithm on some data annotated by people, and then use that system's decisions to easily add more data to our initial pool. Once a system gets to the point of being useful, it quickly becomes unstoppable :)\n",
    "5. And the cooler-yet result is hidden in the last group: at first, it seems a like a weird dump of different categories. All categories appear only once: the original document labeled as _earn_, the first suggestion as _iron-steel_, the others as _crude_ and _copper_. So, what happened here? Again, __machine learning happened__ :) If we pay attention to the documents' title, we can see that, in this group specifically, all dcuments start with the _TALKING POINT_ phrase. These documents are reports by analysts and feature prominently quotes of their statementsl. Although they have not been explicitly labeled as such in the Reuters corpus because in that corpus the tags are topic-based rather than genre-based, the algorithm does __sense__ something special and consistent about this group. If we look at the features triggering each match, it becomes evident that the pronoun _he_ is a constant, which makes sense because articles quoting analysts' opinions will tend to have a significant number of expressions like _he said, he claimed, he reported_. The system is able to pinpoint that on its own, and it is also interesting to think what would have happened if we had removed all stop words (_he_ is almost always considered one) -we would have lost one of the main predictors for the _talking point_ subset of our sample and would have probably never known it was there. Again, knowing the data and trying to minimize our assumptions on its nature proves to be crucial for modelling linguistic phenomena correctly.\n",
    "\n",
    "All things considered, we are already getting plausible results :) Or, as one of my favorite [role models](https://en.wikipedia.org/wiki/Ash_Williams) would, **_groovy_** results :) [1].\n",
    "\n",
    "[1]\n",
    "> fashionable and exciting: sporting a groovy new haircut.\n",
    "\n",
    "> enjoyable and excellent: he played all the remarkably groovy guitar parts himself.\n",
    "\n",
    "\n",
    "Ok, now let's stop playing and let's start taking things seriously. What is the proper way of doing that? The main difference is how the similarity is calculated (over all the dimensions in the dataset), and it involves __vectorization__, turning our BoWs into [__vector__](https://en.wikipedia.org/wiki/Vector)s. \n",
    "\n",
    "> in pattern recognition and machine learning, an n-dimensional vector of numerical features that represent some object\n",
    "\n",
    "Vectors are a very popular input representation and most algorithms expect it or can at least handle it. When you leave home, you should always take with you some code to vectorize your data :)\n",
    "\n",
    "The thing with vectors is, they can grow really big :D Particularly for high-dimensional data like natural languages, where each word is viewed as one dimension and that means that each vector will be __as long as the entire vocabulary__ in our dataset (often referred to as the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)), and most of its values will be __zeroes__ (often referred to as [data sparsity](https://en.wikipedia.org/wiki/Sparse_matrix))[1].\n",
    "\n",
    "[1] By the way, matrices of BoWs are usually called __vector space models__, which is a cool name and a buzz word you should try to throw around whenever you have a chance ;) To make it sound even more impressive, you can also add the adjective _semantic_ up to a grand total of __semantic vector space models__. Apart from the cool factor of the name, it also happens to be technically correct :) (since we are building as our model a space containing some number of vectors, and the information in those vector faithfully represents semantic information -the relevance we saw in earlier sections-).\n",
    "\n",
    "That is why more efficient representations are commonly used. However, vectors are still at the core of any such representation, which simply use some trick or other to transform the vector into a more manageable object __without information loss__. Things can get very complicated in this area and there is an entire field of [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) which is absolutely fascinating but totally beyond the scope of this course :D (For those who are really curious, you can check out (Random Forests/Ensemble Trees)[https://en.wikipedia.org/wiki/Random_forest] or (Principal Component Analysis)[https://en.wikipedia.org/wiki/Principal_component_analysis].\n",
    "\n",
    "To keep things simple, we will repeat the previous experiment using Python's standard Array object from the __numpy__ library. We will convert our BoWs into vectors, then calculate the similarity between documents using the __cosine similarity__. Wait, what?\n",
    "\n",
    "A detailed technical explanation of intuitions behind the [__cosine similarity__](https://en.wikipedia.org/wiki/Cosine_similarity) would take some time :) so, to keep things simple, let's just say that it calculates the ratio between the sum of the weights of any non-zero-weight dimensions shared by two vectors, and the combined total weights for all dimensions of those two vectors. Because of operating in an Euclidean space, the final calculation also involves the __dot product__ and the __square root__ (see below for details :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/1684\n",
      "PARTNERSHIP CUTS STAKE IN ERC INTERNATIONAL & lt ; ERC > Parsow Partnership Ltd , a Nevada investment partnership , said it lowered its stake in ERC International Inc to 343 , 500 shares or 8 . 3 pct of the total outstanding common stock , from 386 , 300 shares , or 9 . 3 pct . In a filing with the Securities and Exchange Commission , Parsow said it sold 42 , 800 ERC common shares between Jan 9 and March 2 at prices ranging from 12 . 125 to 14 . 50 dlrs each . The partnership\n",
      "[u'acq']\n",
      "---\n",
      "   test/20457\n",
      "   [u'acq']\n",
      "   0.615361879397\n",
      "   GROUP LIFTS STAKE IN SMITH INTERNATIONAL & lt ; SII > A group of firms led by Hong Kong - based Industrial Equity ( Pacific ) Ltd said it increased its stake in Smith International Inc common stock to 3 , 997 , 100 shares , or 17 . 5 pct of the total outstanding , from about 14 . 9 pct . In a filing with the Securities and Exchange Commission , the group said it bought 586 , 500 Smith common shares between October 9 and 19 at 7 . 86 dlrs to 9 . 57 dlrs a\n",
      "   ----\n",
      "   training/13182\n",
      "   [u'acq']\n",
      "   0.598163454214\n",
      "   INVESTOR GROUP CUTS ROSPATCH & lt ; RPCH > STAKE A group of New York investors said it lowered its stake in Rospatch Corp to 170 , 250 shares , or 7 . 0 pct of the total outstanding common stock , from 202 , 108 shares , or 8 . 3 pct . In a filing with the Securities and Exchange Commission , the group said it sold a net 31 , 858 shares of Rospatch common stock between February 2 and March 31 at prices ranging from 21 . 50 to 25 . 13 dlrs a share .\n",
      "   ----\n",
      "   training/12995\n",
      "   [u'acq']\n",
      "   0.59413981117\n",
      "   U . K . FIRM UPS ITALY FUND & lt ; ITA > STAKE TO 12 PCT Lloyds Investment Managers Ltd , a London - based investment firm , said it raised it stake in Italy Fund to 760 , 500 shares , or 12 . 0 pct of the total outstanding common stock , from 466 , 000 shares , or 7 . 4 pct . In a filing with the Securities and Exchange Commission , Lloyds said it bought the additional 294 , 500 Italy Fund common shares since November 7 for a total of 3 . 3\n",
      "   ----\n",
      "=====\n",
      "\n",
      "training/1686\n",
      "IRANIAN OIL MINISTER DUE IN ALGERIA ON FRIDAY Iranian Oil Minister Gholamreza Aqazadeh is expected here on Friday for talks with his Algerian counterpart Belkacem Nabi , the official Algerian news agency APS said today . Aqazadeh , who will be accompanied by a large delegation , will have talks on bilateral relations in the field of energy and exchange views with Algerian officials on the current world energy situation , it said .\n",
      "[u'crude']\n",
      "---\n",
      "   training/2838\n",
      "   [u'crude']\n",
      "   0.760602200722\n",
      "   IRANIAN OIL MINISTER ARRIVES IN ALGERIA Iranian Oil Minister Gholamreza Aqazadeh arrived in Algiers at the head of a large delegation for talks on stabilizing oil prices , the official news agency APS said . In a brief arrival statement , he said Iran and Algeria were engaged in \" continuous and stronger cooperation \" on the world petroleum market and had \" deployed considerable efforts to stablise petroleum prices .\" He was greeted on arrival by Belkacem Nabi , the Algerian Minister of Energy , Chemical and Petro - Chemical Industries .\n",
      "   ----\n",
      "   training/3452\n",
      "   [u'crude']\n",
      "   0.557981019389\n",
      "   IRANIAN OIL MINISTER IN UAE TALKS Iranian Oil Minister Gholamreza Aqazadeh is in the United Arab Emirates ( UAE ) to discuss oil prices and the general market situation , Iranian officials accompanying him said . He will meet UAE President Sheikh Zaid bin Sultan al - Nahayan and Oil Minister Mana Said al - Oteiba . Aqazadeh arrived last night after a brief stopover in Riyadh , where he met Saudi Arabia ' s Oil Minister Hisham Nazir . The official Saudi Press Agency quoted him as saying his talks at Riyadh with Nazir had been constructive and good\n",
      "   ----\n",
      "   test/20101\n",
      "   [u'crude', u'ship']\n",
      "   0.487882023915\n",
      "   IRAN PLANS TO FILE LAWSUITS OVER U . S . RAID Iran is preparing lawsuits to file for compensation from the U . S . Over the American raid on its Gulf oil platforms , Tehran radio quoted Iranian Oil Minister Gholamreza Aqazadeh as saying . The lawsuits would be filed with competent international bodies once the exact damage was calculated , he was quoted telling a news conference in Tehran . He earlier estimated the damage from the U . S . Raid at about 500 mln dlrs . The rigs , one of which was heavily shelled by\n",
      "   ----\n",
      "=====\n",
      "\n",
      "training/1681\n",
      "COLOMBIAN INFLATION RISES 2 . 03 PCT IN FEBRUARY colombia ' s cost of living index rose 2 . 03 pct in february after a 3 . 27 pct increase in january and a 3 . 15 pct rise in february 1986 , the government statistics institute said . The rise brought year - on - year inflation to 19 . 77 pct compared with 23 . 72 pct at end - february 1986 and 21 . 66 pct for the year ending january 1987 . The government has predicted that inflation this year would be lower than in 1986\n",
      "[u'cpi']\n",
      "---\n",
      "   training/13007\n",
      "   [u'cpi']\n",
      "   0.839181418946\n",
      "   COLOMBIAN INFLATION STABLE AT AROUND 20 PCT Colombia ' s cost of living index rose 2 . 71 pct in March , after a 2 . 03 pct increase in February and a 2 . 21 pct rise in March 1986 , the government statistics institute said . The result brought year - on - year inflation to 20 . 36 pct compared with 22 . 65 pct at end - March 1986 and 19 . 77 pct for the year ending February 1987 . The government has predicted that inflation this year would be slightly lower than in 1986\n",
      "   ----\n",
      "   training/8173\n",
      "   [u'gnp', u'jobs', u'retail', u'trade']\n",
      "   0.53056766275\n",
      "   FRENCH GDP SHOULD RISE 2 . 3 PCT IN 1988 - MINISTRY French gross domestic product should grow by 2 . 3 pct in 1988 after two pct growth this year and 2 . 1 pct in 1986 , the Finance Ministry said . The latest forecast , prepared by the National Accounts and Budget Commission , assumed an exchange rate of 6 . 20 francs to the dollar this year and next and an average oil import price rising to 18 . 9 dlrs a barrel next year from 17 . 4 dlrs this year and 14 . 7\n",
      "   ----\n",
      "   training/12466\n",
      "   [u'ipi']\n",
      "   0.45115959932\n",
      "   GERMAN INDUSTRIAL OUTPUT RISES 3 . 2 PCT IN FEBRUARY West German industrial production , seasonally adjusted , rose a provisional 3 . 2 pct in February after a downwards revised decline of 3 . 4 pct in January , the Economics Ministry said . The ministry had originally estimated that industrial production fell 3 . 0 pct in January . The February figure is likely to be revised upwards by just under one percentage point next month when the March figure is released , a ministry statement said . The industrial production index , base 1980 , stood at\n",
      "   ----\n",
      "=====\n",
      "\n",
      "training/1680\n",
      "GATT SETS UP DISPUTE PANEL ON CANADIAN HERRING The ruling GATT Council set up a formal dispute panel to examine a U . S . Complaint that a Canadian law prohibiting export of unprocessed herring and salmon was discriminatory . David Wood , official spokesman of the General Agreement on Tariffs and Trade ( GATT ), told a news briefing the decision was taken after bilateral consultations failed to resolve the row . U . S . Ambassador Michael Samuels charged during the Council debate that Canada was trying to preserve domestic jobs by insisting that herring and salmon be\n",
      "[u'trade']\n",
      "---\n",
      "   training/1675\n",
      "   [u'trade']\n",
      "   0.598473401754\n",
      "   GATT COUNCIL DEFERS DECISION ON SEMICONDUCTORS The ruling GATT Council deferred a decision on whether to set up a dispute panel on the basis of a European Community complaint against the U . S .- Japanese agreement on exports of computer semiconductors . David Woods , spokesman of the General Agreement on Tariffs and Trade ( GATT ), told a news briefing that the main parties would continue bilateral talks . This was in the hope of resolving the row before the next Council meeting on April 15 . The five - year accord signed in July 1986 aims to\n",
      "   ----\n",
      "   training/10767\n",
      "   [u'trade']\n",
      "   0.485853914374\n",
      "   NAKASONE SOUNDS CONCILIATORY NOTE IN CHIP DISPUTE Prime Minister Yasuhiro Nakasone sounded a conciliatory note in Japan ' s increasingly bitter row with the United States over trade in computer microchips . \" Japan wants to resolve the issue through consultations by explaining its stance thoroughly and correcting the points that need to be corrected ,\" he was quoted by Kyodo News Service as saying . While expressing regret over America ' s decision to impose tariffs on imports of Japanese electrical goods , Nakasone said Tokyo was willing to send a high - level official to Washington to help\n",
      "   ----\n",
      "   training/13908\n",
      "   [u'trade']\n",
      "   0.484401641265\n",
      "   U . S . CALLS FOR GREATER GATT CHECKS ON TRADE The United States has appealed for stronger powers for the General Agreement on Tariffs and Trade ( GATT ) to enforce world trade rules . The call by U . S . Deputy Trade Representative Michael Smith at a special committee meeting into the future of GATT follows a series of bilateral trade rows which have clouded efforts by the 93 - nation body to promote fair trade . Today ' s meeting was part of the Uruguay trade round launched by ministers last September . The round ,\n",
      "   ----\n",
      "=====\n",
      "\n",
      "training/1682\n",
      "TALKING POINT / GENERAL MOTORS & lt ; GM > General Motors Corp staged an explosive rally on Wall Street after a share buyback program announced yesterday , but analysts said GM ' s future remains clouded by stiff competition and erosion of market share . GM shares rose 3 - 1 / 2 to 79 - 1 / 8 in active trading . Analysts agreed that investors liked the repurchase program but they differed sharply over the carmaker ' s long term prospects . \" I ' m very positive on GM ,\" said Jack Kirnan of Kidder Peabody\n",
      "[u'earn']\n",
      "---\n",
      "   training/8098\n",
      "   [u'money-fx']\n",
      "   0.213486083772\n",
      "   U . S . CREDIT MARKET OUTLOOK - MINI - REFUNDING A hefty slice of new U . S . Treasury supply is not the most welcome prospect for a slumbering credit market , but at least this week ' s offerings should provide it with some focus , economists said . \" Banks and mutual funds have cash that should be put to work , so the auctions should breathe some life into the market ,\" said economists at Merrill Lynch Capital Markets Inc . The Treasury will place a 25 billion dlr package of two - year ,\n",
      "   ----\n",
      "   test/15639\n",
      "   [u'iron-steel']\n",
      "   0.21073869293\n",
      "   TALKING POINT / STEEL COMPANIES Steel stocks , which have had a healthy runup recently , still present some short term investment opportunities , according to some steel analysts . But others say the upturn , based on strong orders and firm prices this spring , has been strictly seasonal and will end soon . They recommend taking profits . \" It ' s that time of year . This is strictly seasonal ,\" said Charles Bradford of Merrill Lynch and Co . \" Orders will be strong for about two months , and there are signs that some (\n",
      "   ----\n",
      "   training/13115\n",
      "   [u'crude']\n",
      "   0.210007805523\n",
      "   CANADA OIL INDUSTRY SET FOR RECOVERY - ANALYSTS Firmer crude oil prices , government aid packages and corporate spending cuts will help Canada ' s oil industry recover from last year sharp downturn , industry analysts said . They said there will be significant earnings gains in 1987 compared to last year ' s dismal results when oil prices dropped about 50 pct . On Canada ' s stock exchanges , energy shares have soared to their highest levels since 1983 , with many issues trading at record highs . \" This is reflecting a tremendous amount of optimism on\n",
      "   ----\n",
      "=====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math, numpy, scipy\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "A = numpy.array\n",
    "\n",
    "\n",
    "#    Re-used from: http://stackoverflow.com/q/3121217\n",
    "#    Python's scipy or numpy implementations would usually be\n",
    "#    the way to go but they expected unnecessarily complicated\n",
    "#    wrapping objects as input and implementing the function\n",
    "#    to handle unwrapped vectors will be much easier in our case.\n",
    "def cosine_distance(u, v):\n",
    "    \"\"\"\n",
    "    Returns the cosine of the angle between vectors v and u. This is equal to\n",
    "    u.v / |u||v|.\n",
    "    \"\"\"\n",
    "    return numpy.dot(u, v) / (math.sqrt(numpy.dot(u, u)) * math.sqrt(numpy.dot(v, v))) \n",
    "cos = cosine_distance\n",
    "\n",
    "\n",
    "#   An object necessary to build our vector matrix,\n",
    "#   a mapping from words to their positional IDs in\n",
    "#   the vector space:\n",
    "dimensions = dict([])\n",
    "i = 0\n",
    "for w in set(reuters.words()):\n",
    "    dimensions[w] = i\n",
    "    i += 1\n",
    "\n",
    "D = len(dimensions.keys())\n",
    "\n",
    "\n",
    "#    Function to transform a our current BoW objects\n",
    "#    (Python dictionaries) into vectors:\n",
    "def bow2vec(bow):\n",
    "    vector = [0.1 for i in range(D)]\n",
    "    for w, weight in bow.items():\n",
    "#         if not known_words[w]:\n",
    "#             continue\n",
    "        word_index = dimensions[w]\n",
    "        vector[word_index] = weight\n",
    "#     print [x for x in vector if x]\n",
    "#     return A(vector).reshape(1, -1)\n",
    "    return A(vector)\n",
    "\n",
    "\n",
    "#    Home-made search engine using BoWs -the cool way (with vectors)\n",
    "for i, bow in bows.items()[:5]:\n",
    "    \n",
    "    candidate_bows = documents_with_some_word_in_common(bow, top=50)\n",
    "    \n",
    "    sim_bows = []\n",
    "    for candidate in list(candidate_bows):\n",
    "        vec1 = bow2vec(bow)\n",
    "        vec2 = bow2vec(bows[candidate])\n",
    "        sim = cos(vec1, vec2)\n",
    "        sim_bow = (candidate, sim, txt(candidate))\n",
    "        sim_bows.append(sim_bow)\n",
    "\n",
    "    sim_bows = sorted(\n",
    "        sim_bows,\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    print i\n",
    "    print txt(i)\n",
    "    print reuters.categories(i)\n",
    "    print '---'\n",
    "    for candidate, sim, text in sim_bows[1:4]:\n",
    "        print '  ', candidate\n",
    "        print '  ', reuters.categories(candidate)\n",
    "        print '  ', sim\n",
    "        print '  ', text\n",
    "        print '  ', '----'\n",
    "    print '====='\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks good :)\n",
    "\n",
    "1. The company acquisitions topic is again matched to several other documents in the same category effortlessly, by the very logic of their content.\n",
    "2. Same for the crude industry topic, which is also even more topically consistent, with all documents being now about news on Iran, just like the original news item.\n",
    "3. The group for the consumer prices index also seems semantically consistent but here it seems that our hacky way of solving this problem may have performed slightly better :) Or simply managed to stay closer to the manual annotation of the data for whatever other reason. In any case, both systems yield plausible results.\n",
    "4. Same for the last topic, the one about TALKING POINTs: again, cosine gets close to finding the hidden category but it would seem again that our home-made implementation has been a bit more sensitive to this particular variable.\n",
    "\n",
    "So, these are the foundations of the BoW/(semantic) vector space model as a representation of natural language inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We have features, we have BoWs, and we know how to turn them into vectors. It is time to put everything together and start predicting. Prediction is the key to automation: given all the previous pairs of _(X, Y)_ that our system has been trained on, the system must now be able to predict a correct _Y_ for a new, previously unobserved _X_.\n",
    "\n",
    "Prediction is performed by building a model: a weighted mapping from features to classes. The classifier will learn how well each feature predicts a given class (that is, how much weight it provides for the hypothesis that _X, therefore Y_).\n",
    "\n",
    "Classifiers differ in the way in which they compute the weights, that is, in which specific algorithm they use for weighting the features. There are many types of algorithms to do that but, in the field of NLP, some of the most important ones are [Naïve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifiers, [Maximum Entropy](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) (Logistic Regression) classifiers, and [Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine) classifiers.\n",
    "\n",
    "We are going to try something crazy and understand each of them in a few lines :) (by the way, we will be borrowing some ideas from [these guys](https://www.datarobot.com/blog/classification-with-scikit-learn/), who provide a really nice introduction to some of the concepts and is definitely worth reading).\n",
    "\n",
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.1393413772792607e-18, 'good_TV'), (2.1484723114408917e-20, 'internet_TV'), (8.789204910440013e-21, 'grown_up_TV'), (4.882891616911118e-22, 'bad_TV')] \n",
      "\n",
      "[(5.859469940293343e-22, 'grown_up_TV'), (1.4648674850733358e-22, 'good_TV'), (4.8828916169111193e-23, 'internet_TV'), (4.882891616911119e-23, 'bad_TV')] \n",
      "\n",
      "[(1.1718939880586683e-18, 'grown_up_TV'), (4.614332577981007e-19, 'bad_TV'), (9.765783233822236e-20, 'good_TV'), (3.255261077940746e-20, 'internet_TV')] \n",
      "\n",
      "[(1.845733031192403e-21, 'bad_TV'), (1.9531566467644475e-22, 'internet_TV'), (1.9531566467644475e-22, 'grown_up_TV'), (9.765783233822237e-23, 'good_TV')] \n",
      "\n",
      "[(9.118149042365925e-19, 'good_TV'), (7.16372284378108e-19, 'internet_TV'), (8.797994115350453e-20, 'grown_up_TV'), (2.4512115916893818e-21, 'bad_TV')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#    Naive Bayes\n",
    "\n",
    "#    Below are the features for our model -the known facts\n",
    "#    we have observed in the data.\n",
    "#\n",
    "#    These are the columns of each row in the list of \n",
    "#    'facts' :\n",
    "#\n",
    "#      [0] Name of the class\n",
    "#      [1] Stars at least one socially awkward geek\n",
    "#      [2] Stars doctors\n",
    "#      [3] Stars families of rich people\n",
    "#      [4] Stars dwarfs and dragons\n",
    "#      [5] Stars talking computers\n",
    "#      [6] Total shows\n",
    "#\n",
    "facts = [\n",
    "    ('good_TV', 40, 15, 10, 35, 50, 150),\n",
    "    ('grown_up_TV', 10, 60, 20, 9, 1, 100),\n",
    "    ('bad_TV', 5, 5, 189, 1, 1, 200),\n",
    "    ('internet_TV', 100, 5, 20, 22, 3, 300)\n",
    "]\n",
    "\n",
    "\n",
    "class Category:    \n",
    "    def __init__(self, row):\n",
    "        self.posteriors = []\n",
    "        self.frequencies = []\n",
    "        self.name = row[0]\n",
    "        self.mass = float(row[-1])\n",
    "        for value in row[1:-1]:\n",
    "            self.posteriors.append(value / self.mass)\n",
    "            self.frequencies.append(value)\n",
    "    \n",
    "    def __call__(self, features_in_show):\n",
    "        probs = [\n",
    "            self.posteriors[i] * feature\n",
    "            for i, feature in enumerate(features_in_show)\n",
    "        ]\n",
    "        if not probs:\n",
    "            return 0.0\n",
    "        prob = probs[0] + (10 ** -4)\n",
    "        for _prob in probs[1:]:\n",
    "            if _prob:\n",
    "                prob *= _prob\n",
    "            else:                        #   Simple additive smoothing to avoid zero-multipli-\n",
    "                prob *= (10 ** -4)       #   cations, which would cancel out our probability \n",
    "        return prob                      #   estimates. This is a well-known drawback of Naïve\n",
    "                                         #   Bayes and there are a number of techniques for \n",
    "                                         #   performing statistical smoothing:\n",
    "                                         #   https://en.wikipedia.org/wiki/Smoothing\n",
    "        \n",
    "class Model:\n",
    "\n",
    "    def __init__(self, facts):\n",
    "        self.categories = [Category(row) for row in facts]\n",
    "        self.mass = float(sum(c.mass for c in self.categories))\n",
    "        self.class_priors = {\n",
    "            i: c.mass / self.mass for i, c in enumerate(self.categories)\n",
    "        }\n",
    "        self.feature_priors = [0.1 for feature in facts[0][1:-1]]\n",
    "\n",
    "        for c in self.categories:\n",
    "            for i, value in enumerate(c.frequencies[1:-1]):\n",
    "                self.feature_priors[i] += value\n",
    "\n",
    "\n",
    "    def prior(self):\n",
    "        prob = self.feature_priors[0]\n",
    "        for _prob in self.feature_priors[1:]:\n",
    "            prob *= _prob\n",
    "        return prob\n",
    "        \n",
    "                \n",
    "    def __call__(self, new_show):\n",
    "        features_in_show = new_show[1:-1]\n",
    "        hypotheses = []\n",
    "        for i, c in enumerate(self.categories):\n",
    "            pxy = c(features_in_show) * self.class_priors[i]\n",
    "            px_y = self.prior()     # This part does not really affect the calculations\n",
    "            p = pxy / px_y          # because it remains constant for all classes.\n",
    "            hypotheses.append((p, c.name))\n",
    "        return sorted(hypotheses, reverse=True)\n",
    "    \n",
    "\n",
    "#\n",
    "#     [1] Stars at least one socially awkward geek\n",
    "#     [2] Stars doctors\n",
    "#     [3] Stars families of rich people\n",
    "#     [4] Stars dwarfs and dragons\n",
    "#     [5] Stars talking computers\n",
    "#     [6] Total shows\n",
    "#\n",
    "m = Model(facts)\n",
    "print m((None, 0, 0, 0, 1, 1, None)), '\\n'\n",
    "print m((None, 0, 1, 0, 0, 0, None)), '\\n'\n",
    "print m((None, 0, 1, 1, 0, 0, None)), '\\n'\n",
    "print m((None, 0, 0, 1, 0, 0, None)), '\\n'\n",
    "print m((None, 1, 0, 0, 1, 0, None)), '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The independence assumption\n",
    "Why is __Naïve Bayes__ called \"naïve\"? Because the model makes an assumption that is usually too well-intentioned :) Naïve Bayes models work on the feature __independence assumption__, which is, by the way, the mean reason why they are simple and fast, but also the main reason why so many people wonder how they can perform so well most of the time :)\n",
    "\n",
    "In NLP, the __independence assumption__ translates into the (very strong) assumption that the probabilities of each word in the document are independent from the presence of any other word in that document. That is, Naïve Bayes calculates the probability of words given the category of the documents they appear in (for instance, the fact that the word _politician_ appears more frequently in texts about politics), but it does not take into account the probability of a word given some other word (for instance, the fact that _politician_ and _ministry_ tend to appear together, or __co-ocurr__,  using the technical term, which stands for \"occurring/appearing together\").\n",
    "\n",
    "For Naïve Bayes, this means that two correlated words will each contribute to the overall score of the class. For the example above, _politician_ and _ministry_, that seems fine, though, since a text containing two words associated with the politics category should get some score from each separate word anyway. Independence seems fine in this case.\n",
    "\n",
    "When is it not okay? In NLP, the __independence assumption__ can be particularly harmful with __collocations__ and __multiwords__. We will see these units in more detail in a bit, for now let's just say they are terms consisting of several tokens, such as _Barak H. Obama, president Barack Obama, cafe latte_ or _Lviv Computer Science Summer School_.\n",
    "\n",
    "From Naïve Bayes naïve perspective, each of the tokens in each of these words would add a proportional probability. That means that documents _d1_ and _d2_ below would both receive the same probability \n",
    "\n",
    "> training_doc1 = {president, Barack, Obama, trade, deal}\n",
    "\n",
    "> training_doc2 = {Obama, war, Iran}\n",
    "\n",
    "> ------\n",
    "\n",
    "> new_doc = {president, Barack, Obama, war, Iran}\n",
    "\n",
    "> ------\n",
    "\n",
    "> training_doc1 ∩ new_doc = 3 / 5\n",
    "\n",
    "> training_doc2 ∩ new_doc = 3 / 5\n",
    "\n",
    "despite the fact that 'training_doc2' is clearly the most relevant given the new document. To summarize, the __independence assumption__ may cause result in false positives due to awarding free (too generous) hits for tokens that are really parts of just one word. In NLP, this is a direct consequence of work-in-progress tokenization (now you see the importance of all that boring stuff about tokenization I told you about earlier ;) and how Computational Linguistics can make things easier for Machine Learning -__this__ is NLP). What this means is that not everything you can split on a whitespace is a word (and the fact that the opposite is also true is a *__well-known__ fact* ;), and sometimes the actual linguistic unit (the thing whose probability you should be calculating) will be something bigger, sometimes spanning a few words and sometimes even *discontinuous*, which makes it necessary to *__take__ these things __into account__*.\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "On the other hand, __Logistic Regression__ or __Maximum Entropy__ (usually abbreviated as __MaxEnt__) classifiers work by finding linear correlations between features (also called __independent variable__s, or __predictors__) and classes (also known as __dependent variable__s). Linear regression is the type of analysis that can tell you if people tend to have increasing amounts of ice-cream as temperatures rise towards the summer.\n",
    "\n",
    "The key difference of logistic regression with respect to standard linear regressions is that, whereas linear regression gives us a coefficient (how associated the independent and dependent variables are, with a value ranging from -1 to +1), logistic regression gives us a __probability-looking normalized score__ (a floating point number between 0. and 1.0) that we can use as an actual probability of the class (dependent variable) given the feature (independent variable). Zero means the feature does not predict the class, one means the feature predicts the class with absolute certainty. The normalization of this score is performed using the [__logistic function__](https://en.wikipedia.org/wiki/Logistic_function) (a type of [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)) hence the technique's name.\n",
    "\n",
    "The math behind logistic regression is a bit complicated and beyond the focus of this course. For convenience, below is some code ([via StackOverlow](http://stackoverflow.com/a/7939259)) with a recap of linear regression. For our purposes, imagine that we replaced [a] temperatures over a period of time and [b] ice-cream in our example above, with [a'] words in a document and [b'] document categories, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics 0.57735026919\n",
      "sports -0.57735026919\n"
     ]
    }
   ],
   "source": [
    "facts = [\n",
    "    # category, 'government', 'politician', 'player', 'team'\n",
    "    ('politics', [1, 1, 0, 0]),\n",
    "    ('politics', [0, 1, 0, 0]),\n",
    "    ('politics', [1, 0, 0, 0]),\n",
    "    ('sports', [0, 0, 1, 1]),\n",
    "    ('sports', [0, 0, 0, 1]),\n",
    "    ('sports', [0, 0, 1, 0]),\n",
    "]\n",
    "\n",
    "categories = [\n",
    "    ('politics', [2, 2, 0, 0]),\n",
    "    ('sports', [0, 0, 2, 2])\n",
    "]\n",
    "\n",
    "def average(x):\n",
    "    assert len(x) > 0\n",
    "    return float(sum(x)) / len(x)\n",
    "\n",
    "def correlation(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    n = len(x)\n",
    "    assert n > 0\n",
    "    avg_x = average(x)\n",
    "    avg_y = average(y)\n",
    "    diffprod = 0\n",
    "    xdiff2 = 0\n",
    "    ydiff2 = 0\n",
    "    for idx in range(n):\n",
    "        xdiff = x[idx] - avg_x\n",
    "        ydiff = y[idx] - avg_y\n",
    "        diffprod += xdiff * ydiff\n",
    "        xdiff2 += xdiff * xdiff\n",
    "        ydiff2 += ydiff * ydiff\n",
    "    return diffprod / math.sqrt(xdiff2 * ydiff2)\n",
    "\n",
    "new_doc = [1, 0, 0, 0]\n",
    "\n",
    "for name, vector in categories:\n",
    "    print name, correlation(vector, new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.640457475681\n",
      "0.359542524319\n"
     ]
    }
   ],
   "source": [
    "#    And then the logistic function:\n",
    "def logistic_function(v):\n",
    "    return 1 / (1 + (math.e ** -v))\n",
    "\n",
    "print logistic_function(0.57735026919)\n",
    "print logistic_function(-0.57735026919)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we just implemented an again-grossly-oversimplified Maximum Entropy classifier :)\n",
    "\n",
    "#### Back to the naïveté of correlations\n",
    "The key advantage of __Logistic Regression__ over __Naïve Bayes__ lies in the former's ability to handle correlated features (check out this [thread on Quora](https://www.quora.com/What-is-the-difference-between-logistic-regression-and-Naive-Bayes) for details). That is, Logistic Regression __does not make the independence assumption__ and is able to deal feature correlations naturally by assigning them lower weights. In this way, a several-word-long linguistic unit no longer contributes to the final score for each of the tokens in it.\n",
    "\n",
    "More specifically, Naïve Bayes calculates a probability for each word and finally multiplies the probabilities for all words, whereas Logistic Regression learns the weight for each word relative to the weights of rest of the words and, therefore, assigns to each its individual contribution over the maximum contribution that they all add up to.\n",
    "\n",
    "That means that,iInstead of three words, each with an independent probability of 1.0 of belonging to the multi-word _president Barack Obama_, we now have three words, each with a ~0.33 probability of belonging to that multi-word, which makes far much more sense because the probability mass now sums up to 1.0 instead of 3.0.\n",
    "\n",
    "\n",
    "\n",
    "## Support Vector Machines\n",
    "__Support Vector Machines__ (SVMs for short), finally, are a powerful algorithm with occasionally slightly crazy training times. We mention them here because they are robust and very successful for some tasks, although their performance varies widely with the particular task. Essentially, they perform the same type of analysis as Logistic Regression classifiers (if we are talking about linear SVMs) with some differences in what is considered or high or low correlation.\n",
    "\n",
    "In practice, Naïve Bayes often performs just as well as SVM classifiers (or better, despite their much higher simplicity) and Logistic Regression usually outperforms both. In my own experience, Maximum Entropy is usually the way to go.\n",
    "\n",
    "The main factor to look for when deciding when to use SVMs is the dimensionality of our space: SVMs can handle high numbers of dimensions but they usually cannot handle the multi-dimensionality of NLP data (at least, no without some serious normalization and dimensionality reduction, which are always possible).\n",
    "\n",
    "That is the main reason why they will probably not be very useful unless you have some way to carry out a massive clean-up and normalization of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual classifiers and actual data\n",
    "\n",
    "We have everything we need: features, labels, BoWs, a vector space model, and classification algorithms to learn a mapping from the features to the labels. It is time to start doing some actual kick-ass prediction.\n",
    "\n",
    "Let's put all the pieces together and go back down to the code once again:\n",
    "\n",
    "### A dataset please\n",
    "\n",
    "Or more than one if possible :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:03, 144.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 0 449 51 0.49 3.46\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 1 449 51 0.57 3.38\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 2 449 51 0.45 3.48\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 3 449 51 0.55 3.2\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 4 449 51 0.59 3.44\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 5 449 51 0.51 3.36\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 6 444 50 0.48 3.08\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 7 444 50 0.54 3.17\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None 8 446 48 0.48 3.04\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "500it [00:02, 174.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lr None 9 355 33 0.52 1.98\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> lr None run 4383 487 0.518 3.159\n",
      "--------------\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 0 449 51 0.2 0.97\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 1 449 51 0.2 0.99\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 2 449 51 0.25 0.98\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 3 449 51 0.2 1.02\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 4 449 51 0.25 1.07\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 5 449 51 0.25 1.08\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 6 444 50 0.24 1.06\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 7 444 50 0.24 0.93\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None 8 446 48 0.27 1.07\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10788it [00:03, 3015.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mnb None 9 355 33 0.33 0.89\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> mnb None run 4383 487 0.243 1.006\n",
      "--------------\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> lr None 0 9656 1132 0.85 14.16\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> lr None 1 9656 1069 0.9 14.86\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> lr None 2 9656 1069 0.9 13.8\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'>"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters, brown\n",
    "\n",
    "#    We will re-use the SimpleCorpusReader class we saw when introducing\n",
    "#    our datasets, and extend it with .categories(), .words(), etc. meth-\n",
    "#    ods like those in NLTK's CorpusReaders so that we can use them with-\n",
    "#    in the same training and testing workflow:\n",
    "from lib.TwentyNewsgroupsCorpusWrapper import TwentyNewsgroupsCorpusWrapper as twenty_newsgroups\n",
    "\n",
    "datasets = [brown, reuters, twenty_newsgroups()]\n",
    "\n",
    "\n",
    "#    We have also implemented a wrapper class 'Classifier' that gives us\n",
    "#    easy and consistent access to scikit-learn's classification algori-\n",
    "#    thms:\n",
    "from lib.Classifier import Classifier\n",
    "\n",
    "#    Although NLTK's datasets usually come with pre-defined train and test\n",
    "#    splits of the data, in our experiments we will ignore that distinct-\n",
    "#    ion and we will be performing cross-validation. When cross-validating,\n",
    "#    the ratio between training and testing data is observed (for instance,\n",
    "#    8 training instances for every 2 test instances) but combining diffe-\n",
    "#    rent parts of the corpus: in the 1st cross-validation fold, the first\n",
    "#    20% of the dataset is used as training and the remaining 80% for tes-\n",
    "#    ting; in the 2nd cross-validation fold, testing is performed on the\n",
    "#    21-40% of the dataset, and training on the remaining 1-20% + 41-100%,\n",
    "#    and so on. Cross-validation is preferable as an evaluation methodolo-\n",
    "#    gy because it is far more robust. Results that generalize well to all\n",
    "#    subsets of our dataset will probably perform well on new data.\n",
    "#\n",
    "#    For convenience, we have also implemented a cross-validation wrapper\n",
    "#    to take care of the experimental design for us:\n",
    "from lib.CrossValidator import CrossValidator\n",
    "\n",
    "lr = Classifier(\n",
    "    classifier='lr',\n",
    ")\n",
    "\n",
    "nb = Classifier(\n",
    "    classifier='mnb',\n",
    ")\n",
    "\n",
    "clfs = [lr, nb]\n",
    "\n",
    "\n",
    "#    Experimental workflow:\n",
    "for dataset in datasets:\n",
    "    for clf in clfs:\n",
    "        c = CrossValidator(clf, dataset, train_r=0.9)\n",
    "        c.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These results are preliminary but very exciting nevertheless. Just to give you a frame of reference, below is a summary of some state-of-the-art results for classification on these corpora (some of these benchmarks are a bit old but I have not been able to find any more recents papers reporting better scores -which definitely does not mean they do not exist):\n",
    "\n",
    "* [This 2015 paper](https://www.researchgate.net/profile/Giacomo_Domeniconi/publication/281743455_A_Study_on_Term_Weighting_for_Text_Categorization_A_Novel_Supervised_Variant_of_tfidf/links/55f6ad7708aeafc8abf4f470.pdf) reports a maximum accuracy of nearly 94% and average accuracies of 92% on the Reuters corpus using __SVMs, and a number of variations of TFIDF as the feature weighting function, and feature selection__. The same configuration yielded scores in the mid-70% range on the 20 Newsgroups dataset. In our cases, __a barely normalized BoW-based vector space model combined with a Logistic Regression classifier__ allowed us to come very close of their top performance for Reuters (within 3-4%) and to outperform by ~10% their results on the 20 Newsgroups dataset. This is the kind of thing why we said earlier that Logistic Regression is usually the way to go ;)\n",
    "\n",
    "* On the Brown corpus, one of the top accuracies reported is [a 2010 52%-result](http://www.aclweb.org/anthology/P10-1077) (over all categories, so quite low but definitely better than random -because you have many more categories than just two, which would amount to a 50% random baseline assuming a uniform distribution-). Our out-of-the box, nearly unmodified classifier matched that performance effortlessly, despite the fact that the authors of that paper were using more advanced features. Again, Logistic Regression can be seen to account for the difference (they used Linear Discriminant Analyis, a precursor to Logistic Regression).\n",
    "\n",
    "* Again on the 20 Newsgroups classifiers, the [Stanford team](http://nlp.stanford.edu) (in many ways, the founders of Natural Language Processing and modern Computational Linguistics) [report a nearly 82% accuracy](http://nlp.stanford.edu/wiki/Software/Classifier/20_Newsgroups) using their classifier. They report some previous results performing up to the ~82% mark, but they also report as \"suspect\" an earlier result achieving nearly 85% accuracy. Our own results suggest this last result actually seemed on the right track :) (for obvious reasons, Stanford people do not like other people getting better accuracies ;)\n",
    "\n",
    "\n",
    "##### A note on evaluation metrics\n",
    "In these experiments, evaluation is being done in terms of [__Accuracy__](https://en.wikipedia.org/wiki/Accuracy_and_precision). Other widely-user important metrics are [__Precision__ and __Recall__](https://en.wikipedia.org/wiki/Precision_and_recall) you should be familiar with.\n",
    "\n",
    "In NLP, a simplified definition of __accuracy__ is used, not its stronger, technical one. Technically speaking, accuracy refers to how consistent a system's measurements are over time. A system __could be accurate while at the same time delivering really poor performance__ (as long as it delivers consistently poor performance :)\n",
    "\n",
    "In NLP, results consistency over time is generally measured as __macro-averaged precision__ over several experimental runs, for instance, and __accuracy__ is used as a synonym of precision when precision and recall converge. That is, all three terms can generally be used as equivalent when our task is such that every _X_ has a corresponding _Y_ and the system can either assign the right _Y_ or not, but always assigns one. I will make it clearer in a minute, stay with me ;)\n",
    "\n",
    "Let's imagine you are pub-questing around Lviv (it could happen ;) Let's imagine the crazy organizers ask you to go to the Opera House and convince 5 Ukrainians of singing the first act of [Rigoletto](https://en.wikipedia.org/wiki/Rigoletto) with you.\n",
    "\n",
    "Let's assume you get 3 people to do it (by bribing them with ice-cream, for instance ;) but let's also assume there are 3 other people who are not really Ukrainian but have heard you saying you will buy them ice-cream and that they really, really want the ice-cream. Two of them totally look like tourists so you can easily tell them to go away but the third one does look like he might be Ukrainian and you absolutely want to win the pub quest so you decide to risk it.\n",
    "\n",
    "In this case, your Precision is 2/3 and your Recall is 2/5. The number of actual Ukrainians who joined you (2) is called __true positives__, the number of false non-Ukrainians who joined (1) you is called __false positives__, the people you are missing to achieve the goal (3) are called __false negatives__, and the number of people you managed not to get fooled into buying an ice-cream (2) to is called _true negatives_.\n",
    "\n",
    "On the other hand, when you are a teacher you get to tell your students to either sing Rigoletto or go home with a note for their parents. Since they will all have to make a decision, __there can be no false negatives__ (i.e., any student who is not classified into singing or not singing); since you cannot force other teacher's students to sing Rigoletto, __there cannot be any false positives__, either; and since the choice is an a/b choice (a dichotomy), the boolean values __true__ and __false__ already account for all the variation, which is why __only Precision applies__ and, as we said, it is used as a synonym of __Accuracy__ in this particular case.\n",
    "\n",
    "Usually, Precision and Recall are reported using a combined score, their [__harmonic mean or F-1 score__](https://en.wikipedia.org/wiki/F1_score).\n",
    "\n",
    "\n",
    "## Experiments with a custom __Feature Extractor__\n",
    "\n",
    "__NOTE__: don't try this at home! :) There seems to be some problem with the implementation of the classifier in the _wlp_ repository and I have not been able to figure out what it is yet. As a result, the Naïve Bayes results are incorrectly low. However, that does not seem to be affecting the feature extraction part because, as far as I am concerned, it is behaving in the usual way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters, brown\n",
    "\n",
    "NGRAMS = [\n",
    "    (1, False),\n",
    "    (2, False),\n",
    "    (3, True)\n",
    "]\n",
    "\n",
    "#    We will re-use the SimpleCorpusReader class we saw when introducing\n",
    "#    our datasets, and extend it with .categories(), .words(), etc. meth-\n",
    "#    ods like those in NLTK's CorpusReaders so that we can use them with-\n",
    "#    in the same training and testing workflow:\n",
    "from lib.TwentyNewsgroupsCorpusWrapper import TwentyNewsgroupsCorpusWrapper as twenty_newsgroups\n",
    "\n",
    "datasets = [brown, reuters, twenty_newsgroups()]\n",
    "datasets = [brown, reuters]\n",
    "\n",
    "\n",
    "#    We have also implemented a wrapper class 'Classifier' that gives us\n",
    "#    easy and consistent access to scikit-learn's classification algori-\n",
    "#    thms:\n",
    "from lib.Classifier import Classifier\n",
    "\n",
    "#    Although NLTK's datasets usually come with pre-defined train and test\n",
    "#    splits of the data, in our experiments we will ignore that distinct-\n",
    "#    ion and we will be performing cross-validation. When cross-validating,\n",
    "#    the ratio between training and testing data is observed (for instance,\n",
    "#    8 training instances for every 2 test instances) but combining diffe-\n",
    "#    rent parts of the corpus: in the 1st cross-validation fold, the first\n",
    "#    20% of the dataset is used as training and the remaining 80% for tes-\n",
    "#    ting; in the 2nd cross-validation fold, testing is performed on the\n",
    "#    21-40% of the dataset, and training on the remaining 1-20% + 41-100%,\n",
    "#    and so on. Cross-validation is preferable as an evaluation methodolo-\n",
    "#    gy because it is far more robust. Results that generalize well to all\n",
    "#    subsets of our dataset will probably perform well on new data.\n",
    "#\n",
    "#    For convenience, we have also implemented a cross-validation wrapper\n",
    "#    to take care of the experimental design for us:\n",
    "from lib.CrossValidator import CrossValidator\n",
    "\n",
    "from lib.FeatureExtractor import FeatureExtractor\n",
    "\n",
    "lr = Classifier(\n",
    "    classifier='lr'\n",
    ")\n",
    "\n",
    "nb = Classifier(\n",
    "    classifier='mnb'\n",
    ")\n",
    "\n",
    "clfs = [nb]\n",
    "\n",
    "xtor_none = None\n",
    "\n",
    "xtor_off = FeatureExtractor(\n",
    "    'off'\n",
    ")\n",
    "\n",
    "xtor_rm = FeatureExtractor(\n",
    "    'rm',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    "    rm_stopwords=True,\n",
    ")\n",
    "\n",
    "xtor_all = FeatureExtractor(\n",
    "    'battery',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    "    rm_stopwords=True,\n",
    "    ngrams=[(1, False), (2, False)],\n",
    "    lemmatize=True\n",
    ")\n",
    "\n",
    "xtors = [xtor_off, xtor_rm, xtor_all]\n",
    "\n",
    "#    Experimental workflow:\n",
    "for dataset in datasets:\n",
    "    for clf in clfs:\n",
    "        for xtor in xtors:\n",
    "            c = CrossValidator(clf, dataset, train_r=0.9, extractor=xtor)\n",
    "#                 c = CrossValidator(clf, dataset, train_r=0.9)\n",
    "            c.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I was not lying :) We got a macro-averaged improvement of ~6% over the Brown and the Reuters corpora without doing anything fancy, just some data clean-up.\n",
    "\n",
    "\n",
    "## Sentiment analysis\n",
    "\n",
    "For the purposes of NLP, [__sentiment analysis__](https://en.wikipedia.org/wiki/Sentiment_analysis) is just a binary or ternary classification problem: binary when classifying into __positive__ or __negative__ sentiment, ternary for tasks also involving a third class for __neutral__ sentiment (neither positive or negative). The terms __positive__ or __negative__ here refer to the subjective notion of goodness: _Daenerys Targaryen = good_, _Cersei Lannister = bad_.\n",
    "\n",
    "In terms of algorithms and workflow, the task remains essentially the same as standard document __classification__, which means that most of the time an actual implementation will consist in training a classifier over as much data as we can lay our hands on :), then run new data through the model.\n",
    "\n",
    "For sentiment analysis (and classification more generally), data size is very likely to be the key performance factor. Feature engineering can help but probably less than just throwing more data at the system, at least in terms of getting quick gains. This is partly because sentiment analysis is a relatively vague task with a lot of long-tail variation but a very well-defined subset of frequent terms used to express positive or negative sentiment.\n",
    "\n",
    "That is, every language has a limited set of swearwords that all speakers learn pretty fast (twice as fast if you are a foreigner :) Given enough data, the system can easily learn that list of words from the training examples. Once you know the term _freaking disgrace_, there is little ambiguity left that could mislead the system :) A similar pattern applies to positive sentiment -the information we want to capture in both cases is in the words themselves (and can therefore be modelled using a BoW-model) rather than in higher-order information like combinations of those words.\n",
    "\n",
    "Let's see some sentiment analysis in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from lib.Classifier import Classifier\n",
    "from lib.CrossValidator import CrossValidator\n",
    "from lib.FeatureExtractor import FeatureExtractor\n",
    "\n",
    "from lib.Sentiment import Sentiment\n",
    "from lib.SentimentCorpusReaderWrapper import SentimentCorpusReaderWrapper as sentiment_corpus\n",
    "\n",
    "NGRAMS = [\n",
    "    (1, False),\n",
    "    (2, False),\n",
    "    (3, True),\n",
    "]\n",
    "\n",
    "datasets = [sentiment_corpus()]\n",
    "\n",
    "sentiment = Sentiment('sentiwordnet')\n",
    "\n",
    "lr = Classifier(\n",
    "    classifier='lr',\n",
    ")\n",
    "\n",
    "nb = Classifier(\n",
    "    classifier='mnb',\n",
    ")\n",
    "\n",
    "clfs = [lr]\n",
    "\n",
    "xtor_none = None\n",
    "\n",
    "xtor_off = FeatureExtractor(\n",
    "    'off'\n",
    ")\n",
    "\n",
    "xtor_clean = FeatureExtractor(\n",
    "    'clean',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    ")\n",
    "\n",
    "xtor_grams = FeatureExtractor(\n",
    "    'ngrams',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    "    ngrams=NGRAMS,\n",
    ")\n",
    "\n",
    "xtor_sent = FeatureExtractor(\n",
    "    'sentiment',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    "    sentiment=sentiment\n",
    ")\n",
    "\n",
    "\n",
    "xtors = [xtor_off, xtor_clean, xtor_sent, xtor_grams]\n",
    "\n",
    "#    Experimental workflow:\n",
    "for clf in clfs:\n",
    "    for dataset in datasets:\n",
    "        for xtor in xtors:\n",
    "            c = CrossValidator(clf, dataset, train_r=0.8, extractor=xtor)\n",
    "            c.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A very important lesson\n",
    "So, two things are important here: first, the results this time are awful -we are very likely __not__ beating anyone's state-of-the-art performance this time XD If you remember the structure of our sentiment dataset (which contained only __positive__ and __negative__ labels), we are performing binary classification so a random baseline would give us 0.5 accuracy. We are close to 0.8 but not even there yet, which means that our model is definitely better than random but not extremely better.\n",
    "\n",
    "The other important thing to note here is that features harmed performance more than anything else -it definitely seems that, in the case of sentiment analysis, whatever the system needs to learn that is going to help it determine whether something is positive or negative, is already there, regardless of our current features and regardless of our polarity dictionaries. This implies that, as long as we have data available, we do not really need to do anything with it.\n",
    "\n",
    "The key lessons here are:\n",
    "1. Algorithms will tend to see correlations in the data more easily than we can, which means that, theoretically, they can determine whether two variables are significantly associated more reliably than we can. As long as the __data is clean__, they will find whatever we want them to find. Yet again, another reason to take very seriously our earlier point about knowing the data.\n",
    "2. Additional data will maximize Recall and, given enough data, we will not be able to teach our system with a dictionary anything it has not learned from the corpus already. In the battle for classification, corpus always beats dictionary.\n",
    "\n",
    "\n",
    "\n",
    "### Dictionaries\n",
    "\n",
    "Traditionally, __polarity dictionaries__ have been used to assign a sentiment to an input text based on the sentiment of the words in that text. In our _wlp_ repository I have included a class called __Sentiment__ illustrating the use of a good range of existing dictionary[1]-based sentiment analysis libraries for Python: __[Afinn](https://pypi.python.org/pypi/afinn), [SenticNet](https://pypi.python.org/pypi/senticnet/0.5.0), [SentiWordNet__](http://sentiwordnet.isti.cnr.it), and __Vader__ (the last one comes with NLTK and I have included a copy of SentiWordNet in the repository, but the other two should be installed. There is a good chance I forgot to ask the organizers to do it ;) so feel free to run _for lib in sentic afinn; do pip install $lib; done_ from the command line whenever you have a second if you want to try the code).\n",
    "\n",
    "The fact that data is usually enough to get a prototype running does not mean that there are not some additional types of information that would also be necessary to correctly understand sentiment. Complex phenomena exist that it would be great to be able to model, but in general there are practical limitations as to how much complexity you want to build into the system: fine-grained sentiment detection would require a deep understanding of syntactic structure on the part of the system, which is a non-trivial task (more on that in a bit). It is also strongly domain-dependent, as well, which means that you would normally need to focus on a particular domain to be able to improve your system significantly.\n",
    "\n",
    "[1] In this context, dictionaries are also called _lexicons_ in the field of NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lib.Sentiment import (\n",
    "    Sentiment\n",
    ")\n",
    "\n",
    "terms = 'good old bad great wonderful terrible unknown'\n",
    "\n",
    "for resource in [Sentiment('afinn'), Sentiment('sentiwordnet'), Sentiment('vader'), Sentiment('sentic')]:\n",
    "    for term in terms.split():\n",
    "        print resource, term, resource.word_sentiment(term)\n",
    "\n",
    "\n",
    "\n",
    "sample_amazon_review = \"\"\"So bad it was good. The movie is slightly brain-dead and full of barely hidden tropes.\n",
    "I started to enjoy it when I stopped taking any part of it seriously and just watched the director blow things up.\n",
    "Runs long. Its like 2 mini-movies, a traditional Transformers movie first, then a Transformers teams with Godzilla\n",
    "smashup in China. It got that awesomely weird. If you like funny and weird with explosions you will love this.\n",
    "\n",
    "For those that like a little social commentary- it is a reflection of our currently tortured relationship with the\n",
    "hero-warrior archetype. We want to watch the hero warrior even though it feels thoroughly discredited. So it was\n",
    "reincarnated as a robot, an ALIEN robot. HELLO, OPTIMUS PRIME! And this particular movie goes further, surrounding\n",
    "the unreconstructed hero with all sorts of examples of hero gone bad from the cruel self-centered CIA director to\n",
    "the clueless, greedy best friend . The human lead, Mark Wahlburg, even has a line: \"Humans make mistakes, that's\n",
    "what we do.\" Sadly there are days when I can relate all too clearly.\"\"\"\n",
    "\n",
    "\n",
    "for resource in [Sentiment('afinn'), Sentiment('sentiwordnet'), Sentiment('vader')]:\n",
    "    print resource, '#review', resource.text_sentiment(sample_amazon_review)\n",
    "    print resource, '#terms', resource.text_sentiment(terms)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges of sentiment analysis\n",
    "\n",
    "As we said, fine-grained, high-quality sentiment analysis is a tricky business and it would require more than just a classifier trained on BoW model. Apart from the often-cited issues stemming from figurative language use such as sarcasm, irony, or metaphors, and apart from the standard problems of all NLP-related tasks (co-references, ambiguity), some of the basic aspects of language also pose challenges of their own. Just to name a few:\n",
    "\n",
    "1. __Negation__. Weird as it may seem, this still poses a problem for many NLP systems. Given that it is a logical rather than a statistical phenomenon, it is difficult to teach a system to interpret natural language negations in a quantitative manner. Usually, most systems end up implementing some rule-based approach for dealing with negation, such as switching the polarity of some number of words following a negation term. The challenge, as usual in NLP, is linguistic variation: there are just too many ways of negating something.\n",
    "\n",
    "2. __Modulated polarity__. If you remember, during the introduction we mention the phenomenon of __modulation__, the fact that some words' meaning is actually updated in context and does not fully exist independently (except in a very abstract representation). This is also true in sentiment analysis, where the polarity of some words depends on the context in which they appear: _high resolution_ is great, whereas _high price_ is not great at all. There is definitely sentiment involved in these expressions, although it is neither explicit or trivial, that is, there is no unambiguous marker that the system can learn from the data (the _freaking disgrace_ example from above). The sentiment is built compositionally by our minds while interpreting the semantic content of the terms involved: _price_ is generally an objective reality (neutral), but the commonsense-knowledge assumption is that humans do not want to pay high prices because they do not want to spend money. This is the kind of knowledge most systems still do not have, although a very successful paradigm has been recently introduced that is already able to handle some of these phenomena: __aspect-based sentiment analysis__. Each concept is decomposed into its salient aspects (price and resolution in the case of a screen, for instance), and the model training incorporates information as to which words refer to each aspect independently and, therefore, which ones are positive for one of the aspects and may not be for the other.\n",
    "\n",
    "3. __Domain-specificity__. A lot of sentiment-related information requires domain-specific data (or dictionaries). Some things could be seen as negative in a domain but not in another. A sentence like _The company shipped 10,000 jobs overseas_ would be probably negative in a text about politics. In a financial report, however, it could be positive in a news item about cutting costs, or negative in a news item about national employment, or rather: negative for the company forced to do the cutting, neutral for the network that is reporting the news, and positive as a measure that may still help save the struggling company. Sentiment is a by definition a subjective category and an oversimplification of what goes on in people's minds. Currently, most sentiment analysis technology can just be seen as __swearword detectors__ :) They can detect obvious mentions of explicit sentiment, but they have no clue as to the potential semantic interactions modulating the final sentiment of a given text. For that, an in-depth knowledge (=data) of the domain is required and, for very specific tasks or narrow domains, that information is usually not there. In all those cases, lexicon-driven approaches will be a good way to fill the gaps. When enough data is available, though, no amount of dictionaries should get in the way of training a Logistic Regression classifier :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
