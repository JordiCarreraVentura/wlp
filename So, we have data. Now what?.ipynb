{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, we have data. Now what?\n",
    "\n",
    "Now we slice it and then we cook it :D\n",
    "\n",
    "At this point, the input we are getting from all these __streamers__ or __readers__ are documents or sentences, usually as Python strings. That is a start but it still does not count as NLP :)\n",
    "\n",
    "The main problem is that string are too large to be useful: for whole documents, it is very unlikely our system will ever come across the same string. Therefore, even if we trained it to perform a perfect action for that document, it would never get a chance to perform that action. That means zero generalization and no automation. In order to really automate something, we need to ensure it is repeatable.\n",
    "\n",
    "At the core of repeatability lies resolution: the higher our system's resolution, and the more parts of a unit it is sensitive to, the greater the likelihood that it will find a pattern shared by its training data that can lead it to provide the correct answer.\n",
    "\n",
    "The process of reducing strings into smaller subunits is called __feature extraction__ and it is the first step towards turning something a person wrote into something a computer can understand.\n",
    "\n",
    "\n",
    "# Basics of __feature extraction__\n",
    "Some of the most trivial preprocessing steps we can perform are sentence splitting and word tokenization. Again, NLTK can help us with that. Let's see some examples from the Reuters corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "i = list(reuters.fileids())[10]\n",
    "print reuters.raw(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print reuters.sents(i.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print reuters.words(i.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances of the NLTK's CorpusReader class will come with built-in methods for accessing this information, which will therefore be available for the corpus supported in NLTK. What if we are given a new dataset, not in NLTK?\n",
    "\n",
    "We will still use NLTK :) (told you, it helps ;) More specifically, methods such as __wordpunct_tokenize__ and __sent_tokenize__. Let's pretend the Reuters text is our new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import (\n",
    "    sent_tokenize as splitter,\n",
    "    wordpunct_tokenize as tokenizer\n",
    ")\n",
    "\n",
    "text = \"\"\"SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION\n",
    "  Mines and Energy Minister Subroto\n",
    "  confirmed Indonesian support for an extension of the sixth\n",
    "  International Tin Agreement (ITA), but said a new pact was not\n",
    "  necessary.\n",
    "      Asked by Reuters to clarify his statement on Monday in\n",
    "  which he said the pact should be allowed to lapse, Subroto said\n",
    "  Indonesia was ready to back extension of the ITA.\n",
    "      \"We can support extension of the sixth agreement,\" he said.\n",
    "  \"But a seventh accord we believe to be unnecessary.\"\n",
    "      The sixth ITA will expire at the end of June unless a\n",
    "  two-thirds majority of members vote for an extension.\"\"\"\n",
    "\n",
    "print splitter(text)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print [tokenizer(sent) for sent in splitter(text)[:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can easily use NLTK's methods to get the sentences in a text, or the words in that text. Some systems do not need sentences as input and are designed to work with the individual words in each document (text classification, sentiment analysis, or term extraction can all work either at the sentence or the document level with little difference overall). However, sentences will be useful when building statistical models like n-gram models (we will see that in more detail in a bit).\n",
    "\n",
    "## Tokens\n",
    "The term _words_ we used in the previous paragraph is technically incorrect. At this level, we are still working with __tokens__, which include all words in a text plus any punctuation signs, plus any numerical expressions, plus virtually anything else. Strictly speaking, the number \"103\" is not a word but it can appear in a text. Therefore, it is a __token__. A __token__ refers to anything that appears within __word boundaries__. Word boundaries are not always well-defined and even today there are inconsistencies in the way some tokens are handled by different systems: some consider _didn't_ two separate words, and tokenize it as _did not_, whereas others take it as a single word. For our purposes, _did not_ is preferable to _didn't_ but, generally speaking, we would like our system to be robust to this type of thing. That is, we don't want to have to teach it explicitly that _did not_ and _didn't_ are two variants of the same linguistic unit. We want the system to infer that from their behavior as observed in the data. That will be when we will be able to claim we are doing something close to machine learning.\n",
    "\n",
    "## Punctuation\n",
    "Although some advanced applications can handle non-word tokens such as punctuation (for instance, to skip parenthetical elements inside a sentence, or appositions), most applications do not do much with it once the sentences have been split. Generally speaking, we can ignore punctuation in most tasks and we will still get the desired results, assuming we can train a good sequence model (we will see what this means later).\n",
    "\n",
    "## Numerical expressions\n",
    "The case of numerical expressions is more complicated because most systems tend to remove any patterns involving digits, which is fine in many cases. Sometimes, however, and depending on the task you are working on, numerical information will be crucial for the system: in many applications in which time information is important (for example, a website for booking flights needs to detect and process the dates of a trip), numerical information cannot be removed. Instead, a specific parser is usually required to handle each particular type of information: temporal expressions, clothing sizes for eBay product match, serial number recognition, etc.\n",
    "\n",
    "##### A quick note on feature engineering\n",
    "_These are all important points regarding __feature engineering__, and the main reason why we said earlier that developing a good understanding of the data is so important in NLP: unless you are certain that your data contains enough examples to account for the phenomena you want to model (so that enough features can be derived from it and the system can learn the appropriate behavior), the algorithm will not be able to make sense of the data. You cannot train a system for detecting flight information without information about flights. NLP often has the responsibility of keeping things real in machine learning and asking questions like **\"So you want your system to detect funny content... Are you detecting any jokes right now?\"**_ :)\n",
    "\n",
    "## Letter case\n",
    "The previous examples also contain a particular type of word variant, __letter case__. Case refers to whether a word is __UPPERCASE__, __lowercase__, __Titlecase__, or some __CoMbInAtIoN__ of both (the last one is often called __camel case__).\n",
    "\n",
    "A common type of preprocessing step consists in lowercasing all words, collapsing or case variants into their lowercase form. This will normally preserve most of the original information (the meaning of _combination_ does not usually change based on its case), although there are obviously exceptions (a _bond_ refers to a type of agreement or to a strong link, whereas _Bond_ is a secret agent. In most corpora there will be no confusion, but that might not be the case in a dataset about movie reviews. Again, knowing your data is important to make this kind of decisions).\n",
    "\n",
    "The goal of collapsing all these variants into a single one is to prevent the system from considering _the, The_ and _THE_ three different words, which would result in incorrectly dividing their frequency counts three-fold when calculating their probabilities, for instance. This type of normalization can easily boost the frequency of some terms several times, which results in a statistical model much closer to the actual truth of the data.\n",
    "\n",
    "## On the topic of variants\n",
    "Letter case is just a subtype ofÂ the more general fact that words have variants, and there are other types of variation that will make your NLP lives miserable. The two main types are __inflectional variation__ and __misspellings__.\n",
    "\n",
    "Inflectional variation is what the type of variation whereby we recognize the words _child_ and _children_ to be two __forms__ of the same __lemma__. The lemma is usually seen as referring to the actual concept, whereas the forms account for morphological variation, alternative shapes that the lemma takes to fit in particular linguistic contexts such as the plural number, or verb tenses. In languages like Spanish or Ukrainian, inflections are much richer than in English, and the amount of variation is also much higher, which usually requires some additional processing.\n",
    "\n",
    "In NLP, the two main ways of dealing with __forms__ is to either __stem__ them or __lemmatize__ them. Stemming is usually faster, easier, and error-prone. The main problem is that there is no guarantee that you end up with actual lemma. Lemmatization, on the other hand, tries to fulfill that requirement, but it has a number of relatively strong pressupositions: 1) part-of-speech tagging, which is not always trivial and that we will introduce later on, and 2) most often, a language-specific dictionary with mappings between forms and their lemmas. For now, let's take a look at the differences with a simple example. As always, NLTK can help us, although there are now other libraries we can use for this: __pattern__ and __spacy__, which offer better performance than NLTK for some tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#    Using NLTK\n",
    "import nltk\n",
    "\n",
    "#   stemmers\n",
    "stemmer1 = nltk.PorterStemmer()\n",
    "stemmer2 = nltk.LancasterStemmer()\n",
    "stemmer3 = nltk.SnowballStemmer(u'english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "lemmatizer.stem = lemmatizer.lemmatize\n",
    "normalizers = [\n",
    "    ('porter_stemmer', stemmer1),\n",
    "    ('lancaster_stemmer', stemmer2),\n",
    "    ('snowball_stemmer', stemmer3),\n",
    "    ('wordnet_lemmatizer', lemmatizer)\n",
    "]\n",
    "\n",
    "test = 'Strange listening women lying children government distribution basis supreme power derives adequate ceremonies'\n",
    "\n",
    "for name, normalizer in normalizers:\n",
    "    print name, '\\t=\\t', [normalizer.stem(token) for token in tokenizer(test)], '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Using pattern\n",
    "import pattern\n",
    "from pattern.en import parse\n",
    "\n",
    "#    Pattern is unnecessarily powerful at this point: its main output are parses,\n",
    "#    complete syntactic analysis. We still don't need that so we will implement a\n",
    "#    wrapper that returns the lemmas only:\n",
    "\n",
    "class PatternStemmer:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        tree = parse(text, lemmata=True)\n",
    "        return [\n",
    "            part[-1] for part in tree.split()[0]\n",
    "        ]\n",
    "\n",
    "stemmer = PatternStemmer()\n",
    "print 'pattern_lemmatizer\\t=\\t', stemmer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Using spacy\n",
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "en_doc = en_nlp(test.decode('utf-8'))\n",
    "\n",
    "print [en_doc.vocab[x.lemma].orth_ for x in en_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__spacy__'s lemmas look pretty great, actually, and are the best of all systems (for this particular example). Although we have skipped it here, __spacy__, like __pattern__, provides lemmas integrated into the syntactic analysis, which as we saw earlier is the only way of being able to choose the correct lemma: lemmas can differ based on a word's part-of-speech, which in turn must be determined by looking at its context in the sentence.\n",
    "\n",
    "## Parts of speech\n",
    "Now is probably a good time to introduce [parts of speech](https://en.wikipedia.org/wiki/Part_of_speech) (PoS for short). The basic idea comes from linguistics and it actually goes back many centuries. Latin grammarians (!!) already described word behavior based on parts of speech, which simply means how a word (form) behaves within a sequence of words.\n",
    "\n",
    "To fully grasp parts of speech, it is better to think in terms of concepts:\n",
    "\n",
    "> _person, height_\n",
    "\n",
    "That is just a sequence of two ideas, two __nouns__. As such, there is no relationship between the two.\n",
    "\n",
    "At this point, language is still not very informative: we are just naming things and all speakers are supposed to know the names of most things anyway. Nouns are a prerequisite for linguistic behavior, but not really its end goal. Language starts to be really useful when it __adds knowledge__, when new information is added through combinations.\n",
    "\n",
    "We started with two ideas, _person_ and _height_ -what if there is a connection between the two? What if that person is tall? _Height_ now becomes an attribute of _person_ (both ideas go together), and language will now use an __adjective__ to express that the two ideas in the sequence are not independent but are actually connected. That is, when an idea specifies another idea, it becomes an adjective (or behaves like one, such as the genitive case in Ukrainian or _de_-preposition phrases in Spanish):\n",
    "\n",
    "> _tall person_\n",
    "\n",
    "So far, we have seen words or combinations of words, but we are still not conveying thoughts. That is, we can build any sequence of nouns and adjectives, as long as we want. The question is, what for? Or rather, where do we stop? The standard way to mark that a sequence of ideas is a closed, complete unit that we can then transmit, is through a __verb__:\n",
    "\n",
    "> _People grow (become taller until they reach adulthood)_    (~_stature_ sense of _height_)\n",
    "\n",
    "> _People climb_                            (_vertical movement_ sense of _height_)\n",
    "\n",
    "> _People (are) at the top_                 (_vertical_ position sense of _height_)\n",
    "\n",
    "> Etc.\n",
    "\n",
    "The examples above still involve the same concepts as before, yet only these last examples are valid sentences that we can find in a corpus, something with a truth value that can be regarded as an assertion about the world. Verbs turn potentially endless sequences of noun combinations into a bounded thought we can express.\n",
    "\n",
    "These are the main parts of speech, although there are a few others:\n",
    "* pronouns (placeholders for nouns),\n",
    "* adverbs (adjectives for verbs or for other adjectives),\n",
    "* prepositions (they create adverbs from nouns or noun phrases, including pronouns),\n",
    "* conjunctions (they create nouns, adjectives or adverbs from verbs and their nouns),\n",
    "* determiners and quantifiers (usually adjectives in origin, but they end up as an indispensable part of nouns, to the point that nouns cannot be used without some determiner. For example, countable nouns in the singular can almost never be used without a determiner: _The dog barked_ versus _*Dog barked_).\n",
    "\n",
    "The three first categories (noun, adjectives and verbs) are open classes: new words are being added every day (especially true for nouns, just think of all new movie and song titles, people names, software version names, etc.) The last four are usually closed classes and their elements tend to remain constant over very large time spans.\n",
    "\n",
    "This is a very broad and overly simplified overview of parts of speech. Since language is messy, there are often cases of unclear boundaries across categories, as well as many cases of ambiguous words intersecting several categories. We will go back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords (or rather, _high-frequency noise_)\n",
    "\n",
    "As it turns out, words belonging to closed classes are those that are usually considered __stop words__. In the field of Linguistics, the words in those classes are often called __function words__ because, rather than conveying meaning, they often perform some more abstract, functional role inside the sentence (that is, they replace open-class words or change their part of speech).\n",
    "\n",
    "In many NLP applications, it is not necessary to go too deep into the linguistic structure of a sentence, which means that the system does not need to understand stop words. Sometimes they even hurt performance: they have extremely high probabilities that can easily mislead a naÃ¯ve statistical system.\n",
    "\n",
    "For this reason, stop words are very often removed as part of the text preprocessing step.\n",
    "\n",
    "\n",
    "### Long-tail distributions\n",
    "Natural language vocabulary is well-known for exhibiting a pattern corresponding to [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law), which is a subcase of the more general [long tail](https://en.wikipedia.org/wiki/Long_tail) statistical distribution (long-tail distributions [have a well-known shape](https://www.dropbox.com/s/ev0r95z8o9iahxc/Long%20tail.png?dl=0)), whereby a small number of outcomes have high frequency and a large number of outcomes have a very low frequency. In language, however, this will not be likely to result in a standard [power law](https://en.wikipedia.org/wiki/Power_law) distribution. Zipf went probably too far when he stated that the frequency of the word at rank _i_-th is twice that of the word at rank _i + 1_-th, but his general point still applies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from lib.TextStreamer import TextStreamer\n",
    "\n",
    "class ZipfsLaw(Counter):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def total(self):\n",
    "        return float(sum(zip(*self.most_common())[1]))\n",
    "    \n",
    "    def half(self):\n",
    "        acc = 0\n",
    "        t = self.total()\n",
    "        for i, (w, f) in enumerate(self.most_common()):\n",
    "            acc += f\n",
    "            if acc / t >= 0.5:\n",
    "                return i + 1\n",
    "        return i\n",
    "\n",
    "                \n",
    "path = '/Users/jordi/Laboratorio/corpora/raw/umbc/webbase_all/delorme.com_shu.pages_89.txt'\n",
    "strr = TextStreamer(path)\n",
    "i = 0\n",
    "zipf = ZipfsLaw()\n",
    "for record in strr:\n",
    "    zipf.update([w.lower() for w in tokenizer(record.strip()) if w.isalpha()])\n",
    "\n",
    "print zipf.most_common(10)\n",
    "print zipf.total()\n",
    "print zipf.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words in the top-10 above are __all__ stop words (function words). In Zipf's original experiments, 135 words were enough to account for half of the probability mass of the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus). We see that this number remains largely the same even today, and that the number is still very close in our case: 144 words account for half the probability mass of our dataset.\n",
    "\n",
    "In some way, there is a constant core of what makes a language the way it is, and we can expect that to be true of any dataset. Of course, this also means that any dataset will contain at least these words and, based on these words, any dataset of a language will \"match\" any other dataset of that language to some extent, making classification tasks hard (there is always some similarity between two English texts: English).\n",
    "\n",
    "This is the central idea behind applications like [__language detection__](https://en.wikipedia.org/wiki/Language_identification), but other NLP tasks aiming at a higher resolution need a way to ignore these common linguistic core.\n",
    "\n",
    "The first way anyone ever thought of doing that, was to create a list with those ~150 words (more or less) and use it as a blacklist for the system: any word in that list would be removed from the document's tokens, otherwise they stayed. As usual, we will rely on NLTK for a well-maintained list of stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import defaultdict as deft\n",
    "\n",
    "blacklist = deft(bool)\n",
    "for w in stopwords.words(u'english'):\n",
    "    blacklist[w] = True\n",
    "\n",
    "new_top_10 = [(w, f) for w, f in zipf.most_common() if not blacklist[w]][:10]\n",
    "print new_top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is definitely better (words such as _work, story_ or _book_ now have reached the top-10), but we still have a few words that do not tells us much: _one, time, new_ or _would_. These words can be considered high-frequency noise: words that may be actual words (rather than __function words__) strictly speaking, but that still convey too little information anyway to be helpful in our applications.\n",
    "\n",
    "However, we have already used the stopword-list trick. Now what? This is where things start to get interesting. \n",
    "\n",
    "### Dealing with high-frequency noise\n",
    "\n",
    "At some point, people realized that having to tell the system which words are important and which ones are not, is not only boring for us, but also does not say much of our NLP skills: is it not possible to find a way of automating this? As it turns out, there is :)\n",
    "\n",
    "One could argue that, if our goal is to develop a really smart machine, something that gets close to artificial intelligence in any meaningful sense,  then that kind of system should also be able to figure out on its own what is important from what is not (to at least a reasonable extent). Although that claim is a bit vague, we can make it more specific by saying that we want the system to be able to learn on its own at least which words convey too little information to be meaningful within a particular dataset.\n",
    "\n",
    "There are many ways of doing that but we will take a look at a few of the most effective: __background-foreground overlap, TF-IDF__, and __conditional probability__ (important because we will be using it in the context of classifiers like Naive Bayes).\n",
    "\n",
    "#### Background-foreground overlap\n",
    "The idea here is simple, we take a frequency distribution for one dataset, another frequency distribution for another dataset (totally independent, or as unrelated as possible), look at the top _n_ words in each distribution, and remove any items in the intersection. We will be left with much more informative content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from collections import Counter\n",
    "from lib.Tools import tokenizer\n",
    "\n",
    "fg_sents = [tokenizer(record) for record in strr]\n",
    "fg_dist = Counter()\n",
    "for sent in fg_sents:\n",
    "    fg_dist.update([w.lower() for w in sent if w.isalpha()])\n",
    "\n",
    "bg_dist = Counter([w.lower() for w in reuters.words() if w.isalpha()])\n",
    "\n",
    "topn = 500\n",
    "\n",
    "bg_top = [w for w, _ in fg_dist.most_common(topn)]\n",
    "fg_top = [w for w, _ in bg_dist.most_common(topn)]\n",
    "\n",
    "noise = [w for w in fg_top if w in bg_top]\n",
    "content = [w for w in fg_top if w not in bg_top]\n",
    "\n",
    "print 'noise:', len(noise), noise[:10]\n",
    "print 'content:', len(content), content[:10]\n",
    "print 'known stopwords:', len([w for w, boolean in blacklist.items() if boolean])\n",
    "print 're-used stopwords:', len([w for w in fg_top if w in bg_top and blacklist[w]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF (mutual information) and conditional probability\n",
    "TFIDF stands for __term frequency * inverse document frequency__. The intuition behind it is that the most relevant words in any document will be the most frequent words in that document (because they appear all over the place: \"work, work, work\") that are, at the same time, the least frequent on the overall dataset (in the sense that they will be unique to the current document: if no or very few other documents speak about the same topic, that topic can be assumed to be very characteristic of that particular document). So, it is a mathematical formula of what is both __important (frequent) and specific (inversely frequent over all documents)__ about a given document. This type of information is usually called the most __salient__ information.\n",
    "\n",
    "TFIDF often works at the document level within the same dataset, so here the system in fact needs the dataset to be divided into documents (that means that TFIDF cannot be directly calculated for short text inputs such as tweets, or at least that it will not yield any meaningful improvements over raw frequency counts unless some other segmentation of the data can be provided, at the topic level for instance).\n",
    "\n",
    "Fortunately, the Reuters corpus is divided into documents so we can use it for our experiments. It is no coincidence ;)\n",
    "\n",
    "The TFIDF formula is quite popular in the field of Information Retrieval and it has been successfully applied in many NLP applications. It is often a standard measure of term relevance and you can hardly go wrong if you use it. Sometimes it seems that half of all NLP happens around the TFIDF formula :) (or some variation of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict as deft\n",
    "\n",
    "\n",
    "#    The number of documents in our corpus:\n",
    "N = len(reuters.fileids())\n",
    "print N\n",
    "\n",
    "\n",
    "#    The TFIDF formula:\n",
    "def tfidf(word, frequency, document_frequencies):\n",
    "    tf = frequency\n",
    "    idf = math.log(N / float(document_frequencies[word]), 10)\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "#    First pass over the data to collect the document frequency for each word:\n",
    "document_frequencies = Counter()\n",
    "for i in reuters.fileids():\n",
    "    doc = reuters.words(i)\n",
    "    for w in set(doc): \n",
    "        document_frequencies[w] += 1\n",
    "\n",
    "        \n",
    "#    Second pass over the data to calculate the TFIDF scores for the words\n",
    "#    in each document:\n",
    "for i in list(reuters.fileids())[:5]:\n",
    "    doc = reuters.words(i)\n",
    "    counts = Counter(doc)\n",
    "    print 'top-5 by frequency:\\t', counts.most_common(5)\n",
    "    tfidf_weighted = sorted(\n",
    "        counts.most_common(),\n",
    "        key=lambda x: tfidf(x[0], x[1], document_frequencies),\n",
    "        reverse=True\n",
    "    )\n",
    "    print 'top-5 by TF-IDF:\\t', tfidf_weighted[:5]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional probability\n",
    "\n",
    "__Conditional probability__, on the other hand, is based on a slightly different idea: for conditional probability, the most important words in a document are not necessarily the most frequent ones.\n",
    "\n",
    "For conditional probability, what makes a word relevant is how much its probability changes in a given subsample of the data with respect to its overall probability over the entire dataset. So, rather than simply taking the most frequent words in a document that are not high-frequency noise (as TFIDF does), conditional probability focuses on the words whose probability changes the most, regardless of how frequent they are.\n",
    "\n",
    "Sometimes, this results in more informative features for our system: whereas TFIDF is really good at modelling topical information about a document, it may also overlook relevant specificities about that document. Let's see it with an example -the class ProbDist below will calculate for us both __TFIDF__ and __conditional probability__ scores for the same documents, and display the top-5 words according to each metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import (\n",
    "    Counter,\n",
    "    defaultdict as deft\n",
    ")\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "\n",
    "import nltk\n",
    "import math\n",
    "from collections import (\n",
    "    Counter,\n",
    "    defaultdict as deft\n",
    ")\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "\n",
    "class ProbDist:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.priors = Counter()\n",
    "        self.mass = 0.0\n",
    "        self.classf = deft(float)\n",
    "        self.posteriors = deft(Counter) \n",
    "        self.masses = deft(float)\n",
    "        self.idf = deft(float)\n",
    "        self.n = 0\n",
    "\n",
    "    def update(self, i, tokens):\n",
    "        l = len(tokens)\n",
    "        self.classf[i] += 1\n",
    "        self.posteriors[i].update(tokens)\n",
    "        self.masses[i] += l\n",
    "        self.priors.update(tokens)\n",
    "        self.mass += l\n",
    "        self.__idf_update(i, tokens)\n",
    "    \n",
    "    def __idf_update(self, i, tokens):\n",
    "        for w in set(tokens):\n",
    "            self.idf[w] += 1\n",
    "        self.n += 1\n",
    "    \n",
    "    def most_common(self, i, n=None):\n",
    "        return self.posteriors[i].most_common(n)\n",
    "    \n",
    "    def most_cond(self, i, n=None):\n",
    "        pxy = [(w, self.prob(w, i)) for w, f in self.posteriors[i].items()]\n",
    "        px_y = {w: self.prob(w) for w, _ in pxy}\n",
    "        conds = sorted(\n",
    "            [(w, (math.log((p / px_y[w]), 2))) for w, p in pxy],\n",
    "#             [(w, p * (p / px_y[w])) for w, p in pxy],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        if not n:\n",
    "            n = len(conds)\n",
    "        return conds[:n]\n",
    "    \n",
    "    def tfidf(self, i, w):\n",
    "        tf = self.posteriors[i][w]\n",
    "        idf = math.log(self.n / self.idf[w], 10)\n",
    "        return tf * idf\n",
    "    \n",
    "    def most_tfidf(self, i, n=None):\n",
    "        tfidfs = [\n",
    "            (w, self.tfidf(i, w))\n",
    "            for w, _ in self.posteriors[i].items()\n",
    "        ]\n",
    "        tfidfs.sort(key=lambda x: x[1], reverse=True)\n",
    "        if not n:\n",
    "            n = len(tfidfs)\n",
    "        return tfidfs[:n]\n",
    "               \n",
    "    def prob(self, w, i=None):\n",
    "        if i:\n",
    "            return self.posteriors[i][w] / self.masses[i]\n",
    "        else:\n",
    "            return self.priors[w] / self.mass\n",
    "    \n",
    "\n",
    "#    First pass over the data to collect prior probabilities:\n",
    "prob_dist = ProbDist()\n",
    "for i in reuters.fileids():\n",
    "    doc = reuters.words(i)\n",
    "    prob_dist.update(i, doc)\n",
    "\n",
    "        \n",
    "#    Second pass over the data and calculation of posterior probabilities:\n",
    "for i in list(reuters.fileids())[:5]:\n",
    "    print reuters.categories(i)\n",
    "    print 'top-5 by frequency:\\t', [w for w, _ in prob_dist.most_common(i, 5)]\n",
    "    print 'top-5 by COND:\\t', [w for w, _ in prob_dist.most_cond(i, 5)]\n",
    "    print 'top-5 by TFIDF:\\t', [w for w, _ in prob_dist.most_tfidf(i, 5)]\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, __conditional probability__ scores higher slightly more specific (less frequent) words that are still relevant to the topic, such as _Sumatran_ in the last document (Sumatra being a part of Indonesia), _additives_ in the second one (definitely relevant in the context of food _preservation_) and _kilowatt, kilolitres_ and _kl_ in the third one, which all make sense in a document about energy production.\n",
    "\n",
    "As we can see, __TFIDF__ and __conditional probability__ complement each other nicely, in the sense that the former is good at describing the general topic of a document, whereas the latter is good at spotting strongly associated terms that can be used for a more fine-grained semantic analysis of the document's information.\n",
    "\n",
    "Incidentally, this is also good to spot potentially important features in our dataset. Remember how we said earlier that, sometimes, it is up to us to decide if our system should take numerical expressions into account or not? (the flight-booking website example). In these examples we are seeing the system trying to find out on its own :)\n",
    "\n",
    "The _kilowatt_ word (and a potential numerical expression nearby associated to it) may not be the most frequent word in a document about energy (and TFIDF may therefore tend to not see), but it is still very salient when measured using conditional probability (because, even if it appears only a few times in the document, it appears even more rarely in the entire dataset). That means that it is probably a good predictor of the class (every time the system finds the word _kilowatt_, it can classify the docuemnt as being about energy) and an equally good feature for our system.\n",
    "\n",
    "__Tip:__ it follows that, if we multiply the conditional probability of a word by its posterior probability in a document, we get a value which is nearly identical to its TFIDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOWs\n",
    "\n",
    "We have now reached a crucial point: after everything we have seen, we not only have a number of ways of breaking down a document into its constituent parts beyond simple tokens, but we also have metrics to approximate an idea of content relevance that is within the range of cognitive plausibility[1], what we as human perceive as relevant ourselves.\n",
    "\n",
    "This means we already have a granular, more discrete representation of a text in terms of dimensions that we can use to measure it, as well as quantitative measurements along those dimensions using the metrics we introduced. Put in other way: until now, all we had was an area. Now we have axes along the dimensions of that area that we can use to place coordinates and locate specific points inside that area.\n",
    "\n",
    "We are finally in a position to start working with text in a scientific, formal way. And if its formal, the a computer can understand it :)\n",
    "\n",
    "The representation that we have been introduced in the previous sections is known as the __Bag-of-Words (BOW)__ model. It is based on the metaphor of our space as a bag in which words are placed, just like [marbles](https://www.dropbox.com/s/bf62o5jljma6ezd/Marbles.jpg?dl=0) :D (Yep, illustrating that was absolutely necessary ;)\n",
    "\n",
    "Each document is a bag of marbles, sorry, __words__ :) and we can assess how similar two documents are by comparing the marbles they contain, sorry, __words__. Oversimplifying __a lot__, it is possible to say that this is the main idea behind most __search engines__, in terms of how retrieve documents from an index.\n",
    "\n",
    "Actually, let's try that ourselves: let's pick a random document from the Reuters corpus and let's display the top-5 most similar documents, to see if we just hacked our own home-made Google. First, we'll do it with simple Python tools, then we will learn the proper way of doing it ;)\n",
    "\n",
    "[1] __Plausible__: something that makes sense to a human observer, or something that gets results using a method that can be expected to be close to the actual phenomenon generating those results. We can explain a result of 2 as either _1 + 1_ or _(6 + 3) - (6 + 1)_. The first option is the most plausible (most reasonable). In this sense, the notion of plausibility has strong ties with notions like [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) and [theoretical elegance](https://en.wikipedia.org/wiki/Elegance):\n",
    "> The proof of a mathematical theorem exhibits mathematical elegance if it is surprisingly simple yet effective and constructive; similarly, a computer program or algorithm is elegant if it uses a small amount of code to great effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Home-made search engine using BoWs -the \"hack way\"\n",
    "\n",
    "#    First, we build the index of documents by their words\n",
    "#    and compute their BoWs at the same time:\n",
    "\n",
    "documents_by_word = deft(list)    # We will use an inverted index to speed up lookup\n",
    "\n",
    "bows = dict([])\n",
    "for i in reuters.fileids():\n",
    "    bow = dict([])\n",
    "    for word, weight in prob_dist.most_tfidf(i):\n",
    "        bow[word] = weight\n",
    "        documents_by_word[word].append(i)\n",
    "    bows[i] = bow\n",
    "\n",
    "\n",
    "#    After that, we iterate over the documents again\n",
    "#    and retrieve the most similar documents:\n",
    "\n",
    "#    A function to get all BoWs sharing some word\n",
    "#    with the input BoW:\n",
    "def documents_with_some_word_in_common(bow, top=None):\n",
    "    if not top:\n",
    "        docs = set([])\n",
    "        for w in bow.keys():\n",
    "            docs.update(documents_by_word[w])\n",
    "        return docs\n",
    "    else:\n",
    "        docs = Counter()\n",
    "        for w in bow.keys():\n",
    "            docs.update(documents_by_word[w])\n",
    "        return set([doc for doc, f in docs.most_common(top)])\n",
    "\n",
    "\n",
    "#    A function to remind us of the top-5 words\n",
    "#    by TFIDF in a particular BoW:\n",
    "def top5(bow):\n",
    "    return sorted(\n",
    "        bow.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:5]\n",
    "\n",
    "\n",
    "#    A function to calculate the similarity\n",
    "#    between two BoWs, for now simply as the\n",
    "#    ratio between the TFIDF of all words\n",
    "#    shared between the two, and the total\n",
    "#    TFIDF of all their words, averaged, and\n",
    "#    minus the average of difference between\n",
    "#    the two:\n",
    "def similarity(bow, i):\n",
    "    sim = 0.0\n",
    "    _sim = 0.0\n",
    "    _bow = bows[i]\n",
    "    shared_words = set(bow.keys()).intersection(set(_bow.keys()))\n",
    "    total_tfidf = sum(bow.values())\n",
    "    _total_tfidf = sum(_bow.values())\n",
    "    matches = []\n",
    "    for w in shared_words:\n",
    "        sim += bow[w] / total_tfidf\n",
    "        _sim += _bow[w] / _total_tfidf\n",
    "        matches.append((w, bow[w]))\n",
    "    return (((sim + _sim) / 2) - (abs(sim - _sim) / 2),\n",
    "            sorted(matches, key=lambda x: x[1], reverse=True)[:5])\n",
    "\n",
    "\n",
    "#    A function to see the first 20 words of a\n",
    "#    particular Reuters docuemnt:\n",
    "def txt(i):\n",
    "    return ' '.join(list(reuters.words(i))[:100])\n",
    "\n",
    "\n",
    "\n",
    "for i, bow in bows.items()[:5]:\n",
    "    candidate_bows = documents_with_some_word_in_common(bow)\n",
    "    sim_bows = sorted(\n",
    "        [(candidate, similarity(bow, candidate), txt(candidate))\n",
    "         for candidate in candidate_bows],\n",
    "        key=lambda x: x[1][0],\n",
    "        reverse=True\n",
    "    )\n",
    "    print i\n",
    "    print txt(i)\n",
    "    print reuters.categories(i)\n",
    "    print '---'\n",
    "    for candidate, sim, text in sim_bows[1:4]:\n",
    "        print '  ', candidate\n",
    "        print '  ', reuters.categories(candidate)\n",
    "        print '  ', sim\n",
    "        print '  ', text\n",
    "        print '  ', '----'\n",
    "    print '====='\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several cool things going on here:\n",
    "\n",
    "1. Highly-relevant, topically-related matches for the first group about company acquisitions.\n",
    "2. It goes deeper than that, though -the first example is also an instance of a thoroughly correct category prediction __just by adding up TFIDF weights over an inverted index__.\n",
    "3. The same considerations apply to the second and third groups, on the crude industry and the consumer price index, respectively.\n",
    "4. Something even cooler can be observed in the fourth group: a machine prediction that __goes beyond__ the human annotation, that is, __an actual instance of machine learning__ :) The original document is about trade disputes (_trade_ category), but the system is matching to two other documents in the _trade_ category __and__ a document belonging to the _corn, grain_ categories. When we take a look at the document, however, we can see it is in fact about **trade disputes in the _corn, gran_ industry**. The system has been able to make a correct inference and assign a new, correct category to the document, even when the human annotators did not do that themselves. This is a perfect example of bootstrapped/semi-supervised learning: we can first train an algorithm on some data annotated by people, and then use that system's decisions to easily add more data to our initial pool. Once a system gets to the point of being useful, it quickly becomes unstoppable :)\n",
    "5. And the cooler-yet result is hidden in the last group: at first, it seems a like a weird dump of different categories. All categories appear only once: the original document labeled as _earn_, the first suggestion as _iron-steel_, the others as _crude_ and _copper_. So, what happened here? Again, __machine learning happened__ :) If we pay attention to the documents' title, we can see that, in this group specifically, all dcuments start with the _TALKING POINT_ phrase. These documents are reports by analysts and feature prominently quotes of their statementsl. Although they have not been explicitly labeled as such in the Reuters corpus because in that corpus the tags are topic-based rather than genre-based, the algorithm does __sense__ something special and consistent about this group. If we look at the features triggering each match, it becomes evident that the pronoun _he_ is a constant, which makes sense because articles quoting analysts' opinions will tend to have a significant number of expressions like _he said, he claimed, he reported_. The system is able to pinpoint that on its own, and it is also interesting to think what would have happened if we had removed all stop words (_he_ is almost always considered one) -we would have lost one of the main predictors for the _talking point_ subset of our sample and would have probably never known it was there. Again, knowing the data and trying to minimize our assumptions on its nature proves to be crucial for modelling linguistic phenomena correctly.\n",
    "\n",
    "All things considered, we are already getting plausible results :) Or, as one of my favorite [role models](https://en.wikipedia.org/wiki/Ash_Williams) would, **_groovy_** results :) [1].\n",
    "\n",
    "[1]\n",
    "> fashionable and exciting: sporting a groovy new haircut.\n",
    "\n",
    "> enjoyable and excellent: he played all the remarkably groovy guitar parts himself.\n",
    "\n",
    "\n",
    "Ok, now let's stop playing and let's start taking things seriously. What is the proper way of doing that? The main difference is how the similarity is calculated (over all the dimensions in the dataset), and it involves __vectorization__, turning our BoWs into [__vector__](https://en.wikipedia.org/wiki/Vector)s. \n",
    "\n",
    "> in pattern recognition and machine learning, an n-dimensional vector of numerical features that represent some object\n",
    "\n",
    "Vectors are a very popular input representation and most algorithms expect it or can at least handle it. When you leave home, you should always take with you some code to vectorize your data :)\n",
    "\n",
    "The thing with vectors is, they can grow really big :D Particularly for high-dimensional data like natural languages, where each word is viewed as one dimension and that means that each vector will be __as long as the entire vocabulary__ in our dataset (often referred to as the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)), and most of its values will be __zeroes__ (often referred to as [data sparsity](https://en.wikipedia.org/wiki/Sparse_matrix))[1].\n",
    "\n",
    "[1] By the way, matrices of BoWs are usually called __vector space models__, which is a cool name and a buzz word you should try to throw around whenever you have a chance ;) To make it sound even more impressive, you can also add the adjective _semantic_ up to a grand total of __semantic vector space models__. Apart from the cool factor of the name, it also happens to be technically correct :) (since we are building as our model a space containing some number of vectors, and the information in those vector faithfully represents semantic information -the relevance we saw in earlier sections-).\n",
    "\n",
    "That is why more efficient representations are commonly used. However, vectors are still at the core of any such representation, which simply use some trick or other to transform the vector into a more manageable object __without information loss__. Things can get very complicated in this area and there is an entire field of [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) which is absolutely fascinating but totally beyond the scope of this course :D (For those who are really curious, you can check out (Random Forests/Ensemble Trees)[https://en.wikipedia.org/wiki/Random_forest] or (Principal Component Analysis)[https://en.wikipedia.org/wiki/Principal_component_analysis].\n",
    "\n",
    "To keep things simple, we will repeat the previous experiment using Python's standard Array object from the __numpy__ library. We will convert our BoWs into vectors, then calculate the similarity between documents using the __cosine similarity__. Wait, what?\n",
    "\n",
    "A detailed technical explanation of intuitions behind the [__cosine similarity__](https://en.wikipedia.org/wiki/Cosine_similarity) would take some time :) so, to keep things simple, let's just say that it calculates the ratio between the sum of the weights of any non-zero-weight dimensions shared by two vectors, and the combined total weights for all dimensions of those two vectors. Because of operating in an Euclidean space, the final calculation also involves the __dot product__ and the __square root__ (see below for details :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math, numpy, scipy\n",
    "\n",
    "\n",
    "A = numpy.array\n",
    "\n",
    "\n",
    "#    Re-used from: http://stackoverflow.com/q/3121217\n",
    "#    Python's scipy or numpy implementations would usually be\n",
    "#    the way to go but they expected unnecessarily complicated\n",
    "#    wrapping objects as input and implementing the function\n",
    "#    to handle unwrapped vectors will be much easier in our case.\n",
    "def cosine_distance(u, v):\n",
    "    \"\"\"\n",
    "    Returns the cosine of the angle between vectors v and u. This is equal to\n",
    "    u.v / |u||v|.\n",
    "    \"\"\"\n",
    "    return numpy.dot(u, v) / (math.sqrt(numpy.dot(u, u)) * math.sqrt(numpy.dot(v, v))) \n",
    "cos = cosine_distance\n",
    "\n",
    "\n",
    "#   An object necessary to build our vector matrix,\n",
    "#   a mapping from words to their positional IDs in\n",
    "#   the vector space:\n",
    "dimensions = dict([])\n",
    "i = 0\n",
    "for w in set(reuters.words()):\n",
    "    dimensions[w] = i\n",
    "    i += 1\n",
    "\n",
    "D = len(dimensions.keys())\n",
    "\n",
    "\n",
    "#    Function to transform a our current BoW objects\n",
    "#    (Python dictionaries) into vectors:\n",
    "def bow2vec(bow):\n",
    "    vector = [0.1 for i in range(D)]\n",
    "    for w, weight in bow.items():\n",
    "        if not known_word[w]:\n",
    "            continue\n",
    "        word_index = dimensions[w]\n",
    "        vector[word_index] = weight\n",
    "#     print [x for x in vector if x]\n",
    "#     return A(vector).reshape(1, -1)\n",
    "    return A(vector)\n",
    "\n",
    "\n",
    "#    Home-made search engine using BoWs -the cool way (with vectors)\n",
    "for i, bow in bows.items()[:5]:\n",
    "    \n",
    "    candidate_bows = documents_with_some_word_in_common(bow, top=50)\n",
    "    \n",
    "    sim_bows = []\n",
    "    for candidate in list(candidate_bows):\n",
    "        vec1 = bow2vec(bow)\n",
    "        vec2 = bow2vec(bows[candidate])\n",
    "        sim = cos(vec1, vec2)\n",
    "        sim_bow = (candidate, sim, txt(candidate))\n",
    "        sim_bows.append(sim_bow)\n",
    "\n",
    "    sim_bows = sorted(\n",
    "        sim_bows,\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    print i\n",
    "    print txt(i)\n",
    "    print reuters.categories(i)\n",
    "    print '---'\n",
    "    for candidate, sim, text in sim_bows[1:4]:\n",
    "        print '  ', candidate\n",
    "        print '  ', reuters.categories(candidate)\n",
    "        print '  ', sim\n",
    "        print '  ', text\n",
    "        print '  ', '----'\n",
    "    print '====='\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks good :)\n",
    "\n",
    "1. The company acquisitions topic is again matched to several other documents in the same category effortlessly, by the very logic of their content.\n",
    "2. Same for the crude industry topic, which is also even more topically consistent, with all documents being now about news on Iran, just like the original news item.\n",
    "3. The group for the consumer prices index also seems semantically consistent but here it seems that our hacky way of solving this problem may have performed slightly better :) Or simply managed to stay closer to the manual annotation of the data for whatever other reason. In any case, both systems yield plausible results.\n",
    "4. Same for the last topic, the one about _TALKING POINT_s: again, cosine gets close to finding the hidden category but it would seem again that our home-made implementation has been a bit more sensitive to this particular variable.\n",
    "\n",
    "So, these are the foundations of the BoW/(semantic) vector space model as a representation of natural language inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We have features, we have BoWs, and we know how to turn them into vectors. It is time to put everything together and start predicting. Prediction is the key to automation: given all the previous pairs of _(X, Y)_ that our system has been trained on, the system must now be able to predict a correct _Y_ for a new, previously unobserved _X_.\n",
    "\n",
    "Prediction is performed by building a model: a weighted mapping from features to classes. The classifier will learn how well each feature predicts a given class (that is, how much weight it provides for the hypothesis that _X, therefore Y_).\n",
    "\n",
    "Classifiers differ in the way in which they compute the weights, that is, in which specific algorithm they use for weighting the features. There are many types of algorithms to do that but, in the field of NLP, some of the most important ones are [NaÃ¯ve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) classifiers, [Maximum Entropy](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) (Logistic Regression) classifiers, and [Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine) classifiers.\n",
    "\n",
    "We are going to try something crazy and understand each of them in a few lines :) (by the way, we will be borrowing some ideas from [these guys](https://www.datarobot.com/blog/classification-with-scikit-learn/), who provide a really nice introduction to some of the concepts and is definitely worth reading).\n",
    "\n",
    "## NaÃ¯ve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Naive Bayes\n",
    "\n",
    "#    Below are the features for our model -the known facts\n",
    "#    we have observed in the data.\n",
    "#\n",
    "#    These are the columns of each row in the list of \n",
    "#    'facts' :\n",
    "#\n",
    "#      [0] Name of the class\n",
    "#      [1] Stars at least one socially awkward geek\n",
    "#      [2] Stars doctors\n",
    "#      [3] Stars families of rich people\n",
    "#      [4] Stars dwarfs and dragons\n",
    "#      [5] Stars talking computers\n",
    "#      [6] Total shows\n",
    "#\n",
    "facts = [\n",
    "    ('good_TV', 40, 15, 10, 35, 50, 150),\n",
    "    ('grown_up_TV', 10, 60, 20, 9, 1, 100),\n",
    "    ('bad_TV', 5, 5, 189, 1, 1, 200),\n",
    "    ('internet_TV', 100, 5, 20, 22, 3, 300)\n",
    "]\n",
    "\n",
    "\n",
    "class Category:    \n",
    "    def __init__(self, row):\n",
    "        self.posteriors = []\n",
    "        self.frequencies = []\n",
    "        self.name = row[0]\n",
    "        self.mass = float(row[-1])\n",
    "        for value in row[1:-1]:\n",
    "            self.posteriors.append(value / self.mass)\n",
    "            self.frequencies.append(value)\n",
    "    \n",
    "    def __call__(self, features_in_show):\n",
    "        probs = [\n",
    "            self.posteriors[i] * feature\n",
    "            for i, feature in enumerate(features_in_show)\n",
    "        ]\n",
    "        if not probs:\n",
    "            return 0.0\n",
    "        prob = probs[0] + (10 ** -4)\n",
    "        for _prob in probs[1:]:\n",
    "            if _prob:\n",
    "                prob *= _prob\n",
    "            else:                        #   Simple additive smoothing to avoid zero-multipli-\n",
    "                prob *= (10 ** -4)       #   cations, which would cancel out our probability \n",
    "        return prob                      #   estimates. This is a well-known drawback of NaÃ¯ve\n",
    "                                         #   Bayes and there are a number of techniques for \n",
    "                                         #   performing statistical smoothing:\n",
    "                                         #   https://en.wikipedia.org/wiki/Smoothing\n",
    "        \n",
    "class Model:\n",
    "\n",
    "    def __init__(self, facts):\n",
    "        self.categories = [Category(row) for row in facts]\n",
    "        self.mass = float(sum(c.mass for c in self.categories))\n",
    "        self.class_priors = {\n",
    "            i: c.mass / self.mass for i, c in enumerate(self.categories)\n",
    "        }\n",
    "        self.feature_priors = [0.1 for feature in facts[0][1:-1]]\n",
    "\n",
    "        for c in self.categories:\n",
    "            for i, value in enumerate(c.frequencies[1:-1]):\n",
    "                self.feature_priors[i] += value\n",
    "\n",
    "\n",
    "    def prior(self):\n",
    "        prob = self.feature_priors[0]\n",
    "        for _prob in self.feature_priors[1:]:\n",
    "            prob *= _prob\n",
    "        return prob\n",
    "        \n",
    "                \n",
    "    def __call__(self, new_show):\n",
    "        features_in_show = new_show[1:-1]\n",
    "        hypotheses = []\n",
    "        for i, c in enumerate(self.categories):\n",
    "            pxy = c(features_in_show) * self.class_priors[i]\n",
    "            px_y = self.prior()     # This part does not really affect the calculations\n",
    "            p = pxy / px_y          # because it remains constant for all classes.\n",
    "            hypotheses.append((p, c.name))\n",
    "        return sorted(hypotheses, reverse=True)\n",
    "    \n",
    "\n",
    "#\n",
    "#     [1] Stars at least one socially awkward geek\n",
    "#     [2] Stars doctors\n",
    "#     [3] Stars families of rich people\n",
    "#     [4] Stars dwarfs and dragons\n",
    "#     [5] Stars talking computers\n",
    "#     [6] Total shows\n",
    "#\n",
    "m = Model(facts)\n",
    "print m((None, 0, 0, 0, 1, 1, None)), '\\n'\n",
    "print m((None, 0, 1, 0, 0, 0, None)), '\\n'\n",
    "print m((None, 0, 1, 1, 0, 0, None)), '\\n'\n",
    "print m((None, 0, 0, 1, 0, 0, None)), '\\n'\n",
    "print m((None, 1, 0, 0, 1, 0, None)), '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The independence assumption\n",
    "Why is __NaÃ¯ve Bayes__ called \"naÃ¯ve\"? Because the model makes an assumption that is usually too well-intentioned :) NaÃ¯ve Bayes models work on the feature __independence assumption__, which is, by the way, the mean reason why they are simple and fast, but also the main reason why so many people wonder how they can perform so well most of the time :)\n",
    "\n",
    "In NLP, the __independence assumption__ translates into the (very strong) assumption that the probabilities of each word in the document are independent from the presence of any other word in that document. That is, NaÃ¯ve Bayes calculates the probability of words given the category of the documents they appear in (for instance, the fact that the word _politician_ appears more frequently in texts about politics), but it does not take into account the probability of a word given some other word (for instance, the fact that _politician_ and _ministry_ tend to appear together, or __co-ocurr__,  using the technical term, which stands for \"occurring/appearing together\").\n",
    "\n",
    "For NaÃ¯ve Bayes, this means that two correlated words will each contribute to the overall score of the class. For the example above, _politician_ and _ministry_, that seems fine, though, since a text containing two words associated with the politics category should get some score from each separate word anyway. Independence seems fine in this case.\n",
    "\n",
    "When is it not okay? In NLP, the __independence assumption__ can be particularly harmful with __collocations__ and __multiwords__. We will see these units in more detail in a bit, for now let's just say they are terms consisting of several tokens, such as _Barak H. Obama, president Barack Obama, cafe latte_ or _Lviv Computer Science Summer School_.\n",
    "\n",
    "From NaÃ¯ve Bayes naÃ¯ve perspective, each of the tokens in each of these words would add a proportional probability. That means that documents _d1_ and _d2_ below would both receive the same probability \n",
    "\n",
    "> training_doc1 = {president, Barack, Obama, trade, deal}\n",
    "\n",
    "> training_doc2 = {Obama, war, Iran}\n",
    "\n",
    "> ------\n",
    "\n",
    "> new_doc = {president, Barack, Obama, war, Iran}\n",
    "\n",
    "> ------\n",
    "\n",
    "> training_doc1 â© new_doc = 3 / 5\n",
    "\n",
    "> training_doc2 â© new_doc = 3 / 5\n",
    "\n",
    "despite the fact that 'training_doc2' is clearly the most relevant given the new document. To summarize, the __independence assumption__ may cause result in false positives due to awarding free (too generous) hits for tokens that are really parts of just one word. In NLP, this is a direct consequence of work-in-progress tokenization (now you see the importance of all that boring stuff about tokenization I told you about earlier ;) and how Computational Linguistics can make things easier for Machine Learning -__this__ is NLP). What this means is that not everything you can split on a whitespace is a word (and the fact that the opposite is also true is a *__well-known__ fact* ;), and sometimes the actual linguistic unit (the thing whose probability you should be calculating) will be something bigger, sometimes spanning a few words and sometimes even *discontinuous*, which makes it necessary to *__take__ these things __into account__*.\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "On the other hand, __Logistic Regression__ or __Maximum Entropy__ (usually abbreviated as __MaxEnt__) classifiers work by finding linear correlations between features (also called __independent variable__s, or __predictors__) and classes (also known as __dependent variable__s). Linear regression is the type of analysis that can tell you if people tend to have increasing amounts of ice-cream as temperatures rise towards the summer.\n",
    "\n",
    "The key difference of logistic regression with respect to standard linear regressions is that, whereas linear regression gives us a coefficient (how associated the independent and dependent variables are, with a value ranging from -1 to +1), logistic regression gives us a __probability-looking normalized score__ (a floating point number between 0. and 1.0) that we can use as an actual probability of the class (dependent variable) given the feature (independent variable). Zero means the feature does not predict the class, one means the feature predicts the class with absolute certainty. The normalization of this score is performed using the [__logistic function__](https://en.wikipedia.org/wiki/Logistic_function) (a type of [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)) hence the technique's name.\n",
    "\n",
    "The math behind logistic regression is a bit complicated and beyond the focus of this course. For convenience, below is some code ([via StackOverlow](http://stackoverflow.com/a/7939259)) with a recap of linear regression. For our purposes, imagine that we replaced [a] temperatures over a period of time and [b] ice-cream in our example above, with [a'] words in a document and [b'] document categories, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "facts = [\n",
    "    # category, 'government', 'politician', 'player', 'team'\n",
    "    ('politics', [1, 1, 0, 0]),\n",
    "    ('politics', [0, 1, 0, 0]),\n",
    "    ('politics', [1, 0, 0, 0]),\n",
    "    ('sports', [0, 0, 1, 1]),\n",
    "    ('sports', [0, 0, 0, 1]),\n",
    "    ('sports', [0, 0, 1, 0]),\n",
    "]\n",
    "\n",
    "categories = [\n",
    "    ('politics', [2, 2, 0, 0]),\n",
    "    ('sports', [0, 0, 2, 2])\n",
    "]\n",
    "\n",
    "def average(x):\n",
    "    assert len(x) > 0\n",
    "    return float(sum(x)) / len(x)\n",
    "\n",
    "def correlation(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    n = len(x)\n",
    "    assert n > 0\n",
    "    avg_x = average(x)\n",
    "    avg_y = average(y)\n",
    "    diffprod = 0\n",
    "    xdiff2 = 0\n",
    "    ydiff2 = 0\n",
    "    for idx in range(n):\n",
    "        xdiff = x[idx] - avg_x\n",
    "        ydiff = y[idx] - avg_y\n",
    "        diffprod += xdiff * ydiff\n",
    "        xdiff2 += xdiff * xdiff\n",
    "        ydiff2 += ydiff * ydiff\n",
    "    return diffprod / math.sqrt(xdiff2 * ydiff2)\n",
    "\n",
    "new_doc = [1, 0, 0, 0]\n",
    "\n",
    "for name, vector in categories:\n",
    "    print name, correlation(vector, new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    And then the logistic function:\n",
    "def logistic_function(v):\n",
    "    return 1 / (1 + (math.e ** -v))\n",
    "\n",
    "print logistic_function(0.57735026919)\n",
    "print logistic_function(-0.57735026919)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we just implemented an again-grossly-oversimplified Maximum Entropy classifier :)\n",
    "\n",
    "#### Back to the naÃ¯vetÃ© of correlations\n",
    "The key advantage of __Logistic Regression__ over __NaÃ¯ve Bayes__ lies in the former's ability to handle correlated features (check out this [thread on Quora](https://www.quora.com/What-is-the-difference-between-logistic-regression-and-Naive-Bayes) for details). That is, Logistic Regression __does not make the independence assumption__ and is able to deal feature correlations naturally by assigning them lower weights. In this way, a several-word-long linguistic unit no longer contributes to the final score for each of the tokens in it.\n",
    "\n",
    "More specifically, NaÃ¯ve Bayes calculates a probability for each word and finally multiplies the probabilities for all words, whereas Logistic Regression learns the weight for each word relative to the weights of rest of the words and, therefore, assigns to each its individual contribution over the maximum contribution that they all add up to.\n",
    "\n",
    "That means that,iInstead of three words, each with an independent probability of 1.0 of belonging to the multi-word _president Barack Obama_, we now have three words, each with a ~0.33 probability of belonging to that multi-word, which makes far much more sense because the probability mass now sums up to 1.0 instead of 3.0.\n",
    "\n",
    "\n",
    "\n",
    "## Support Vector Machines\n",
    "__Support Vector Machines__ (SVMs for short), finally, are a powerful algorithm with occasionally slightly crazy training times. We mention them here because they are robust and very successful for some tasks, although their performance varies widely with the particular task. Essentially, they perform the same type of analysis as Logistic Regression classifiers (if we are talking about linear SVMs) with some differences in what is considered or high or low correlation.\n",
    "\n",
    "In practice, NaÃ¯ve Bayes often performs just as well as SVM classifiers (or better, despite their much higher simplicity) and Logistic Regression usually outperforms both. In my own experience, Maximum Entropy is usually the way to go.\n",
    "\n",
    "The main factor to look for when deciding when to use SVMs is the dimensionality of our space: SVMs can handle high numbers of dimensions but they usually cannot handle the multi-dimensionality of NLP data (at least, no without some serious normalization and dimensionality reduction, which are always possible).\n",
    "\n",
    "That is the main reason why they will probably not be very useful unless you have some way to carry out a massive clean-up and normalization of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual classifiers and actual data\n",
    "\n",
    "We have everything we need: features, labels, BoWs, a vector space model, and classification algorithms to learn a mapping from the features to the labels. It is time to start doing some actual kick-ass prediction.\n",
    "\n",
    "Let's put all the pieces together and go back down to the code once again:\n",
    "\n",
    "### First, give me a dataset please\n",
    "\n",
    "Or more than one if possible :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:02, 173.20it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/compat.py:656: RuntimeWarning: Argument <type 'str'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n",
      "  return transliterate(method(self))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 0 449 51 0.51 3.19\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 1 449 51 0.47 3.23\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 2 449 51 0.59 3.21\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 3 449 51 0.57 2.96\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 4 449 51 0.39 3.29\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 5 449 51 0.49 3.26\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 6 444 50 0.58 3.13\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 7 444 50 0.46 2.93\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr 8 446 48 0.52 3.46\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:02, 173.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " None lr 9 355 33 0.55 1.9\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None lr run 4383 487 0.513 3.056\n",
      "--------------\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 0 449 51 0.22 0.93\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 1 449 51 0.24 0.93\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 2 449 51 0.22 0.91\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 3 449 51 0.2 0.92\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 4 449 51 0.24 0.92\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 5 449 51 0.27 0.93\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 6 444 50 0.24 0.9\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 7 444 50 0.24 0.95\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb 8 446 48 0.23 0.96\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10788it [00:05, 1897.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " None mnb 9 355 33 0.33 0.73\n",
      "<CategorizedTaggedCorpusReader in u'/Users/jordi/nltk_data/corpora/brown'> None mnb run 4383 487 0.243 0.908\n",
      "--------------\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> None lr 0 9656 1132 0.86 13.64\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> None lr 1 9656 1069 0.9 13.19\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> None lr 2 9656 1069 0.91 13.33\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> None lr 3 9656 1069 0.9 13.62\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> None lr 4 9656 1069 0.9 17.22\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'> None lr 5 9656 1069 0.91 15.68\n",
      "<CategorizedPlaintextCorpusReader in u'/Users/jordi/nltk_data/corpora/reuters'>"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters, brown\n",
    "\n",
    "#    We will re-use the SimpleCorpusReader class we saw when introducing\n",
    "#    our datasets, and extend it with .categories(), .words(), etc. meth-\n",
    "#    ods like those in NLTK's CorpusReaders so that we can use them with-\n",
    "#    in the same training and testing workflow:\n",
    "from lib.TwentyNewsgroupsCorpusWrapper import TwentyNewsgroupsCorpusWrapper as twenty_newsgroups\n",
    "\n",
    "datasets = [brown, reuters, twenty_newsgroups()]\n",
    "\n",
    "\n",
    "#    We have also implemented a wrapper class 'Classifier' that gives us\n",
    "#    easy and consistent access to scikit-learn's classification algori-\n",
    "#    thms:\n",
    "from lib.Classifier import Classifier\n",
    "\n",
    "#    Although NLTK's datasets usually come with pre-defined train and test\n",
    "#    splits of the data, in our experiments we will ignore that distinct-\n",
    "#    ion and we will be performing cross-validation. When cross-validating,\n",
    "#    the ratio between training and testing data is observed (for instance,\n",
    "#    8 training instances for every 2 test instances) but combining diffe-\n",
    "#    rent parts of the corpus: in the 1st cross-validation fold, the first\n",
    "#    20% of the dataset is used as training and the remaining 80% for tes-\n",
    "#    ting; in the 2nd cross-validation fold, testing is performed on the\n",
    "#    21-40% of the dataset, and training on the remaining 1-20% + 41-100%,\n",
    "#    and so on. Cross-validation is preferable as an evaluation methodolo-\n",
    "#    gy because it is far more robust. Results that generalize well to all\n",
    "#    subsets of our dataset will probably perform well on new data.\n",
    "#\n",
    "#    For convenience, we have also implemented a cross-validation wrapper\n",
    "#    to take care of the experimental design for us:\n",
    "from lib.CrossValidator import CrossValidator\n",
    "\n",
    "lr = Classifier(\n",
    "    classifier='lr',\n",
    ")\n",
    "\n",
    "nb = Classifier(\n",
    "    classifier='mnb',\n",
    ")\n",
    "\n",
    "clfs = [lr, nb]\n",
    "\n",
    "\n",
    "#    Experimental workflow:\n",
    "for dataset in datasets:\n",
    "    for clf in clfs:\n",
    "        c = CrossValidator(clf, dataset, train_r=0.9)\n",
    "        c.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These results are preliminary but very exciting nevertheless. Just to give you a frame of reference, below is a summary of some state-of-the-art results for classification on these corpora (some of these benchmarks are a bit old but I have not been able to find any more recents papers reporting better scores -which definitely does not mean they do not exist):\n",
    "\n",
    "* [This 2015 paper](https://www.researchgate.net/profile/Giacomo_Domeniconi/publication/281743455_A_Study_on_Term_Weighting_for_Text_Categorization_A_Novel_Supervised_Variant_of_tfidf/links/55f6ad7708aeafc8abf4f470.pdf) reports a maximum accuracy of nearly 94% and average accuracies of 92% on the Reuters corpus using __SVMs, and a number of variations of TFIDF as the feature weighting function, and feature selection__. The same configuration yielded scores in the mid-70% range on the 20 Newsgroups dataset. In our cases, __a barely normalized BoW-based vector space model combined with a Logistic Regression classifier__ allowed us to come very close of their top performance for Reuters (within 3-4%) and to outperform by ~10% their results on the 20 Newsgroups dataset. This is the kind of thing why we said earlier that Logistic Regression is usually the way to go ;)\n",
    "\n",
    "* On the Brown corpus, one of the top accuracies reported is [a 2010 52%-result](http://www.aclweb.org/anthology/P10-1077) (over all categories, so quite low but definitely better than random -because you have many more categories than just two, which would amount to a 50% random baseline assuming a uniform distribution-). Our out-of-the box, nearly unmodified classifier matched that performance effortlessly, despite the fact that the authors of that paper were using more advanced features. Again, Logistic Regression can be seen to account for the difference (they used Linear Discriminant Analyis, a precursor to Logistic Regression).\n",
    "\n",
    "* Again on the 20 Newsgroups classifiers, the [Stanford team](http://nlp.stanford.edu) (in many ways, the founders of Natural Language Processing and modern Computational Linguistics) [report a nearly 82% accuracy](http://nlp.stanford.edu/wiki/Software/Classifier/20_Newsgroups) using their classifier. They report some previous results performing up to the ~82% mark, but they also report as \"suspect\" an earlier result achieving nearly 85% accuracy. Our own results suggest this last result actually seemed on the right track :) (for obvious reasons, Stanford people do not like other people getting better accuracies ;)\n",
    "\n",
    "\n",
    "## Experiments with a custom __Feature Extractor__\n",
    "NaÃ¯ve Bayes performance went up wrt sklearn's TfIdfVectorizer, no idea why.\n",
    "Speed also seems higher.\n",
    "\n",
    "20 Newsgroups with n-grams 1-3 won't fit in memory :( (12Gb RAM, w/ 30 compressed. I can run it on my Mac because it has 16Gb)\n",
    "\n",
    "SpaCy lemmatization: supposed to be really fast but for me it was working at 1 doc per second (!)\n",
    "\n",
    "BoWs, types of feature extraction.\n",
    "\n",
    "As you can see, I am not crazy :) We got a macro-averaged improvement of 1% without doing anything fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Here is the class for feature extraction\n",
    "from lib.FeatureExtractor import FeatureExtractor\n",
    "\n",
    "#    Below are the different parameterizations we will be testing\n",
    "NGRAMS = [\n",
    "    (1, False),\n",
    "    (2, False),\n",
    "    (3, False)\n",
    "]\n",
    "\n",
    "#    Same classifiers as above but now using a minimum frequency\n",
    "#    threshold for the features; we will be generating many of\n",
    "#    them and we do not want the system to waste time with rare\n",
    "#    ones:\n",
    "lr = Classifier(\n",
    "    classifier='lr',\n",
    "    min_df=3\n",
    ")\n",
    "\n",
    "nb = Classifier(\n",
    "    classifier='mnb',\n",
    "    min_df=3\n",
    ")\n",
    "\n",
    "clfs = [nb, lr]\n",
    "\n",
    "\n",
    "#    The 20 Newsgroups corpus is a bit of a monster and running\n",
    "#    full feature extractors on it could burn my laptop before\n",
    "#    the demo :D so I will be skipping it for these tests. The-\n",
    "#    re would absolutely be no problem running it on a server\n",
    "#    cluster, for instance:\n",
    "datasets = [brown, reuters]\n",
    "\n",
    "\n",
    "#    Below are the feature extractors:\n",
    "xtor0 = None\n",
    "\n",
    "xtor1 = FeatureExtractor(\n",
    "    'off'\n",
    ")\n",
    "\n",
    "xtor2 = FeatureExtractor(\n",
    "    'rm-junk',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    ")\n",
    "\n",
    "xtor3 = FeatureExtractor(\n",
    "    'rm-noise',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    "    rm_stopwords=True\n",
    ")\n",
    "\n",
    "xtor4 = FeatureExtractor(\n",
    "    'coll',\n",
    "    collocations=True\n",
    ")\n",
    "\n",
    "xtor5 = FeatureExtractor(\n",
    "    'lemma',\n",
    "    lemmatize=True\n",
    ")\n",
    "\n",
    "xtor6 = FeatureExtractor(\n",
    "    'ngrams',\n",
    "    ngrams=NGRAMS,\n",
    ")\n",
    "\n",
    "xtor7 = FeatureExtractor(\n",
    "    'battery',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    "    rm_stopwords=True,\n",
    "    collocations=True\n",
    ")\n",
    "\n",
    "xtor8 = FeatureExtractor(\n",
    "    'battery',\n",
    "    rm_numbers=True,\n",
    "    rm_punct=True,\n",
    "    rm_stopwords=True,\n",
    "    ngrams=NGRAMS,\n",
    "    lemmatize=True\n",
    ")\n",
    "\n",
    "xtors = [xtor1, xtor2, xtor3, xtor4, xtor5, xtor6, xtor7, xtor8]\n",
    "\n",
    "#    Experimental workflow:\n",
    "for dataset in datasets:\n",
    "    for clf in clfs:\n",
    "        for xtor in xtors:\n",
    "            c = CrossValidator(clf, dataset, train_r=0.9, extractor=xtor)\n",
    "#                 c = CrossValidator(clf, dataset, train_r=0.9)\n",
    "            c.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# More features\n",
    "\n",
    "## N-grams model\n",
    "\n",
    "## PoS-tags special\n",
    "\n",
    "## Synsets\n",
    "\n",
    "## Collocations\n",
    "\n",
    "hurt performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
