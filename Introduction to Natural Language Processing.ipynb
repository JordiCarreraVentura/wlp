{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Natural Language Processing?\n",
    "\n",
    "In order to understand the meaning of the term _Natural Language Processing_ (__NLP__ for short[1]), it is a good idea to start by looking at the words that make up the term.\n",
    "\n",
    "[1] Also known as _computational linguistics_, _text analytics_ and _text mining_. All these terms are largely synonyms.\n",
    "\n",
    "\n",
    "## _Processing_\n",
    "The term _processing_ refers to the fact that we will be using machines to do the work. That is, our goal is to automate. By definition, automation implies machines, and machines imply engineering, which is why NLP is, in that regard, a subfield of computer engineering. But not only that.\n",
    "\n",
    "## _Language_\n",
    "The term _natural language_ refers to what we want to automate. A _language_ is a means of communication where there is an arbitrary pairing of signals (X) and their meaning (Y).\n",
    "\n",
    "\n",
    "### ... or, put in other code,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-nlp true \"hello world\"\n",
      "-nlp false \"helloworld\"\n",
      "-nlp false \"hell o wor ld\"\n",
      "-nlp false \"undefined\"\n",
      "-nlp false \"asdfasdasdasdf\"\n",
      "\n",
      "+nlp true \"hello world\"\n",
      "+nlp true \"helloworld\"\n",
      "+nlp true \"hell o wor ld\"\n",
      "+nlp false \"undefined\"\n",
      "+nlp false \"asdfasdasdasdf\"\n"
     ]
    }
   ],
   "source": [
    "#    Data for Machine Learning\n",
    "training_data = [\n",
    "    ('hello world', 'true'),\n",
    "    ('*', 'false')\n",
    "]\n",
    "\n",
    "#    Real-world linguistic inputs from user-generated content\n",
    "real_world_linguistic_data = [\n",
    "    'hello world',\n",
    "    'helloworld', 'hell o wor ld',\n",
    "    'undefined', 'asdfasdasdasdf'\n",
    "]\n",
    "\n",
    "\n",
    "def machine_learning(input):\n",
    "    for stimulus, response in training_data:\n",
    "        if input == stimulus:\n",
    "            return response\n",
    "    return training_data[-1][1]\n",
    "\n",
    "ml = machine_learning\n",
    "\n",
    "for test in real_world_linguistic_data:\n",
    "    print '-nlp', ml(test), '\\\"%s\\\"' % test\n",
    "print\n",
    "        \n",
    "\n",
    "#    Data for Computational Linguistics\n",
    "dictionary_of_accepted_terms = ['hello world', 'foo', 'bar']\n",
    "\n",
    "\n",
    "#    Definition of each field of study:\n",
    "def computational_linguistics(string):\n",
    "    forms = []\n",
    "    for w in dictionary_of_accepted_terms:\n",
    "        sim = len(set(string.lower()).intersection(set(w))) / float(len(w))\n",
    "        if sim >= 0.5:\n",
    "            forms.append((sim, w))\n",
    "    if forms:\n",
    "        return sorted(forms)[-1][1]\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "\n",
    "def natural_language_processing(test):\n",
    "    _test = cl(test)\n",
    "    try:\n",
    "        assert _test\n",
    "    except Exception:\n",
    "        return None\n",
    "    return ml(_test)\n",
    "\n",
    "\n",
    "cl = computational_linguistics\n",
    "nlp = natural_language_processing\n",
    "\n",
    "for test in real_world_linguistic_data:\n",
    "    print '+nlp', nlp(test), '\\\"%s\\\"' % test    #  Natural Language Processing is just a wrapper\n",
    "                                                #  over Machine Learning in order to handle \n",
    "                                                #  linguistic inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be the first time someone has reduced a bunch of scientific disciplines to a few Python functions :D but I have the feeling that the idea of being able to do just some day is the main reason why most of you are reading this. Now let's go a bit deeper in the definition, in a slightly 'verbose=True' manner ;)\n",
    "\n",
    "\n",
    "#### _Natural language_\n",
    "The adjective _natural_ stands in opposition to _artificial_ (man-made) and distinguishes languages created by humans and that we design explicitly to be perfect (i.e., non-redundant and unambiguous, like Math, musical notation, traffic lights or Python), from other languages that have not been so designed, or _natural languages_.\n",
    "\n",
    "Natural languages develop spontanenously over time just as a result of being used by speakers and mostly as a factor of 1) the way they are learned by children in one generation and passed on to the next and 2) historical accidents happening to that language (_twerk_ is a historical accident that will now be in English for some time before it can be washed away ;)\n",
    "\n",
    "#### Actually just _messy_ language\n",
    "From our previous point it follows that languages are subject to standard evolutionary processes like those happening in biological systems, and that results in a number of interesting properties that can be roughly summarized by saying that natural languages are __really messy__. Below is a more detailed list:\n",
    "\n",
    "1. __Existing linguistic units change over time__. Their form changes (irregular verbs turn into regular verbs, BrEn _learnt_ > AmEn _learned_) but, more importantly, their meaning changes and adapts to new realities: ten years ago, the word _notebook_ would have never been applied to something that has no pages and needs electricity to write on it.\n",
    "\n",
    "2. __New linguistic units are added all the time__: new words are invented as human realities change and almost every time Steve Jobs opened his mouth :D The vocabulary expands at a very fast pace and an important part of human linguistic behavior is to be able to deduce the meaning of unknown words based on their usage and on contextual clues, as well as by inference (generalizing from known attributes of similar words). Understanding language involves a lot of guessing all the time.\n",
    "\n",
    "3. When linguistic units are produced, they usually __contain errors__. Try typing something on your smartphone and see what happens :) For humans, spoken communication is generally easier and faster, but everyone has trouble pronouncing a word occasionally. Over time, some errors become so widespread that they end up being the accepted way of saying something:\n",
    ">   __mobile phone__\n",
    ">     * mobile(1)  *phone that moves\n",
    ">     * mobile(2)   phone that is easily portable (so what do we have the word \"portable\" for?)\n",
    "\n",
    "4. __Most linguistic units denote ideas rather than things__. Whereas it is relatively easy for a robot to perceive a ball using some sensors and push it around a room, it is much more difficult to teach a computer the meaning of a term like _civil liberties_. We still have no effective representation for a large part of the knowledge that we use every day to understand language, and some of those ideas are even explicitly encoded in language itself (it is generally challenging to come up with an exhaustive explanation of the meaning and usage of the present perfect tense, or of polite forms in the languages that have them, like German, Japanese or Spanish). \n",
    "\n",
    "5. __Hierarchical structure__. Languages work at different levels of abstraction, which require different levels of resolution:\n",
    ">    _Characters < words < sentences < documents_\n",
    "\n",
    "Currently, technologies working at the __document__ level provide fairly good results, and systems working at the __sentence__ level achieve very good performance depending on the task. However, at the __word__ level there is still a lot of room for improvement. Most available technologies rely heavily on having a wider linguistic context available to generate results, or on hand-crafted information.\n",
    "\n",
    "\n",
    "So, when a computer looks at it:\n",
    "\n",
    "1. __Human language is inherently very high-dimensional and sparse.__ From a computer's perspective, languages consist of too many different words: most of them appear very rarely, a few of them appear too often, and many of them, when they appear, do not always appear with the same meaning (!). That last part is known as the modulation of meaning in context, or __semantic modulation__, and it means that words modify each other's meaning (which is, in turn, the cornerstone of the [Distributional principle](https://en.wikipedia.org/wiki/Distributional_semantics)).\n",
    "This is true of virtually all words but some of them actually __require__ modulation: intrinsically ambiguous words such as _firm_ (_solid_ as an adjective, a type of business when used as a noun) are like SchrÃ¶dinger's cat: their meaning is linguistically undefined until they are observed in a context that allows the speaker to disambiguate them: _He works at a law firm_ versus _The government has a firm control of the situation_.\n",
    "\n",
    "2. __Human language is (unnecessarily?) redundant.__ In language, there are many different ways to express the same thought (or the same overall idea):\n",
    "   * _That movie was terrible._\n",
    "   * _The movie was awful._\n",
    "   * _I did not like anything about this film._\n",
    "   * _Last night at the cinema -it was awful!_\n",
    "   * _What did I just watch?_\n",
    "   * _I am never getting that hour and a half of my life back._\n",
    "   * _Who would do something like this?_\n",
    "   * _I almost had a mental stroke watching that film._\n",
    "   * ... and more.\n",
    "\n",
    "For a computer, these all mean:\n",
    "\n",
    "    SENTIMENT(IDofMovieInDatabase, -5)\n",
    "\n",
    ":D The differences are stylistic and colorful, and humans can handle them relatively effortlessly -decoding all these variants is a type of mental gymnastics that our brains like to do, and generally find interesting: people who can come up with more creative and surprising ways of making an assertion are usually regarded as witty and can use that to call other people's attention, which is extremely important during an information exchange. For computers, however, __those people are simply annoying :)__\n",
    "\n",
    "Despite all the variation above, the appropriate behavior could still be triggered by the latter, more abstract representation: *__Computer, do not recommend me movies like this ever again__*. We want our system to get to that abstract interpretation, to the same conclusion, from any of the starting points above. Doing that in each case requires a varying number of intermediate steps.\n",
    "\n",
    "Take for instance the last variant, _I almost had a mental stroke watching that film_. The ability to detect the correct interpretation of this sentence would require the system to be aware of a number assumptions:\n",
    "\n",
    "Assumption | Topic | Description | Formalization\n",
    "--- | --- | --- | ---\n",
    "1 | Stroke | People can suffer mental strokes. | *HAS(person, stroke) && IS_OF_TYPE(stroke, mental) && ...*\n",
    "2 | Stroke | Mental strokes are highly negative. | *SENTIMENT(stroke, -5)*\n",
    "3 | Watching | People watch films. | *SAME_AS(film, movie) && WATCH(person, movie)*\n",
    "4 | Watching | The action of watching consists in receiving visual input. |\n",
    "5 | Watching | Receiving visual input is a simple thing for humans and does rarely cause permanent brain damage. |\n",
    "6 | Language | People sometimes do not speak literally, which involves a partial[2] violation of linguistic meaning. |\n",
    "7 | Language | When people do not speak literally, sometimes they exaggerate. |\n",
    "8 | Language | When people exaggerate, they make a stronger statement than it is literally correct but that they expect the listener to be able to perceive as such. |\n",
    "9 | Language | When the \"error\" is resolved by the listener, the differential between the strength of the non-literal expression and the inferred, literal one, can be seen as additional emphasis. | \n",
    "\n",
    "[2] A total violation of linguistic rules would make communication impossible.\n",
    "\n",
    "\n",
    "That is a high-level summary of all the knowledge required to correctly interpret the example above (a detailed representation would need e.g. further decomposing \"movie\" into some kind of annotation such as **[+event, +information]**, or sometimes into a database entry like **{'year': int, 'director': str, 'duration (minutes)': int, 'cast': List: str }** ).\n",
    "\n",
    "Over the course of their early development, up until adolescence, and probably borrowing also from long-evolved instincts, humans seem able to easily master assumptions 6-9, get exposed to tons of examples of facts such as those in 1-5, and can easily derive many more through inference. Computers, however, still need to be taught all of this explicitly, and we still lack a suitable representation for most of it.\n",
    "\n",
    "For most purposes, we are currently still at stage 3 but working hard on stage 4, with lots of great research teams doing very interesting early work (mostly through manual annotation and validation by humans combined with automated extraction and generalization workflows). However, it would seem that most of the work still needs to be done. Assumptions such as 6-9, on the other hand, still seem a bit far away.\n",
    "\n",
    "In this sense, the adjective _natural_ in natural language also opposes the adjective _artificial_ in _artificial intelligence_, and it is the reason why natural language is an area of work within artificial intelligence: computers, artificial systems, cannot yet understand natural language without help, and there is a whole area of research on how to make that possible. That is the focus of NLP, and it lies right at the __intersection of engineering, linguistics, and machine learning__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications and examples\n",
    "\n",
    "\n",
    "### Speech recognition\n",
    "\n",
    " > Development of technology that transforms spoken language into text by electronic systems.\n",
    "\n",
    "*From [Wikipedia](https://en.wikipedia.org/wiki/Speech_recognition)*\n",
    "\n",
    "1. Transform what people say into strings, which can be\n",
    "  * the final output or\n",
    "  * input for NLP.\n",
    "  * __DEMO__: in our phone (_where am I?_, _play [SONG](https://www.youtube.com/watch?v=bbr60I0u2Ng)_)\n",
    "2. Input (directive) vs. interactive\n",
    "  * Knowledge required.\n",
    "3. Word error rates\n",
    "  * 0.3% for number recognition over the phone.\n",
    "  * 10% for listening to the news on TV.\n",
    "  * 30% for conversation between many speakers with background noise.\n",
    "4. New word challenge. \n",
    "5. Based on sequential models (Hidden Markov Models).\n",
    "6. Data acquisition bottleneck.\n",
    "  * Tens to thousands of hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine translation\n",
    "\n",
    "*(Adapted from [Wikipedia](https://en.wikipedia.org/wiki/Machine_translation)*)\n",
    "\n",
    "Machine translation is the task of giving a computer a text in one language as input and getting back its translation into any other language. Most of you are probably familiar with [__Google Translate__](https://translate.google.com), which is probably the most paradigmatic example of the task and of what can be achieved with very large amounts of data. We all know it makes mistakes but, as its training dataset keeps increasing, it has an endless stream of data to learn from, and that data stream will eventually provide the exact translation of the whole paragraph you just wrote.\n",
    "\n",
    "> ese flujo de datos de vez en cuando va a proporcionar la traducciÃ³n exacta de todo el pÃ¡rrafo que acaba de escribir.\n",
    "\n",
    "> ÑĞ¾ Ğ¿Ğ¾ÑÑĞº Ğ´Ğ°Ğ½Ğ¸Ñ ÑĞ½Ğ¾Ğ´Ñ Ğ·Ğ°Ğ±ĞµĞ·Ğ¿ĞµÑĞ¸ÑÑ ÑĞ¾ÑĞ½Ğ¸Ğ¹ Ğ¿ĞµÑĞµĞºĞ»Ğ°Ğ´ Ğ²ÑÑĞ¾Ğ³Ğ¾ Ğ¿ÑĞ½ĞºÑÑ Ğ²Ğ¸ ÑÑĞ»ÑĞºĞ¸ ÑĞ¾ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ğ»Ğ¸.\n",
    "\n",
    "Just as the number Pi contains a copy of anything you will ever write :)\n",
    "\n",
    "Ultimately, __machine translation__ is looking up a word's translation over a humongously huge __bilingual dictionary__. That dictionary is actually pretty bad because the entries are not for words but rather for full sentences, that is why you need to use \n",
    "\n",
    "\n",
    "In many ways, __machine translation__\n",
    "\n",
    "At one level, MT simply replaces words in one language (source language) with words in another language (target language). However, the mapping between the words in each language is not bi-univocal: sometimes, one source word translates into more than one target words; other times, two source terms translate into a single target term. Apart from that, translation exhibits the full range of complexity in natural languages, which means that deep linguistic understanding would be necessary strictly speaking to be able to perform automated translation in a well-founded way.\n",
    "\n",
    "Although great success currently mapping a text to its translation, it is still not the type of translation we would like: via ideas rather than examples. At this point we still only have a super-smart parrot that repeats things people say but has no understanding of what it is saying and that is why it often says it at the wrong time or makes mistakes. The crucial transition to \n",
    "\n",
    "5. Non-obvious evaluation.\n",
    "  * Hard to come by proper evaluation data. Sometimes due to lack of resources, some other times due to the intrinsic difficulty of providing a correct answer. Like in the case of __machine translation__, many answers are actually possible. Like in the case of __machine translation__, metrics such as [BLEU](https://en.wikipedia.org/wiki/BLEU) (Bilingual Evaluation Understudy), [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) or [WER](https://en.wikipedia.org/wiki/Word_error_rate) are used.\n",
    "\n",
    "> Most commercial machine translation is successful thanks to customization and focus on specific tasks/domains, decreasing error rates by limiting the scope of the potential substitutions. This technique is particularly effective with formulaic language such as legal documents. Informal, spontaneous conversation is much more challenging and machine translation of that type of input is currently an unsolved problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document classification\n",
    "1. Spam filtering.\n",
    "  * Detection of fake reviews.\n",
    "  * Plagiarism detection.\n",
    "2. Authorship attribution.\n",
    "3. Sentiment analysis.\n",
    "  * Opinion mining.\n",
    "  * Early warning systems.\n",
    "\n",
    "\n",
    "### Sentiment analysis\n",
    "1. *Should I go see this movie or not?, Are my employees happy or not?*\n",
    "2. Many providers/tools.\n",
    "3. Non-trivial for the same reasons as \n",
    "  * parsing\n",
    "  * Aspect-based sentiment analysis/Attribute-based understanding\n",
    "4. Slack emotion tracking bot\n",
    "\n",
    "\n",
    "### A couple of interesting classification use cases\n",
    "[Assist.ai](https://assist.ai)\n",
    "\n",
    "[Xeneta](https://medium.com/xeneta/boosting-sales-with-machine-learning-fbcf2e618be3#.ynm031lo2)\n",
    "Lead qualification for improving the efficiency of a sales department. The asked the question, _Given a company description, can we train an algorithm to predict whether or not itâs a potential customer?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural language generation\n",
    "1. Canned (some number of fixed strings as options used interchangeably for a each relevant generation field. Heavily lexicalized, i.e., very reliant on dictionaries/hard-coding/etc.)\n",
    "2. Re-used data.\n",
    "3. [Finite-state](https://en.wikipedia.org/wiki/Finite-state_machine) NLP generation: the system's response is based on a template with some placeholders and some knowledge of their fillers, any semantic or syntactic properties that would require a different message. This is what your system does when the message _Compressing 2 files_ changes into _Compressing 1 **file**_.\n",
    "\n",
    "\n",
    "### Automatic text summarization\n",
    "1. Given a text, return a new text with a length that is a user-defined fraction of the length of the input, and as much of its original content as possible.\n",
    "2. Some examples:\n",
    "  * Action items from a meeting.\n",
    "  * Summary of an email thread.\n",
    "  * Paper abstracts.\n",
    "  * List of events in series of documents.\n",
    "3. How?\n",
    "  1. **Usupervised**. Unless specified otherwise, some variation over a TFIDF-weighted algorithm.\n",
    "  2. **Supervised**. If there is a corpus of _reference summaries_ available.\n",
    "4. Extractive and abstractive summarization.\n",
    "  * Extractive: the output consists of sentences from the original text.\n",
    "  * Extractive: the output involves some level of Natural Language Generation.\n",
    "5. __Maximal Marginal Relevance__. Iteratively and dynamically choose the next sentence that should go into the summary given the current activation state based on previous sentences. [Example](https://techcrunch.com/2016/07/17/softbank-is-reportedly-bidding-to-buy-chip-giant-arm-for-31-billion/).\n",
    "6. Non-obvious evaluation.\n",
    "  * Hard to come by proper evaluation data. Sometimes due to lack of resources, some other times due to the intrinsic difficulty of providing a correct answer. Like in the case of __machine translation__, many answers are actually possible. Like in the case of __machine translation__, metrics such as BLEU or ROUGE are used.\n",
    "  * _It works and we are happy with it, but there is no theoretical guarantee it is the best._\n",
    "7. We know we want it to involve understanding:\n",
    "  * The summary of a text about a person should probably include biographical data about that person.\n",
    "  * The summary of a text about a news item should probably answer what, when, where.\n",
    "  * The summary of a text about medical research should include the sample, the methodology, and the results.\n",
    "  * ... and so on.\n",
    "\n",
    "\n",
    "### Text normalization\n",
    "1. Spellchecking\n",
    "  * Re-capitalization.\n",
    "2. Autocomplete.\n",
    "3. Swipe-style keyboards for smartphones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question and Answer (Q&A/*QA*) systems (NLP wrapper around Information Retrieval)\n",
    "1. Return the answer to a query expressed in natural language or, more importantly, admit that the answer is not known.\n",
    "3. From [Eliza](http://manifestation.com/neurotoys/eliza.php3/) to [Viv](https://www.youtube.com/watch?v=Rblb3sptgpQ). Still not at a HAL-stage.\n",
    "  * Eliza was just a set of insightfully vague templates with some cleverly unspecific Natural Language Generation built into it. We must not underestimate the power of vagueness in human communication -__many politicians make their entire careers as objects on the class *Eliza*__ :D\n",
    "2. Examples\n",
    "   * _What are the most recent results in the field of Q&A?_\n",
    "   * _Whose idea was the pub quest?_\n",
    "   * _How can I reduce stress?_\n",
    "2. An example like the last one in the list above should be translated into\n",
    "          candidates = SELECT ID FROM scientific_disciplines sd WHERE (\n",
    "                    sd.area LIKE '^q.*a$' AND       #  An entity detected as 'AREA' (of knowledge)\n",
    "             sd.subject LIKE '^results?$'    #  A tag assigned by a document classification algorithm.\n",
    "          ) ORDER BY sd.year DESCENDING             #  An entity detected as 'YEAR'.\n",
    "  * Will normally benefit from Named Entity Recognition (NER).\n",
    "  * Will normally benefit from having available a database with relevant data (_=taxonomy, ontology_)\n",
    "  * Will normally start by rewriting the user query into a statement and then trying to match it to documents in the dataset.\n",
    "  * After rewriting, a sequential n-gram model (Hidden Markov Model) scores the similarity between the answer and each candidate question.\n",
    "     * Sparsity can easily hurt performance for high _n_'s.\n",
    "     * The userâs question is often syntactically close to actual answer: _Where is[1] the[2] White[3] House[4]?_ > _The[2] White[3] House[4] is[1] in Washington DC._\n",
    "4. Performance.\n",
    "  * Depends heavily on question type, ranging from 0.6 (_why_- or _how_-questions) to 0.9 (dates, strongly statistically associated and unambiguous concepts, etc.).\n",
    "  * Sometimes web data results in higher results than standard evaluation datasets.\n",
    "  \n",
    "5. [An applied example.](http://nbviewer.jupyter.org/github/JordiCarreraVentura/question_answer/blob/master/Question%20and%20Answer%20assignment.html#Question-and-Answer-assignment)\n",
    "  * Nearly 90% accuracy, but the dataset is probably too small so we should not be over-confident.\n",
    "  * State-of-the-art is 84-85% Mean Reciprocal Rank on the [TREC evaluation task](http://aclweb.org/aclwiki/index.php?title=Question_Answering_(State_of_the_art)) of the ACL (Association for Computational Linguistics), which is like the NASA of NLP.\n",
    "\n",
    "\n",
    "#### A kick-ass Q&A example\n",
    "[University professorâs teaching assistant was an AI](http://www.smh.com.au/technology/innovation/professor-reveals-to-students-that-his-assistant-was-an-ai-all-along-20160513-gou6us.html)\n",
    "\n",
    "Are you getting any ideas? :) (=_Lviv Data Science Summer School Bot_)\n",
    "\n",
    "#### Many other prominent examples\n",
    "- [Maluuba](http://www.maluuba.com)\n",
    "- [Siri](https://www.apple.com/ios/siri/)\n",
    "- [Google Now](https://www.google.com/search/about/learn-more/now/)\n",
    "- [Cortana](http://www.windowscentral.com/cortana)\n",
    "- [Amazon Echo](https://www.amazon.com/Amazon-Echo-Bluetooth-Speaker-with-WiFi-Alexa/dp/B00X4WHP5E?ie=UTF8&*Version*=1&*entries*=0)/[Amazon Alexa](https://www.engadget.com/2015/07/03/amazon-echo/)\n",
    "\n",
    "##### ... even in Spain!\n",
    "- [Hutoma](http://www.hutoma.com)\n",
    "- [Inbenta](https://www.inbenta.com/en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Understanding (NLU)\n",
    "\n",
    "This is it :) That is the ultimately goal and what all the major companies have their own teams working on right now. Behind every virtual assistant, there is a team of people taking care of the knowledge-engineering.\n",
    "\n",
    "1.Entity extraction\n",
    "\n",
    "       PERSON[Jon Snow] is a character in SHOW[Game of Thrones].\n",
    "\n",
    "2. Parsing: relation extraction\n",
    "\n",
    "       Jon Snow is a ROLE[character+{in, of, at, from}] Game of Thrones.\n",
    "\n",
    "3. Tuple/Triple/n?-ple extraction\n",
    "\n",
    "       IsCharacter('Jon Snow', 'Game of Thrones')\n",
    "\n",
    "4. Semantic relations (easy to transform into [OWL](https://en.wikipedia.org/wiki/Web_Ontology_Language)/[RDF](https://en.wikipedia.org/wiki/Resource_Description_Framework)-type ontologies).\n",
    "\n",
    "5. Could answer already a few different questions:\n",
    "\n",
    "       Who are the main characters in Game of Thrones?\n",
    "       Is Jon Snow a character from Game of Thrones?\n",
    "       Who is Jon Snow?\n",
    "\n",
    "2. A huge number of applications:\n",
    "  * Structure content according to search parameters for easier browsing (any company with a big catalogue of user-generated data needs this): people, products, jobs, institutions, etc.\n",
    "  * Extending in-house knowledge-bases with an endless stream of recent publications/up-to-date data.\n",
    "\n",
    "3. Huge potential gains in many big industries:\n",
    "  * Healthcare (lots of research papers and data on clinial trials)\n",
    "  * Finance (lots of financial reports)\n",
    "  * Legal (tons of patents)\n",
    "  * Human capital management and counter-terrorism\n",
    "\n",
    "4. Already massive datasets available:\n",
    "   * Relations\n",
    "     * [Freebase](https://developers.google.com/freebase/). Google took it offline but the last version of the data is still downloadable; a 32 Gb .zip file full of relation triples :) A NLP scientist's dream come true!\n",
    "     * [ConceptNet](http://conceptnet5.media.mit.edu).\n",
    "   * Entities or concepts\n",
    "     * [Semantically Enriched Wikipedia](http://lcl.uniroma1.it/sew/, because WordNet is too small :)\n",
    "     * [BabelNet](http://babelnet.org).\n",
    "     * [OmegaWiki](http://www.omegawiki.org).\n",
    "     * [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/about)\n",
    "     * Kaggle, Yelp, Microsoft, Google challenges/competitions.\n",
    "     * [Linguistic Data Consortium](https://www.ldc.upenn.edu).\n",
    "\n",
    "##### A cool example of fully-developed knowledge acquisition pipeline\n",
    "Carnegie Mellon University's [__Never-Ending learning system__](http://www.cs.cmu.edu/~tom/pubs/NELL_aaai15.pdf).\n",
    "\n",
    "__NOTE:__ By the way, Carnegie Mellon University, together with Stanford University and the University of Edinburgh, are the research centers doing some of the coolest research on NLP. You probably want to read anything published by people working there.\n",
    "\n",
    "\n",
    "##### An extremely inspiring and socially relevant use case: \"Manolo\"\n",
    "__ScrapingHub's blog__. [How web-scraping reveals corruption](https://blog.scrapinghub.com/2016/03/09/how-web-scraping-is-revealing-lobbying-and-corruption-in-peru/) Peruvian journalists are going through **Manolo**'s data in order to find cases of corruption and report them. There is money in serving ads on search results, but __Manolo__ can make the world a better place.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* NLP is about creating systems that can understand people when they speak using their own words, and that can talk back to them using a similar language.\n",
    "* Even a shallow of understanding of language, however, can already provide a competitive advantage in a commercial environment, and many companies exist that develop applied NLP solutions. Any task involving linguistic data and repetition in the workplace can probably be automated, like answering recurring questions about a laptop's technical specifications on Amazon, or translating instruction manuals that contain very simple and repeating linguistic structures. The philosophy behind NLP is that no humans should be harmed to perform anything that can be done by a computer.\n",
    "* NLP is a subfield of __engineering__ (because it involves automation), __linguistics__ (because it involves the formal study of human languages), and __computer science__ (because it involves machine learning). The machine learning component already takes care of the part about receiving some input and performing a human-like action as a response. NLP is required, however, to map linguistic inputs to a suitable formal representation. In that sense, NLP is about formalizing language and using machine learning methods to aid in that task.\n",
    "* Down to its bare minimum, NLP is about __extracting structured knowledge from unstructured (user-generated) data__, and use it to train systems that can react to linguistic stimuli in the same way as a human would."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
